{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b008da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "MODEL_PATH='../Llama-3.1-8B-Instruct'\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "SUMMARIES_PATH = \"./finetune_summaries_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc317583",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16).to(DEVICE)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b4af005",
   "metadata": {},
   "outputs": [],
   "source": [
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "docs = [(fn, open(os.path.join(DOCS_PATH, fn), 'r', encoding='utf-8').read()) for fn in tqdm(os.listdir(DOCS_PATH)) if fn.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bfd842",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(question):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21a0a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "template = \"The following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\\n\\n\\\"{content}\\\".\"\n",
    "\n",
    "# create folder (with exists_ok = True)\n",
    "os.makedirs(SUMMARIES_PATH, exist_ok=True)\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    if not os.path.exists(os.path.join(SUMMARIES_PATH, f\"{d[0].split('.')[0]}.json\")):\n",
    "        prompt = tokenizer.apply_chat_template(\n",
    "            create_chat(template.format(content=d[1])), \n",
    "            tokenize=False, \n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "\n",
    "        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "        prompt_len = input_ids.shape[1]\n",
    "        output_ids = model.generate(\n",
    "            input_ids,\n",
    "            max_new_tokens=1000,\n",
    "            pad_token_id=128004,\n",
    "            eos_token_id=128009,\n",
    "            do_sample=False,\n",
    "            top_p=1.0,\n",
    "        )[:, prompt_len:]\n",
    "        summary = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "        chat = {\"question\": prompt, \"answer\": summary}\n",
    "\n",
    "        with open(os.path.join(SUMMARIES_PATH, f\"{d[0].split(\".\")[0]}.json\"), \"w\", encoding=\"utf-8\") as f:\n",
    "            json.dump(chat, f, ensure_ascii=False, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
