{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "MODEL_PATH='../../Llama-3.2-3B-Instruct'\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "QUESTIONS_PATH = \"../rag_questions_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 27.37it/s]\n"
     ]
    }
   ],
   "source": [
    "model  = AutoModelForCausalLM.from_pretrained(MODEL_PATH, torch_dtype=torch.bfloat16)\n",
    "model = PeftModel.from_pretrained(model, \"./qlora_checkpoints/checkpoint-460\").to(DEVICE).merge_and_unload().eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(question):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 21 Apr 2025\n",
      "\n",
      "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?\"\n",
    "prompt_with_context = tokenizer.apply_chat_template(\n",
    "    create_chat(question), \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To study and forecast the evolution of carbon emissions on a country-scale, researchers and policymakers typically use a combination of indicators that capture various aspects of the energy sector, economy, and human behavior. Some of the main indicators used to study and forecast carbon emissions include:\n",
      "\n",
      "1. **Energy intensity**: This refers to the amount of energy consumed per unit of GDP (Gross Domestic Product) or per capita. Energy intensity is a key driver of greenhouse gas emissions, as it reflects the amount of energy required to produce goods and services.\n",
      "2. **Economic growth**: Economic growth is a key driver of energy demand and carbon emissions. As a country's economy grows, so does its energy demand, which can lead to increased emissions.\n",
      "3. **Renewable energy share**: The share of renewable energy sources in the energy mix is a key indicator of a country's transition towards a low-carbon economy. An increasing share of renewables can reduce emissions, while a decreasing share can lead to increased emissions.\n",
      "4. **Fossil fuel combustion**: Fossil fuel combustion is a significant source of greenhouse gas emissions. Monitoring the amount of fossil fuels consumed can help track emissions trends and identify areas for improvement.\n",
      "5. **Transportation sector emissions**: The transportation sector is a significant contributor to greenhouse gas emissions, particularly in countries with large populations and economies. Monitoring emissions from transportation can provide insights into the effectiveness of policies aimed at reducing emissions.\n",
      "6. **Energy efficiency**: Energy efficiency measures, such as energy-efficient appliances and building codes, can reduce energy consumption and lower emissions.\n",
      "7. **Population growth and demographics**: Changes in population growth, urbanization, and demographic trends can impact energy demand and emissions.\n",
      "8. **Agricultural sector emissions**: The agricultural sector is a significant source of greenhouse gas emissions, particularly methane and nitrous oxide, which are associated with livestock and fertilizer use.\n",
      "9. **Industrial sector emissions**: The industrial sector, including manufacturing and mining, is a significant source of greenhouse gas emissions, particularly carbon dioxide and methane.\n",
      "10. **Carbon intensity**: Carbon intensity measures the amount of carbon dioxide emitted per unit of economic activity, providing a more granular picture of a country's emissions profile.\n",
      "\n",
      "These indicators were chosen for several reasons:\n",
      "\n",
      "1. **Correlation with emissions**: Each indicator has a strong correlation with carbon emissions, making them useful for identifying trends and patterns in emissions.\n",
      "2. **Policy relevance**: Many of these indicators are directly impacted by policy decisions, making them useful for evaluating the effectiveness\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(tokenizer, skip_prompt=True)\n",
    "\n",
    "input_ids = tokenizer.encode(prompt_with_context, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    pad_token_id=128004,\n",
    "    eos_token_id=128009,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_chat(question, answer):\n",
    "    return [\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{answer}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:27<00:00, 27.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 10.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "losses = list()\n",
    "question_filenames = os.listdir(QUESTIONS_PATH)\n",
    "\n",
    "for question_filename in tqdm(question_filenames):\n",
    "    with open(os.path.join(QUESTIONS_PATH, question_filename), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    target_chat = tokenizer.apply_chat_template(\n",
    "        create_target_chat(qa[\"question\"], qa[\"answer\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokens = tokenizer(target_chat, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).to(DEVICE)\n",
    "    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        create_chat(qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    ignore_idx = tokenizer.encode(prompt, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).shape[1]\n",
    "\n",
    "    filtered_labels = tokens[\"input_ids\"].clone()\n",
    "    ignore_mask = torch.zeros_like(filtered_labels, dtype=torch.bool)\n",
    "    ignore_mask[0, :ignore_idx] = True\n",
    "    filtered_labels[ignore_mask] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**tokens, labels=filtered_labels)\n",
    "        losses.append(outputs.loss.item())\n",
    "        loss += outputs.loss.item()\n",
    "\n",
    "perplexity = math.exp(loss/len(question_filenames))\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Average time per request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/747 [00:00<?, ?it/s]/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 747/747 [2:51:33<00:00, 13.78s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per request: 13.78 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "\n",
    "for question_filename in tqdm(question_filenames):\n",
    "    with open(os.path.join(QUESTIONS_PATH, question_filename), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        create_chat(qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    model.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=128004,\n",
    "        eos_token_id=128009,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "print(f\"Average time per request: {(time.time() - begin) / len(question_filenames):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
