{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" Index (NDVI) have a typical error of ap- proximately 0.05 (over a natural range of −1→1) at the pixel level over a five month test set horizon, out-performing simple phase-folded models based on his- torical averaging. We also demonstrate that embeddings learnt by EarthPT hold semantically meaningful information and could be exploited for downstream tasks such as highly granular, dynamic land use classification. Excitingly, we note that the abundance of EO data provides us with – in theory – quadrillions of training tokens. Therefore, if we assume that EarthPT follows neural scaling laws akin to those derived for Large Language Models (LLMs), there is currently no data- imposed limit to scaling EarthPT and other similar ‘Large Observation Models.’ 1 Introduction Deep learning’s current ‘hot topics’ are foundation models in the vein of EleutherAI’s GPT-NeoX, OpenAI’s GPT- Nmodels, DeepMind’s Chinchilla, and the RWKV Foundation’s eponymous model [1–5]. These remarkably simple models contain a few standard deep learning building blocks and are trained by repeatedly predicting the next item in a sequence. Surprisingly, these models’ perfor- mances scale with dataset and model size via a simple power law [6, 7]. Even more astoundingly, at a certain scale of data and compute, these models display ‘emergent abilities’ such as apparent knowledge of arithmetic, law, geography, and history [e.g. 8]. In March 2022 a team at Google DeepMind discovered that – optimally – the size of these foundation models should be scaled in a roughly equal proportion to the size of the dataset used to train them [4]. Smith and Geach [9] demonstrated that this implies that the current constraint on state-of-the-art textual foundation model performance is dataset size, and not model size as previously thought. Although we are running out of useful high quality textual data to train foundation models, there remains an untapped abundance of high quality data in other domains [10, 11]. Smith and Geach [9] argue that astronomy is one such domain, and we argue here that remote sensing data sets, and in particular Earth Observation (EO) spatial and temporal data, can also be used as an additional non-textual data mode to aid in the training of ever larger, more generalist, and more performant foundation models. Here we demonstrate that EO imaging data can be used to train a sizable transformer model in the spirit of large language modelling. To this end we train a Chinchilla-optimal 700M parameter de- coding transformer model on 14B tokens of EO data in the form of multispectral time series for just over one hundred million individual pixels. The time series are analogous to word and sen- tence sequences in textual models, but in this case represent surface-level (solar) reflectance values Tackling Climate Change with Machine Learning workshop at NeurIPS 2023, New Orleans. ∗mike.smith@aspiaspace.commeasured in a number of passbands across the 400–2300 nm spectral range – i.e. the wavelengths corresponding to traditional ‘optical’ EO imagery. Single pixel time series are commonly used in remote sensing to train transformer and self-attention based networks on supervised tasks [e.g. 12, 13]. However, currently few works apply these models in a self-supervised manner. Those that do are typically limited to very short – or even single step – time series inputs [e.g. 14, 15]. The closest approach in the literature to EarthPT is perhaps Tseng et al. [16]. They show that an encoding transformer model [i.e. 17] is capable of learning semantically meaningful embeddings from remote sensing time series. Their model is trained on a relatively small dataset comprised of 21.5M tokens arranged into chunks of shape [time,channel] ≡[12,19]. Tseng et al. [16] note their model’s capability despite its small size. Our work has a diametrical and complementary purpose; we aim to demonstrate that a transformer model trained on EO data is capable of scaling to the extent that we have seen in the natural language domain, with similar potential for wide utilisation and impact. In particular, we demonstrate that EarthPT can accurately forecast reflectance values well into the future, thus providing a method to predict – and therefore an opportunity to mitigate – future events associated with environmental threats such as drought. 2 Methods This section describes the datasets that we use to train EarthPT and the hyperparameters and training routine of our chosen decoding transformer architecture. Training imagery. ClearSky is a proprietary deep learning algorithm that accurately predicts the equivalent of European Space Agency Sentinel-2 imagery products across 10 spectral bands: Blue, Green, Red, Red Edge 1-4, NIR, and SWIR 1 and 2. The input data for ClearSky is Sentinel\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy and dirty parts removed:\n\nIndex (NDVI) has a typical error of approximately 0.05 over a natural range of -1 to 1 at the pixel level over a five-month test set horizon, outperforming simple phase-folded models based on historical averaging. We demonstrate that embeddings learned by EarthPT hold semantically meaningful information and can be exploited for downstream tasks such as highly granular, dynamic land use classification. The abundance of Earth Observation (EO) data provides quadrillions of training tokens, suggesting that there is no data-imposed limit to scaling EarthPT and similar \"Large Observation Models.\"\n\nIntroduction\n\nDeep learning's current focus is on foundation models, such as EleutherAI's GPT-NeoX, OpenAI's GPT-N models, DeepMind's Chinchilla, and the RWKV Foundation's eponymous model. These models contain a few standard deep learning building blocks and are trained by predicting the next item in a sequence. Surprisingly, their performances scale with dataset and model size via a simple power law. At a certain scale of data and compute, these models display \"emergent abilities\" such as knowledge of arithmetic, law, geography, and history. A team at Google DeepMind discovered that the size of these foundation models should be scaled in a roughly equal proportion to the size of the dataset used to train them. Smith and Geach demonstrated that this implies that the current constraint on state-of-the-art textual foundation model performance is dataset size, not model size.\n\nAlthough we are running out of high-quality textual data to train foundation models, there remains an untapped abundance of high-quality data in other domains. Smith and Geach argue that astronomy is one such domain, and we argue that remote sensing data sets, particularly Earth Observation (EO) spatial and temporal data, can be used to aid in the training of ever larger, more generalist, and more performant foundation models. We demonstrate that EO imaging data can be used to train a sizable transformer model in the spirit of large language modeling.\n\nTo this end, we trained a Chinchilla-optimal 700M parameter decoding transformer model on 14B tokens of EO data in the form of multispectral time series for over 100 million individual pixels. The time series represent surface-level (solar) reflectance values measured in a number of passbands across the 400-2300 nm spectral range. Single pixel time series are commonly used in remote sensing to train transformer and self-attention based networks on supervised tasks. However, few works apply these models in a self-supervised manner, typically using very short or single-step time series inputs.\n\nOur work has a complementary purpose; we aim to demonstrate that a transformer model trained on EO data can scale to the extent seen in the natural language domain, with similar potential for wide utilization and impact. We demonstrate that EarthPT can accurately forecast reflectance values well into the future, providing a method to predict and mitigate future events associated with environmental threats such as drought.\n\nMethods\n\nThis section describes the datasets used to train EarthPT and the hyperparameters and training routine of our chosen decoding transformer architecture. We used ClearSky, a proprietary deep learning algorithm that predicts the equivalent of European Space Agency Sentinel-2 imagery products across 10 spectral bands. The input data for ClearSky is Sentinel-2 imagery."
}