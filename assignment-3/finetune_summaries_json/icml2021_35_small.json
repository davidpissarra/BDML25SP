{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" regardless of lobbies or prevailing political orientation, all government legal actions are published daily in the Federal Of- ﬁcial Gazette. However, with hundreds of decrees issued every day by the authorities, it is absolutely burdensome to manually analyze all these pro- cesses and ﬁnd out which ones can pose serious environmental hazards. In this paper, we propose the DeepPolicyTracker, a promising deep learn- ing model that uses a state-of-the-art pretrained natural language model to classify government acts and track changes in the environmental poli- cies. We also provide the used dataset annotated by domain experts and show some results already obtained. In the future, this system should serve to scale up the high-quality tracking of all oﬁcial documents with a minimum of human supervision and contribute to increasing society’s awareness of every government action. 1. Introduction Brazil has one of the largest reserves of biodiversity in the world, such as the Amazon rainforest, Cerrado and Atlantic forest. The preservation of these biomes is essential for the country to be able to fulﬁll the objectives of the Paris Agree- ment (Rochedo et al., 2018), since 78% of greenhouse gas emissions in Brazil come from land use and cover change (West et al., 2019). In 2020, while global emissions fell as 1Polytechnic School, University of Sao Paulo, Sao Paulo, Brazil 2Pol´ıtica por Inteiro, Sao Paulo, Brazil. Correspondence to: Fl ´avio Nakasato Cac ¸ ˜ao<ﬂavio.cacao@usp.br >. Tackling Climate Change with Machine Learning Workshop at ICML 2021.a result of the coronavirus pandemic, in Brazil they grew substantially driven by deforestation and farming (Spring, 2020); the Amazon Rainforest deforestation rate was the greatest of the decade (Silva Junior et al., 2021). At the same time, the country is an agribusiness powerhouse, with 26.6% of the GDP related to it (CEPEA-ESALQ, 2021) and is governed by a president who did not hide his intentions to weaken current environmental policies (Nature, 2018). In this complex environment, it is extremely important, and it is a high leverage strategy, to be able to track the acts issued by the government, in order to alert and empower civil so- ciety with qualiﬁed and clear information (Rolnick et al., 2019). However, as noted in (Grimmer & Stewart, 2013), this is an arduous task for manual work alone: hundreds of highly technical documents are issued every day by the Congress and the Executive branches. Therefore, these issues represent an opportunity for the most recent models that automate the natural language processing (NLP) tasks. Pretrained language models have started to become popular in recent years and have set new quality standards in virtually all NLP tasks such as classiﬁcation, translating and question answering. They have millions or billions of parameters and are built upon the Transformer architectures (Vaswani et al., 2017). These models are pre- trained in a self-supervised way over huge databases, like the entire Wikipedia, and then they can be used to solve other language problems, with ﬁne-tunings in smaller domain- speciﬁc datasets. The result obtained in general is much superior when compared to a model trained solely on the smaller dataset (Raffel et al., 2020). In this work, we contribute with the DeepPolicyTracker model, an approach to classify federal government acts using a state-of-the-art NLP technique, called BERT (De- vlin et al., 2019), which is a bidirectional encoder architec- ture. In order to ﬁne-tune this architecture to our speciﬁc scope, a preliminary dataset was prepared where thousands of documents were initially pre-classiﬁed and ﬁltered by a rule-based robot and then reviewed and enriched under the supervision of domain experts. It is noteworthy that the system formed only by the rule-based robot followed by a layer of human supervision today feeds one of theDeepPolicyTracker: Tracking Changes In Environmental Policy In The Brazilian Federal Ofﬁcial Gazette With Deep Learning Figure 1. The operational ﬂow of the data pipeline. A rule-based robot scrapes and pre-classiﬁes all ofﬁcial documents released every day (step a), and loads them into a general database (step b). The most relevant acts related to the environment are sent to a team of domain experts (step c), who manually reviews the robot’s classiﬁcations and enriches the database with new information. Errors found by annotators are regularly used to improve the rule-based robot (step d). This ﬁltered and veriﬁed subset of data is loaded into the Government\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nRegardless of lobbies or prevailing political orientation, all government legal actions are published daily in the Federal Official Gazette. However, with hundreds of decrees issued every day by the authorities, it is absolutely burdensome to manually analyze all these processes and find out which ones can pose serious environmental hazards. In this paper, we propose the DeepPolicyTracker, a promising deep learning model that uses a state-of-the-art pre-trained natural language model to classify government acts and track changes in environmental policies. We also provide the used dataset annotated by domain experts and show some results already obtained. In the future, this system should serve to scale up the high-quality tracking of all official documents with a minimum of human supervision and contribute to increasing society's awareness of every government action.\n\nBrazil has one of the largest reserves of biodiversity in the world, including the Amazon rainforest, Cerrado, and Atlantic forest. The preservation of these biomes is essential for the country to fulfill the objectives of the Paris Agreement (Rochedo et al., 2018), since 78% of greenhouse gas emissions in Brazil come from land use and cover change (West et al., 2019). In 2020, while global emissions fell as a result of the coronavirus pandemic, in Brazil they grew substantially driven by deforestation and farming (Spring, 2020); the Amazon Rainforest deforestation rate was the greatest of the decade (Silva Junior et al., 2021). At the same time, the country is an agribusiness powerhouse, with 26.6% of the GDP related to it (CEPEA-ESALQ, 2021) and is governed by a president who did not hide his intentions to weaken current environmental policies (Nature, 2018). In this complex environment, it is extremely important, and it is a high-leverage strategy, to be able to track the acts issued by the government, in order to alert and empower civil society with qualified and clear information (Rolnick et al., 2019). However, as noted in (Grimmer & Stewart, 2013), this is an arduous task for manual work alone: hundreds of highly technical documents are issued every day by the Congress and the Executive branches.\n\nTherefore, these issues represent an opportunity for the most recent models that automate the natural language processing (NLP) tasks. Pre-trained language models have started to become popular in recent years and have set new quality standards in virtually all NLP tasks such as classification, translation, and question answering. They have millions or billions of parameters and are built upon the Transformer architectures (Vaswani et al., 2017). These models are pre-trained in a self-supervised way over huge databases, like the entire Wikipedia, and then they can be used to solve other language problems, with fine-tunings in smaller domain-specific datasets. The result obtained in general is much superior when compared to a model trained solely on the smaller dataset (Raffel et al., 2020).\n\nIn this work, we contribute with the DeepPolicyTracker model, an approach to classify federal government acts using a state-of-the-art NLP technique, called BERT (Devlin et al., 2019), which is a bidirectional encoder architecture. In order to fine-tune this architecture to our specific scope, a preliminary dataset was prepared where thousands of documents were initially pre-classified and filtered by a rule-based robot and then reviewed and enriched under the supervision of domain experts. It is noteworthy that the system formed only by the rule-based robot followed by a layer of human supervision feeds into the DeepPolicyTracker.\n\nThe operational flow of the data pipeline is as follows: a rule-based robot scrapes and pre-classifies all official documents released every day (step a), and loads them into a general database (step b). The most relevant acts related to the environment are sent to a team of domain experts (step c), who manually reviews the robot's classifications and enriches the database with new information. Errors found by annotators are regularly used to improve the rule-based robot (step d). This filtered and verified subset of data is loaded into the Government."
}