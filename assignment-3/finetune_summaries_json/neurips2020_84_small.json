{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" offer these insights at scale, unbiased to company- and country-level reporting. In this proposal, we document the development of an extensible corpus of labelled and unlabelled Sentinel-1 and Sentinel-2 imagery for the purposes of sensor fusion research. We make a large corpus and supporting code publicly available. We propose our own experiment design for the development ofDeepSentinel, a general-purpose semantic embedding model. Our aspiration is to provide pretrained models for transfer learning applications, signiﬁcantly accelerating the impact of machine learning-enhanced earth observation on climate change mitigation. 1 Earth observation for climate change mitigation Satellite-based earth observation plays a central role in measuring climate change impacts and risks [5]. Medium-resolution satellites (10-30m spatial resolution), despite being initially designed for environmental monitoring, are being increasingly used in applications focusing on the interface between the environment and the real economy for the purposes of ﬁnancial risk measurement: estimating carbon dioxide emissions[ 11], methane emissions[ 32], and localising large ﬁxed-capital assets[ 18]. For the purposes of assessing ﬁnancial risk due to climate change, these satellites provide the globally exhaustive and unbiased view required by ﬁnancial decision makers. Deploying analysis at this global scale can only be accomplished with the use of machine learning. These climate change risk applications are impaired by two perennial challenges with satellite- based earth observation: the presence of atmospheric interference, and a shortage of training labels. Atmospheric interference is not equally distributed around our planet. Excessive cloud cover makes surface retrievals using multispectral imagery very challenging in certain geographies, negating its otherwise ‘exhaustive‘ coverage. Many cloudy geographies are in the global south, precisely where populations are the most vulnerable to climate change and where conventional reported data is the most sparse. These same geographies are where ﬁnancial institutions in the global north concentrate their risk exposure to generate outsize returns, and are where civil society groups must be most vigilant to identify and respond to neocolonial practises by these same institutions. The shortage of training labels for machine learning with earth observation data is well documented. Land use and land cover data is available with moderate spatial and temporal resolution in Europe (e.g. the Copernicus Corine Land Cover[ 27]) and the US (e.g. the USDA Cropland Data Layer [ 28]), \u0003Contact: lucas.kruitwagen@smithschool.ox.ac.uk ; Bio: https://lkruitwagen.github.io/ 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.Figure 1: DeepSentinel summary, showing pretraining and ﬁne-tuning curriculum and the variety of use case applications. however data for the rest of the planet are sparse (e.g. OpenStreetMaps [ 23]). These datasets include only broad categories of land use and land cover, and are not ﬁt for the purpose of localising speciﬁc categories of industrial infrastructure. Financial institutions just beginning to reckon with geospatial data, leaving a large gap in existence and availability of spatially-localised asset-level data.[4] We propose a general-purpose sensor fusion semantic embedding model to overcome these dual challenges of atmospheric interference and label availability. DeepSentinel will use sensor fusion of SAR Sentinel-1 data and multispectral Sentinel-2 data to provide latent space embeddings of surface conditions even in excessively cloudy conditions. Self-supervised pretraining followed by ﬁne tuning on land-use and land-cover data will create a general purpose embedding model suitable for a wide range of downstream applications using transfer learning, see Figure 1. This proposal describes the current state-of-the-art in sensor fusion, our progress towards a data corpus for training purposes, and our proposed experiment design for delivering DeepSentinel. 2 Sensor fusion SAR imagery carries complementary information to multispectral imagery and so is useful beyond its penetration of atmospheric conditions. Sentinel-1 SAR C-band backscatter is sensitive to moisture and surface types, and so has found applications in both natural environments (e.g. with classiﬁcation of forests[ 26] and crop types[ 22,29,21]), and the built human environment (e.g. road classiﬁcation [35]). These properties are complimentary to the multispectral data provided by Sentinel-2, which has sensors designed to detect aerosols, water vapour, chlorophyll, and visual spectra. Sensor fusion describes the complementary combination of data from multiple sources to improve inference quality beyond what would be possible from either sensor individually. Fusion of Sentinel-1 and Sentinel-2 imagery has been studied for cloud removal[ 7,12,2,19], synthetic imagery generation [14, 3, 9], and land cover classiﬁcation [31, 8, 30]. With the exception of [ 19], most of these studies obtain a limited corpus for a speciﬁc area of interest. With DeepSentinel, we propose a general\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nWe propose developing an extensible corpus of labelled and unlabelled Sentinel-1 and Sentinel-2 imagery for sensor fusion research. This corpus will be made publicly available along with supporting code. Our goal is to create a general-purpose semantic embedding model, DeepSentinel, which can be used for transfer learning applications and accelerate the impact of machine learning-enhanced earth observation on climate change mitigation.\n\nEarth observation plays a crucial role in measuring climate change impacts and risks. Medium-resolution satellites (10-30m spatial resolution) are being used to measure environmental impacts, financial risk, and estimate carbon dioxide emissions, methane emissions, and localise large fixed-capital assets. These satellites provide a globally exhaustive and unbiased view required by financial decision-makers. However, deploying analysis at this global scale can only be accomplished with the use of machine learning.\n\nClimate change risk applications are impaired by two challenges: atmospheric interference and a shortage of training labels. Atmospheric interference is not equally distributed around the planet, with excessive cloud cover making surface retrievals challenging in certain geographies. Many cloudy geographies are in the global south, where populations are vulnerable to climate change and conventional reported data is sparse. Financial institutions in the global north concentrate their risk exposure in these areas, and civil society groups must be vigilant to identify and respond to neocolonial practices.\n\nThe shortage of training labels for machine learning with earth observation data is well-documented. Land use and land cover data is available in Europe and the US, but data for the rest of the planet is sparse. These datasets include only broad categories of land use and land cover, and are not suitable for localising specific categories of industrial infrastructure. Financial institutions are just beginning to use geospatial data, leaving a large gap in the existence and availability of spatially-localised asset-level data.\n\nWe propose a general-purpose sensor fusion semantic embedding model to overcome these challenges. DeepSentinel will use sensor fusion of SAR Sentinel-1 data and multispectral Sentinel-2 data to provide latent space embeddings of surface conditions even in excessively cloudy conditions. Self-supervised pretraining followed by fine-tuning on land-use and land-cover data will create a general-purpose embedding model suitable for a wide range of downstream applications using transfer learning.\n\nSensor fusion combines complementary information from SAR and multispectral imagery. Sentinel-1 SAR C-band backscatter is sensitive to moisture and surface types, while Sentinel-2 provides data on aerosols, water vapour, chlorophyll, and visual spectra. Sensor fusion has been studied for cloud removal, synthetic imagery generation, and land cover classification. However, most studies have used a limited corpus for a specific area of interest. With DeepSentinel, we propose a general-purpose model that can be used for a wide range of applications."
}