{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" $50/tCO 2achieves carbon emissions re- ductions of up to 10%. Our results demonstrate scalability to larger problems than tackled in ex- isting literature, and indicate the potential for RL to contribute to decarbonising power systems. 1. Introduction and Related Work The power sector is the single largest contributor to global CO 2emissions (Ritchie & Roser, 2020) and requires rapid decarbonisation to achieve climate goals. One of the funda- mental problems in power systems operation is determining the on/off (commitment) schedules of generation to meet de- mand, the unit commitment (UC) problem. The UC problem is traditionally solved by mixed-integer linear programming (MILP), with a reserve constraint enforced to manage un- certainties. However, in high renewables systems, such deterministic methods are outperformed by stochastic for- mulations (Ruiz et al., 2009), but these have seen limited real-world application due to high computational costs and other practical challenges (Bertsimas et al., 2012). In this paper, we tackle the day-ahead UC problem with a combination of self-play reinforcement learning (RL) and traditional planning methods. A policy is trained by self- play RL in a power system environment, and applied at test time to reduce the branching factor of a search tree which 1UCL Energy Institute, London, UK. Correspondence to: Patrick de Mars <patrick.demars.14@ucl.ac.uk >. Tackling Climate Change with Machine Learning Workshop at ICML 2021.is solved by A* search (Russell & Norvig, 2009). We use ‘guided A* search’ to solve UC problems with stochastic demand and wind generation, using forecasts based on GB power system data. We consider power systems of 10, 20 and 30 generators, each with and without a carbon price to understand: (1) cost-competitiveness with an MILP bench- mark; (2) scalability to larger power systems; (3) the impact of carbon pricing on carbon emissions. Compared with MILP, guided A* search achieves lower operating costs and loss of load probability, with no loss of performance with increasing problem size. Implementing a carbon price shifts generation away from coal-ﬁred power stations towards less carbon intensive generation, reducing total carbon emissions by up to 10%. Our research contributes a novel and scalable approach applying RL to the UC problem, and shows signif- icant potential to learn complex operational strategies and outperform MILP approaches. Existing research in this area (Jasmin & TP, 2009; Dalal & Mannor, 2015; Jasmin et al., 2016; Navin & Sharma, 2019; Li et al., 2019) has focused on small numbers of generators, in part due to the combinatorial action space that limits the application of existing RL methods ‘out-of-the-box’. In the most similar research (Dalal & Mannor, 2015), tree search methods are applied to a system of 12 generators, which, to the best of our knowledge, is the largest prior study in this area. However, the problem considered is deterministic and does not consider generalisability to unseen problems. In subsequent related research, a larger power system is considered but the UC problem is simpliﬁed to a single commitment decision per day (Dalal et al., 2016). To the best of our knowledge, our research is unique in testing on unseen proﬁles and investigating carbon pricing. In the next section, the UC problem is formulated as a Markov Decision Process (MDP), suitable for RL methods. We present our methodology of guided A* search in Section 3, which we apply in experiments described in Section 4. We discuss our results and conclude the paper in Section 5. 2. Unit Commitment as an MDP To apply RL methods, we formulate the UC problem as an episodic MDP (Sutton & Barto, 2018), with episodes con- sisting of 48 decision periods reﬂecting half-hour market settlement periods. At each timestep, the agent receivesGuided A* Search for Scheduling Power Generation Under Uncertainty Figure 1. Example search tree representing the UC problem for a system of 3 generators. Nodes on the tree represent observations, comprised of demand and wind forecasts and generator up/down times. Actions are combinatorial decisions to commit or decommit generators at the next timestep. The step cost of traversing an edge is the expected operating cost. an observation consisting of the following components: (1) current generator up/down times ut; (2) demand forecast dt; (3) wind forecast wt. Demand and wind forecast errors xtandytare included in the state stbut unobserved by the agent. The errors are sampled from error distributions Xt andYt, which are modelled using with auto\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nThe cost of $50 per ton of CO2 achieves carbon emissions reductions of up to 10%. Our results demonstrate scalability to larger problems than those tackled in existing literature, and indicate the potential for reinforcement learning (RL) to contribute to decarbonizing power systems.\n\n1. Introduction and Related Work\n\nThe power sector is the single largest contributor to global CO2 emissions (Ritchie & Roser, 2020) and requires rapid decarbonization to achieve climate goals. One of the fundamental problems in power systems operation is determining the on/off (commitment) schedules of generation to meet demand, the unit commitment (UC) problem. The UC problem is traditionally solved by mixed-integer linear programming (MILP), with a reserve constraint enforced to manage uncertainties. However, in high renewables systems, such deterministic methods are outperformed by stochastic formulations (Ruiz et al., 2009), but these have seen limited real-world application due to high computational costs and other practical challenges (Bertsimas et al., 2012).\n\nIn this paper, we tackle the day-ahead UC problem with a combination of self-play reinforcement learning (RL) and traditional planning methods. A policy is trained by self-play RL in a power system environment, and applied at test time to reduce the branching factor of a search tree, which is solved by A* search (Russell & Norvig, 2009). We use 'guided A* search' to solve UC problems with stochastic demand and wind generation, using forecasts based on GB power system data. We consider power systems of 10, 20, and 30 generators, each with and without a carbon price to understand:\n\n(1) cost-competitiveness with an MILP benchmark;\n(2) scalability to larger power systems;\n(3) the impact of carbon pricing on carbon emissions.\n\nCompared with MILP, guided A* search achieves lower operating costs and loss of load probability, with no loss of performance with increasing problem size. Implementing a carbon price shifts generation away from coal-fired power stations towards less carbon-intensive generation, reducing total carbon emissions by up to 10%.\n\nOur research contributes a novel and scalable approach applying RL to the UC problem, and shows significant potential to learn complex operational strategies and outperform MILP approaches. Existing research in this area (Jasmin & TP, 2009; Dalal & Mannor, 2015; Jasmin et al., 2016; Navin & Sharma, 2019; Li et al., 2019) has focused on small numbers of generators, in part due to the combinatorial action space that limits the application of existing RL methods 'out-of-the-box'.\n\nIn the most similar research (Dalal & Mannor, 2015), tree search methods are applied to a system of 12 generators, which, to the best of our knowledge, is the largest prior study in this area. However, the problem considered is deterministic and does not consider generalizability to unseen problems. In subsequent related research, a larger power system is considered, but the UC problem is simplified to a single commitment decision per day (Dalal et al., 2016). To the best of our knowledge, our research is unique in testing on unseen profiles and investigating carbon pricing.\n\n2. Unit Commitment as a Markov Decision Process (MDP)\n\nTo apply RL methods, we formulate the UC problem as an episodic MDP (Sutton & Barto, 2018), with episodes consisting of 48 decision periods reflecting half-hour market settlement periods. At each timestep, the agent receives an observation consisting of the following components:\n\n(1) current generator up/down times;\n(2) demand forecast;\n(3) wind forecast.\n\nDemand and wind forecast errors are included in the state but unobserved by the agent. The errors are sampled from error distributions, which are modeled using auto-regressive models."
}