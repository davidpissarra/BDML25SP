{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"@gmail.com Nishant Balaji Electrical and Computer Engineering UC San Diego La Jolla, CA 926093 nibalaji@ucsd.eduCurt Schurgers Electrical and Computer Engineering UC San Diego La Jolla, CA 926093 cschurgers@eng.ucsd.edu Ryan Kastner Computer Science and Engineering UC San Diego La Jolla, CA 926093 kastner@eng.ucsd.eduMathias Tobler San Diego Zoo Wildlife Alliance Beckman Center for Conservation Research Escondido, CA 92027 MTobler@sdzwa.org Ian Ingram San Diego Zoo Wildlife Alliance Beckman Center for Conservation Research Escondido, CA 92027 iingram@sdzwa.org Abstract The potential of passive acoustic monitoring (PAM) as a method to reveal the consequences of climate change on the biodiversity that make up natural sound- scapes can be undermined by the discrepancy between the low barrier of entry to acquire large ﬁeld audio datasets and the higher barrier of acquiring reliable species level training, validation, and test subsets from the ﬁeld audio. These subsets from a deployment are often required to verify any machine learning models used to assist researchers in understanding the local biodiversity. Especially as many models convey promising results from various sources that may not translate to the collected ﬁeld audio. Labeling such datasets is a resource intensive process due to the lack of experts capable of identifying bioacoustics at a species level as well as the overwhelming size of many PAM audiosets. To address this challenge, we Tackling Climate Change with Machine Learning workshop at NeurIPS 2021have tested different sampling techniques on an audio dataset collected over a two- week long August audio array deployment on the Scripps Coastal Reserve (SCR) Biodiversity located adjacent to sandstone cliffs and the Paciﬁc Ocean in La Jolla, California. These sampling techniques involve creating four subsets using stratiﬁed random sampling, limiting samples to the daily bird vocalization peaks, and using a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) trained for bird presence/absence audio classiﬁcation. We found that a stratiﬁed random sample baseline only achieved a bird presence rate of 44% in contrast with a sample that randomly selected clips with high hybrid CNN-RNN predictions that were collected during bird activity peaks at dawn and dusk yielding a bird presence rate of 95%. The signiﬁcantly higher bird presence rate demonstrates how intelligent, machine learning-assisted selection of audio data can signiﬁcantly reduce the amount of time that domain experts listen to audio without vocalizations of interest while building a ground truth for machine learning models. 1 Introduction Passive acoustic monitoring (PAM) is a method of garnering an understanding of various ecosystems that involves deploying a large amount of audio recorders that autonomously collect audio clips from natural soundscapes over time. [ 11] Combining PAM with machine learning techniques has created a niche to better understand the impacts of climate change on many noisy indicator species that are too small for large scale monitoring via traditional biodiversity surveying techniques such as trapping, monitoring feeding sites, and camera trap arrays [13, 9, 15, 16, 7, 2, 12, 14]. Due to the wider availability of low-cost hardware ﬁt for PAM deployments [ 4] and open source pre-trained machine learning models [ 10,5,3] the barrier of entry for researchers breaking into the ecoacoustics [ 6] discipline is decreasing. However, it is very easy to be led on by promising results of pre-trained models on publicly available datasets that may not translate well when applied to noisier ﬁeld audio recordings. [ 1] A crucial step towards understanding the biodiversity of an ecosystem using PAM and machine learning requires generating species level ground truth labels on a subset of audio recordings from the ﬁeld for the purpose of testing and if necessary, validation and training of promising models. Creating ground truth requires a lot of time from the limited pool of experts capable of labeling audio data at a species level. For the sake of reducing the ﬁnancial and temporal costs of the research process, it is in the best interest of researchers to develop methods that make sure that the audio being delivered to experts for labeling, have a higher probability of containing vocalizations of interest. To meet this challenge, we have explored various methods to increase the probability of extracting bird vocalizations from a PAM ﬁeld deployment we conducted on the California coast. These methods involve a baseline stratiﬁed random sampling technique, sampling with knowledge of diurnal bird vocalization trends, as well as using a neural network model designed for audio event detection with low resource training sets [ 8] that has been encapsulated in the Github repository Microfaune with a pre-trained model for binary bird classiﬁcation. Microfaune can be decomposed into a convolutional neural network\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nNishant Balaji, Electrical and Computer Engineering, UC San Diego, La Jolla, CA 92609\nCurt Schurgers, Electrical and Computer Engineering, UC San Diego, La Jolla, CA 92609\nRyan Kastner, Computer Science and Engineering, UC San Diego, La Jolla, CA 92609\nMathias Tobler, San Diego Zoo Wildlife Alliance, Beckman Center for Conservation Research, Escondido, CA 92027\nIan Ingram, San Diego Zoo Wildlife Alliance, Beckman Center for Conservation Research, Escondido, CA 92027\n\nAbstract:\nPassive acoustic monitoring (PAM) can be undermined by the discrepancy between the low barrier to acquiring large field audio datasets and the higher barrier of acquiring reliable species-level training, validation, and test subsets from the field audio. These subsets are required to verify machine learning models used to understand local biodiversity. Labeling datasets is a resource-intensive process due to the lack of experts capable of identifying bioacoustics at a species level and the overwhelming size of many PAM datasets. To address this challenge, we tested different sampling techniques on an audio dataset collected over a two-week deployment on the Scripps Coastal Reserve Biodiversity in La Jolla, California.\n\nWe used stratiﬁed random sampling, sampling during daily bird vocalization peaks, and a hybrid convolutional neural network (CNN) and recurrent neural network (RNN) trained for bird presence/absence classification. Our results show that a stratiﬁed random sample baseline achieved a bird presence rate of 44%, while a sample with high hybrid CNN-RNN predictions collected during bird activity peaks at dawn and dusk yielded a bird presence rate of 95%. This significantly higher bird presence rate demonstrates how intelligent, machine learning-assisted selection of audio data can reduce the time domain experts spend listening to audio without vocalizations of interest while building a ground truth for machine learning models.\n\nIntroduction:\nPassive acoustic monitoring (PAM) is a method of understanding ecosystems by deploying audio recorders that collect audio clips from natural soundscapes over time. Combining PAM with machine learning techniques has created a niche to better understand the impacts of climate change on indicator species that are too small for large-scale monitoring via traditional biodiversity surveying techniques. Due to the availability of low-cost hardware and open-source pre-trained machine learning models, the barrier to entry for researchers in ecoacoustics is decreasing. However, promising results from pre-trained models on publicly available datasets may not translate well to noisier field audio recordings.\n\nA crucial step towards understanding biodiversity using PAM and machine learning requires generating species-level ground truth labels on a subset of audio recordings from the field. Creating ground truth requires a lot of time from the limited pool of experts capable of labeling audio data at a species level. To reduce the financial and temporal costs of research, it is essential to develop methods that increase the probability of extracting vocalizations of interest from field deployments. We explored various methods to increase the probability of extracting bird vocalizations from a PAM field deployment on the California coast, including stratiﬁed random sampling, sampling with knowledge of diurnal bird vocalization trends, and using a neural network model designed for audio event detection with low-resource training sets."
}