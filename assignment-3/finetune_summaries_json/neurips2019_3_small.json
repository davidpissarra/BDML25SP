{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\", primarily because accurate hydrological forecasts require models that are calibrated to past data. In this work we present a large-scale LSTM-based modeling approach that - by training on large data sets - learns a diversity of hydrological behaviors. Previous work shows that this model is more accurate than current state-of-the-art models, even when the LSTM-based approach operates out-of-sample and the latter in- sample. In this work, we show how this model can assess the sensitivity of the underlying systems with regard to extreme (high and low) ﬂows in individual watersheds over the continental US. 1 Introduction Floods and droughts affect more people than any other type of weather-related natural hazard [ 5], and the propensities for both are likely to increase under climate change [6, 19]. The most common strategy for assessing impacts of climate change on hydrologic systems uses models calibrated in individual catchments1against historical records [ 20]. This strategy neglects the fact that a change in climate also leads to changes in the catchment characteristics, which is not realistic under climate change and other anthropogenic inﬂuences [ 14]. Currently, the primary challenges are: (i) simplistic models [ 20,4], (ii) unreliable parameter ﬁtting [ 9], (iii) drastic performance degradation in large-scale (e.g. continental or global) modeling, [ 2] and (iv) not accounting for changing environmental conditions in the setup [13]. Recently Kratzert et al. [12] proposed an approach for large-scale hydrological simulation that outperforms a wide range of traditional models. It consists of an Long Short-Term Memory network (LSTM) [ 8] with a modiﬁed input gate, trained on meteorological time series data from hundreds of riverine systems, where static catchment characteristics are used to condition the model for a speciﬁc site. These characteristics comprise of topographic attributes (e.g. mean elevation, drainage area), soil properties (e.g. percentage of clay, soil conductivity), as well as climate and vegetation indices (e.g. mean annual precipitation, aridity, leaf area index). Furthermore, in previous publications Kratzert et al. [10] showed that the LSTM learns to model real hydrological processes (e.g., the amount of snow in a basin) in it’s memory cell states without training on any type of direct snow-related data (except total precipitation). This modeling of real hydrological processes provides at least some \u0003LIT AI Lab & Institute for Machine Learning, Johannes Kepler University Linz, Austria yDepartment of Geological Sciences, University of Alabama, Tuscaloosa, AL United States 1Acatchment (orbasin,watershed ) is the upstream area that drains to a certain point in a river. Workshop on Tackling Climate Change with Machine Learning 33rd Conference on Neural Information Process- ing Systems (NeurIPS 2019), Vancouver, Canada.conﬁdence that the LSTM learns some of the underlying physical process instead of just a simplistic mapping, e.g. on basis of spurious correlations. In other words, there exists a proof-of-concept that deep learning can transfer information about hydrologic processes and behaviors between basins, time and unobserved locations. This is revolu- tionary in the Hydrological Sciences, where the problem of Prediction in Ungauged Basins was the decadal problem of the International Association of Hydrological Sciences from 2003-2012 [ 18], and is generally considered to be unsolved [3]. Here we use the EA-LSTM [ 12] model to investigate which watersheds in the continuous USA have the largest sensitivities to climate-related forcings in extreme low-ﬂow and high-ﬂow periods. 2 Methods 2.1 Entity-Aware Long Short-Term Memory Network (EA-LSTM) The EA-LSTM [ 12] consists of an adapted LSTM cell, where static ( xs) and dynamic input input features ( xd) are used explicitly for different purposes: i=\u001b(Wixs+bi) (1) f[t] =\u001b(Wfxd[t] +Ufh[t\u00001] +bf) (2) g[t] = tanh( Wgxd[t] +Ugh[t\u00001] +bg) (3) o[t] =\u001b(Woxd[t] +Uoh[t\u00001] +bo) (4) c[t] =f[t]\fc[t\u00001] +i\fg[t] (5) h[t] =o[t]\ftanh(c[t]); (6) Heretis the time step ( 1\u0014t\u0014T),i[t],f[t]ando[t]are the input gate, forget gate, and output gate, respectively, g[t]is the cell input, h[t\u00001]is the recurrent input, c[t\u00001]the cell\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nAccurate hydrological forecasts require models that are calibrated to past data. In this work, we present a large-scale LSTM-based modeling approach that learns a diversity of hydrological behaviors by training on large datasets. Previous work has shown that this model is more accurate than current state-of-the-art models, even when operating out-of-sample and in-sample. We demonstrate how this model can assess the sensitivity of underlying systems to extreme (high and low) flows in individual watersheds over the continental US.\n\nFloods and droughts affect more people than any other type of weather-related natural hazard, and their propensities are likely to increase under climate change. The most common strategy for assessing the impacts of climate change on hydrologic systems uses models calibrated in individual catchments against historical records. However, this strategy neglects the fact that a change in climate also leads to changes in catchment characteristics, which is not realistic under climate change and other anthropogenic influences.\n\nThe primary challenges in current hydrological modeling are: (i) simplistic models, (ii) unreliable parameter fitting, (iii) drastic performance degradation in large-scale modeling, and (iv) not accounting for changing environmental conditions. Recently, Kratzert et al. proposed an approach for large-scale hydrological simulation that outperforms traditional models. It consists of a Long Short-Term Memory network (LSTM) with a modified input gate, trained on meteorological time series data from hundreds of riverine systems. The model is conditioned for a specific site using static catchment characteristics, including topographic attributes, soil properties, climate, and vegetation indices.\n\nIn previous publications, Kratzert et al. showed that the LSTM learns to model real hydrological processes, such as the amount of snow in a basin, without training on direct snow-related data. This modeling of real hydrological processes provides confidence that the LSTM learns underlying physical processes instead of simplistic mappings based on spurious correlations. This is revolutionary in the Hydrological Sciences, where the problem of predicting hydrologic processes in ungauged basins was a decadal problem from 2003-2012 and is generally considered unsolved.\n\nWe use the EA-LSTM model to investigate which watersheds in the continental US have the largest sensitivities to climate-related forcings in extreme low-flow and high-flow periods.\n\n2 Methods\n\n2.1 Entity-Aware Long Short-Term Memory Network (EA-LSTM)\n\nThe EA-LSTM consists of an adapted LSTM cell, where static and dynamic input features are used explicitly for different purposes. The equations for the EA-LSTM are:\n\ni[t] = σ(Wixs + bi)\nf[t] = σ(Wfxd[t] + Ufh[t-1] + bf)\ng[t] = tanh(Wgxd[t] + Ugh[t-1] + bg)\no[t] = σ(Woxd[t] + Uoh[t-1] + bo)\nc[t] = f[t] * c[t-1] + i * g[t]\nh[t] = o[t] * tanh(c[t])\n\nwhere i, f, and o are the input gate, forget gate, and output gate, respectively, g is the cell input, h[t-1] is the recurrent input, and c[t-1] is the cell state."
}