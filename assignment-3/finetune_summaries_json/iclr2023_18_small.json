{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" agents participating in DR programs. Un- fortunately, MARL algorithms do not naturally allow one to incorporate safety guarantees, preventing their real-world deployment. To meet safety constraints, we propose a safety layer that minimally adjusts each agent’s decisions. We in- vestigate the influence of using a reward function that reflects these safety adjust- ments. Results show that considering safety aspects in the reward during training improves both convergence speed and performance of the MARL agents in the investigated numerical experiments. 1 I NTRODUCTION The electrification of the heating and mobility sector entails both challenges and opportunities for power grid operation. Controllable electric loads can provide a source of flexibility for meeting the volatility in renewable generation, but coordination is required to take full advantage of this flexi- bility and avoid undesired demand peaks. Mechanisms to adjust the power consumption patterns of end customers for improving the reliability of power grid operation and decreasing operational costs are summarized under the term demand response (DR). For private households, DR mechanisms are usually price-based, meaning that electricity pricing schemes are used to incentivize a desired consumption behavior of customers acting as autonomous agents (Siano, 2014). Early research in this field focused on the day-ahead consumption scheduling of autonomous agents under dynamic pricing schemes (Mohsenian-Rad et al., 2010; Li et al., 2011), neglecting uncer- tainties in both consumer behavior and generation forecasting. For training autonomous agents in real-time DR settings, multi-agent reinforcement learning (MARL) offers a model-free and adap- tive framework with the capability of handling large amounts of data as well as complex nonlinear problems (V ´azquez-Canteli & Nagy, 2019). MARL has been applied successfully for reducing peak loads (V ´azquez-Canteli et al., 2020; Ebell & Pruckner, 2021; Bahrami et al., 2021) as well as op- erational costs (Christensen et al., 2020; Shojaeighadikolaei et al., 2021). Despite these promising results, practical deployment of MARL for DR is hindered by its inability to incorporate physical constraints such as storage capacities. Existing work addresses this problem using reward shaping (Kofinas et al., 2018), over-dimensioning components (V ´azquez-Canteli et al., 2020), or employing backup controllers on the component level (V ´azquez-Canteli et al., 2020; Bahrami et al., 2021). Of these options, only backup controllers can guarantee constraint satisfaction. However, previous work fails to investigate the influence of overriding decisions of the MARL algorithm. We address this research gap by incorporating a constraint violation penalty into the reward used during MARL training. Our proposed safety layer ensures minimal interference by using a shrinking horizon model-predictive control formulation (Ye & Kolmanovsky, 2022) to project each agent’s decision onto the feasible set defined by agent-specific constraints. The penalty is proportional to the necessary adjustment. We compare training with and without the penalty added to the reward and 1Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2023 use a perfect-knowledge day-ahead scheduling approach (Li et al., 2011) as a theoretical maximum to evaluate overall performance of the real-time MARL approach. 2 P ROBLEM FORMULATION We consider a group of Nhouseholds served by a single energy provider. Each household partici- pates in the DR program to optimize its own payoff. The goal of the energy provider is to minimize the operational cost as well as the aggregated discomfort. Since it cannot directly manage the house- holds’ power consumption, it issues dynamic prices to incentivize the households to adopt a socially optimal behavior. Next, we describe the case study and the perfect-knowledge solution approach from Li et al. (2011), which we use as a theoretical maximum for algorithm performance. Then, we formulate the real-time interactions as a partially observable Markov Game (POMG) to solve it using MARL. 2.1 C ASE STUDY Each household i∈ {1,..., N}has a non-shiftable base load ℓi,tcorresponding to household appli- ances (such as dishwashers), a battery, and a shiftable load corresponding to a heating, ventilation, and cooling system (HV AC). We consider a discrete-time setting with time interval ∆tand time horizon T. At each time step t, households can shape their power consumption by adjusting the bat- tery charge/discharge power pB i,tand the power consumed by the HV AC, denoted pAC i,t. The overall demand di,tof a household is obtained using the power balance equation di,t=ℓi,t+pAC i,t+pB i,t. (1) The energy stored in the battery, denoted xB\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and unnecessary data removed:\n\nAgents participating in demand response (DR) programs face a challenge: MARL algorithms do not inherently provide safety guarantees, hindering their real-world deployment. To address this, we propose a safety layer that minimally adjusts each agent's decisions. We investigate the impact of incorporating a reward function that reflects these safety adjustments. Our results show that considering safety aspects in the reward during training improves both convergence speed and performance of the MARL agents in our numerical experiments.\n\nThe electrification of the heating and mobility sector presents both challenges and opportunities for power grid operation. Controllable electric loads can provide flexibility for meeting the volatility in renewable generation, but coordination is required to take full advantage of this flexibility and avoid undesired demand peaks. Demand response mechanisms adjust power consumption patterns to improve power grid reliability and decrease operational costs. For private households, DR mechanisms are usually price-based, using electricity pricing schemes to incentivize desired consumption behavior.\n\nEarly research focused on day-ahead consumption scheduling under dynamic pricing schemes, neglecting uncertainties in consumer behavior and generation forecasting. For training autonomous agents in real-time DR settings, multi-agent reinforcement learning (MARL) offers a model-free and adaptive framework that can handle large amounts of data and complex nonlinear problems. MARL has been applied successfully for reducing peak loads and operational costs. However, its practical deployment for DR is hindered by its inability to incorporate physical constraints such as storage capacities.\n\nTo address this issue, we propose a safety layer that ensures minimal interference by using a shrinking horizon model-predictive control formulation to project each agent's decision onto the feasible set defined by agent-specific constraints. We incorporate a constraint violation penalty into the reward used during MARL training, proportional to the necessary adjustment. We compare training with and without the penalty added to the reward and use a perfect-knowledge day-ahead scheduling approach as a theoretical maximum to evaluate overall performance.\n\nWe consider a group of N households served by a single energy provider. Each household participates in the DR program to optimize its own payoff. The energy provider's goal is to minimize operational cost and aggregated discomfort. To achieve this, it issues dynamic prices to incentivize households to adopt a socially optimal behavior.\n\nWe describe a case study and a perfect-knowledge solution approach, which we use as a theoretical maximum for algorithm performance. We then formulate the real-time interactions as a partially observable Markov Game (POMG) to solve it using MARL.\n\nEach household has a non-shiftable base load corresponding to household appliances, a battery, and a shiftable load corresponding to a heating, ventilation, and cooling system (HVAC). We consider a discrete-time setting with time interval ∆t and time horizon T. At each time step t, households can shape their power consumption by adjusting the battery charge/discharge power and the power consumed by the HVAC. The overall demand of a household is obtained using the power balance equation di,t = ℓi,t + pAC i,t + pB i,t."
}