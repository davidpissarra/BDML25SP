{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" a fundamental problem for anticipating and mitigating the impacts of climate change. Recently, data-driven approaches for weather forecast- ing based on deep learning have shown great promise, achieving accuracies that are competitive with operational systems. However, those methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what truly contributes to their success. Here we introduce Stormer, a simple transformer model that achieves state-of-the-art performance on weather forecasting with minimal changes to the standard transformer back- bone. We identify the key components of Stormer through careful empirical anal- yses, including weather-specific embedding, randomized dynamics forecast, and pressure-weighted loss. At the core of Stormer is a randomized forecasting ob- jective that trains the model to forecast the weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a tar- get lead time and combine them to obtain better forecast accuracy. On Weather- Bench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormer’s favorable scaling properties, showing consistent improvements in forecast accuracy with in- creases in model size and training tokens. 1 M ETHODOLOGY We introduce Stormer, an effective deep learning model for weather forecasting. We focus on the simplicity of the architecture, and aim to show that such an architecture can achieve competitive forecast performances with a well-designed framework. We first present the overall training and inference procedure of Stormer, and proceed to describe the model architecture we implement in practice. We provide an in-depth discussion about the different design choices of Stormer in Ap- pendix D and empirically demonstrate the importance of each component in Appendix F.2. 1.1 T RAINING We train Stormer to forecast the weather dynamics ∆δt=Xδt−X0, which is the difference between two consecutive weather conditions, X0andXδt, across the time interval δt. A common practice in previous works (Keisler, 2022; Lam et al., 2023) is to use a small fixed value of δtsuch as 6 hours. However, as we show in Figure 6a, while small intervals tend to work well for short lead times, larger intervals excel at longer lead times (beyond 7 days) due to less error accumulation. Therefore, having a model that can produce forecasts at different intervals and combine them in an effective manner has the potential to improve the performance of single-interval models. This motivates ourrandomized dynamics forecasting objective, which trains Stormer to forecast the dynamics at 1Published as a workshop paper at “Tackling Climate Change with Machine Learning”, ICLR 2024 random intervals δtby conditioning on δt: L(θ) =Eδt∼P(δt),(X0,Xδt)∼D\u0002||fθ(X0, δt)−∆δt||2 2\u0003, (1) in which P(δt)is the distribution of the random interval. In our experiments, unless otherwise spec- ified, P(δt)is a uniform distribution over three values δt∼ U{ 6,12,24}. These three time intervals play an important role in atmospheric dynamics. The 6 and 12-hour values help to encourage the model to learn and resolve the diurnal cycle (day-night cycle), one of the most important oscillations in the atmosphere driving short-term dynamics (e.g., temperature over the course of a day). The 24- hour value filters the effects of the diurnal cycle and allows the model to learn longer, synoptic-scale dynamics which are particularly important for medium-range weather forecasting (Holton, 2004). 1.1.1 P RESSURE -WEIGHTED LOSS Due to the large number of variables being predicted, we use a physics-based weighting function to weigh variables near the surface higher. Since each variable lies on a specific pressure level, we can use pressure as a proxy for the density of the atmosphere at each level. This weighting allows the model to prioritize near-surface variables, which are important for weather forecasting and have the most societal impact. The final objective function that we use for training is: L(θ) =E 1 V HWVX v=1HX i=1WX j=1w(v)L(i)(b∆vij δt−∆vij δt)2 . (2) The expectation is over δt, X 0, and Xδtwhich we omit for notational simplicity. In this equation, w(v)is the weight of variable v, and L(i)is the latitude-weighting factor. 1.1.2 M ULTI -STEP FINETUNING To produce forecasts at a lead time beyond the training\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nA fundamental problem in anticipating and mitigating the impacts of climate change is the need for accurate weather forecasting. Recent data-driven approaches based on deep learning have shown great promise, achieving accuracies competitive with operational systems. However, these methods often employ complex, customized architectures without sufficient ablation analysis, making it difficult to understand what contributes to their success. We introduce Stormer, a simple transformer model that achieves state-of-the-art performance on weather forecasting with minimal modifications to the standard transformer backbone. We identify the key components of Stormer through careful empirical analysis, including weather-specific embedding, randomized dynamics forecasting, and pressure-weighted loss.\n\nAt the core of Stormer is a randomized forecasting objective that trains the model to forecast weather dynamics over varying time intervals. During inference, this allows us to produce multiple forecasts for a target lead time and combine them to obtain better forecast accuracy. On Weather-Bench 2, Stormer performs competitively at short to medium-range forecasts and outperforms current methods beyond 7 days, while requiring orders-of-magnitude less training data and compute. Additionally, we demonstrate Stormer's favorable scaling properties, showing consistent improvements in forecast accuracy with increases in model size and training tokens.\n\nWe introduce Stormer, an effective deep learning model for weather forecasting. We focus on the simplicity of the architecture and aim to show that such an architecture can achieve competitive forecast performances with a well-designed framework. We present the overall training and inference procedure of Stormer and describe the model architecture implemented in practice. We provide an in-depth discussion about the different design choices of Stormer in Appendix D and empirically demonstrate the importance of each component in Appendix F.2.\n\nWe train Stormer to forecast the weather dynamics, which is the difference between two consecutive weather conditions, across a time interval. A common practice in previous works is to use a small fixed value of the time interval, such as 6 hours. However, we show that larger intervals excel at longer lead times (beyond 7 days) due to less error accumulation. Therefore, having a model that can produce forecasts at different intervals and combine them effectively has the potential to improve the performance of single-interval models. This motivates our randomized dynamics forecasting objective, which trains Stormer to forecast the dynamics at random intervals by conditioning on the interval.\n\nOur objective function is:\n\nL(θ) = E[||fθ(X0, δt) - ∆δt||2]\n\nwhere P(δt) is the distribution of the random interval. In our experiments, P(δt) is a uniform distribution over three values: δt ∼ U{6, 12, 24}. These three time intervals play an important role in atmospheric dynamics. The 6 and 12-hour values help to encourage the model to learn and resolve the diurnal cycle (day-night cycle), one of the most important oscillations in the atmosphere driving short-term dynamics. The 24-hour value filters the effects of the diurnal cycle and allows the model to learn longer, synoptic-scale dynamics, which are particularly important for medium-range weather forecasting.\n\nDue to the large number of variables being predicted, we use a physics-based weighting function to weigh variables near the surface higher. Since each variable lies on a specific pressure level, we can use pressure as a proxy for the density of the atmosphere at each level. This weighting allows the model to prioritize near-surface variables, which are important for weather forecasting and have the most societal impact. The final objective function that we use for training is:\n\nL(θ) = E[1/V ∑Vv=1 ∑H i=1 ∑W j=1 w(v) L(i)(b∆vij δt - ∆vij δt)2]\n\nwhere w(v) is the weight of variable v, and L(i) is the latitude-weighting factor.\n\nTo produce forecasts at a lead time beyond the training data, we use multi-step fine-tuning."
}