{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" to cities and lives, especially as they grow more intense due to climate change effects. By implementing the Bayesian conﬁdence interval using dropout in an RNN, we im- prove the actionability of the predictions, for example by estimating the areas to evacuate in the landfall region. We used an RNN to predict the trajectory of the storms at 6-hour intervals. We used latitude, longitude, windspeed, and pressure features from a Statistical Hurricane Intensity Prediction Scheme (SHIPS) dataset of about 500 tropical storms in the Atlantic Ocean. Our results show how neural network dropout values affect our predictions and Bayesian intervals. 1 I NTRODUCTION We look at a dataset of tropical storm data in the Atlantic Ocean from 1982 to 2017 and perform deep learning predictions with uncertainty bounds on trajectories of the storms. The result of these storms, particularly the strongest ones called hurricanes, deﬁned as having wind speeds exceeding 74 miles per hour, can be devastating because of their strong winds and heavy precipitation that can cause dangerous tides. Tropical storms can cause major environmental disasters when they reach land, such as the 2005 Hurricane Katrina that resulted in over 850 deaths and caused major economic damage and the 2012 Hurricane Sandy that caused almost $70 billion in damage across much of the eastern United States, with peak winds of 115 mph (Hurricane). According to the National Oceanic and Atmospheric Administration (NOAA), it is likely that global warming will cause hurricanes in the upcoming century to be more intense by 1 to 10% globally (with higher peak winds and lower central pressures), which will result in a higher proportion of more severe storms (NOAA). Historically, hurricane trajectory predictions have used statistical methods that can be limiting be- cause of the nonlinearity and complexity of atmospheric systems. Deep learning techniques and speciﬁcally recurrent neural networks (RNNs) have grown in popularity in recent years as a strong method for approaching prediction problems because of the ability to extract important features and relationships from complex high-dimensional data, especially for forecasting and classiﬁca- tion (McDermott & Wikle, 2019). We implemented a number of improvements over previous deep learning prediction work by Alemany et al. (2019), including predicting exact storm locations in lat- itude/longitude instead of a grid value and using a prediction window that uses all previous hurricane data rather than a ﬁxed-size sliding window. While hurricane trajectory predictions have seen improvements recently (SHIPS), we build on pre- vious work to include a fundamental uncertainty measure in the prediction for the ﬁrst time as part of a neural network framework. The uncertainty measure is especially valuable for understanding a deﬁned location range rather than only a point estimate, which is important for evacuation and safety/preparation purposes. The National Hurricane Center (NHC) builds their uncertainty cone such that2 3of historical forecast errors over the previous 5 years fall within the circle, whereas we use a rigorous Bayesian prediction model to build our conﬁdence intervals. 1Published at ICLR 2020 Workshop: Tackling Climate Change with Machine Learning 2 Q UANTIFYING UNCERTAINTY WITH A BAYESIAN RNN Gal & Ghahramani (2016a;b); Gal (2016) showed that we can use a deep learning model that uses dropout to model uncertainty by using the dropout in both the training and testing passes, without the need for an inference framework (Appendix B). This technique of interpreting dropout as a Bayesian approximation of a deep Gaussian process provides a simple method for estimating the conﬁdence of a neural network output, in addition to the standard point estimate output. The key idea with Bayesian RNN dropout is that weights have probability distributions rather than ﬁxed point estimates. We perform multiple forward passes of the network, each time with a new set of weights, which result from a different set of dropouts during the prediction phase. The authors showed that if dropout is seen as a variational Monte Carlo approximation to a Bayesian posterior, then the natural way to apply it to recurrent layers is to generate a dropout mask that zeroes out both feedforward and recurrent connections for each training sequence, but to keep the same mask for each time step in the sequence as a form of variational inference (see Figures 1 and 2). This varies from the na ¨ıve way of applying dropout to RNNs, which would generate new dropout masks for each input sample, regardless of which time sequence it was from. Figure 1: Standard RNN model Figure 2: Variational Bayes RNN model By enabling dropout during the testing phase, every forward pass with a given input will result in a different output. These non-deterministic predictions can be interpreted as samples from a probabilistic distribution, i.e., a Bayesian approximation. By applying dropout to all the weight layers in a neural network, we are essentially drawing each weight from a Bernou\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nTo cities and lives, especially as they grow more intense due to climate change effects. By implementing the Bayesian confidence interval using dropout in a recurrent neural network (RNN), we improve the actionability of the predictions, for example, by estimating the areas to evacuate in the landfall region. We used an RNN to predict the trajectory of the storms at 6-hour intervals. We used latitude, longitude, wind speed, and pressure features from a Statistical Hurricane Intensity Prediction Scheme (SHIPS) dataset of about 500 tropical storms in the Atlantic Ocean.\n\nWe look at a dataset of tropical storm data in the Atlantic Ocean from 1982 to 2017 and perform deep learning predictions with uncertainty bounds on trajectories of the storms. The result of these storms, particularly the strongest ones called hurricanes, defined as having wind speeds exceeding 74 miles per hour, can be devastating due to their strong winds and heavy precipitation that can cause dangerous tides. Tropical storms can cause major environmental disasters when they reach land, such as the 2005 Hurricane Katrina that resulted in over 850 deaths and caused major economic damage, and the 2012 Hurricane Sandy that caused almost $70 billion in damage across much of the eastern United States, with peak winds of 115 mph.\n\nAccording to the National Oceanic and Atmospheric Administration (NOAA), it is likely that global warming will cause hurricanes in the upcoming century to be more intense by 1 to 10% globally, with higher peak winds and lower central pressures, resulting in a higher proportion of more severe storms. Historically, hurricane trajectory predictions have used statistical methods that can be limiting due to the nonlinearity and complexity of atmospheric systems. Deep learning techniques, specifically recurrent neural networks (RNNs), have grown in popularity in recent years as a strong method for approaching prediction problems due to their ability to extract important features and relationships from complex high-dimensional data, especially for forecasting and classification.\n\nWe implemented a number of improvements over previous deep learning prediction work, including predicting exact storm locations in latitude/longitude instead of a grid value and using a prediction window that uses all previous hurricane data rather than a fixed-size sliding window. While hurricane trajectory predictions have seen improvements recently, we build on previous work to include a fundamental uncertainty measure in the prediction for the first time as part of a neural network framework. The uncertainty measure is especially valuable for understanding a defined location range rather than only a point estimate, which is important for evacuation and safety/preparation purposes.\n\nThe National Hurricane Center (NHC) builds their uncertainty cone such that historical forecast errors over the previous 5 years fall within the circle, whereas we use a rigorous Bayesian prediction model to build our confidence intervals.\n\nQuantifying Uncertainty with a Bayesian RNN\n\nGal and Ghahramani (2016a, b) and Gal (2016) showed that we can use a deep learning model that uses dropout to model uncertainty by using the dropout in both the training and testing passes, without the need for an inference framework. This technique of interpreting dropout as a Bayesian approximation of a deep Gaussian process provides a simple method for estimating the confidence of a neural network output, in addition to the standard point estimate output. The key idea with Bayesian RNN dropout is that weights have probability distributions rather than fixed point estimates.\n\nWe perform multiple forward passes of the network, each time with a new set of weights, which result from a different set of dropouts during the prediction phase. The authors showed that if dropout is seen as a variational Monte Carlo approximation to a Bayesian posterior, then the natural way to apply it to recurrent layers is to generate a dropout mask that zeroes out both feedforward and recurrent connections for each training sequence, but to keep the same mask for each time step in the sequence as a form of variational inference.\n\nBy enabling dropout during the testing phase, every forward pass with a given input will result in a different output. These non-deterministic predictions can be interpreted as samples from a probabilistic distribution, i.e., a Bayesian approximation. By applying dropout to all the weight layers in a neural network, we are essentially drawing each weight from a Bernoulli distribution."
}