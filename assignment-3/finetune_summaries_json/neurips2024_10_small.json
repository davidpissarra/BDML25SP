{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"aboschi, cullen.bash} @hpe.com Abstract Reducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs). In this paper, we introduce Green-DCC, which proposes Reinforcement Learning- based hierarchical controller techniques to dynamically optimize temporal and geographical workload distribution between data centers that belong to the same DCC. The environment models non-uniform external weather, carbon intensity, computing resources, cooling capabilities, and dynamic bandwidth costs, which provide constraints and interdependencies. We adapted and evaluated various reinforcement learning approaches, comparing their aggregate carbon emissions across the DCC, demonstrating Green-DCC’s effectiveness for controlling and testing advanced data center control algorithms for sustainability. Workload -+ Data Center  Level (Local) Con  Weather...  Grid Carbon IA!rl. l  Intensity ~ flm -+  dJ;n ~,-4. ~ \"'ft  ~ Geographic Shift  of Workload )  Data Center Data Center Cluster  Level (Global) Control  Data Center  Figure 1: Spatio temporal hierarchical workload distribution. 1 Introduction Major cloud service providers and corporations often have geographically distributed data centers, enabling enterprise users to schedule large-scale batch workloads across various locations. These decisions are influenced by factors such as weather, grid carbon intensity, workload characteristics, compute instance availability, latency requirements, and energy costs. This paper aims to develop a hierarchical machine learning framework for distributing large-scale batch workloads from mul- tiple enterprise users across data centers to reduce sustainability metrics like carbon emissions as ∗Corresponding author †These authors contributed equally Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.shown in Figure 1. The dynamic nature of workload requirements, weather patterns, carbon in- tensity fluctuations, energy pricing, and system states with workload reassignments renders static solutions ineffective, motivating the use of reinforcement learning (RL) algorithms for our workload distribution framework. Our main contributions include hierarchical control using RL algorithms to simultaneously shift load in real-time within and across data centers for sustainability goals and a customizable data center cluster simulator. 2 Related Work Geographical workload distribution Ahmad et al. (2020) strategies vary based on factors like work- load type, energy sources, and data center resources. Prior approaches, such as Stochastic Cost Minimization Zinober (1994); Guo et al. (2013); Chang et al. (2019) and Mixed Integer Linear Programming Khalil et al. (2019), often rely on static solutions or relax constraints, limiting their effectiveness in dynamic environments. Our work addresses these limitations by incorporating a holistic, reinforcement learning-based approach that considers bandwidth, delay models, carbon footprint, and heterogeneous data center specifications in an online setting. By prioritizing sustain- ability alongside performance, our work provides a more comprehensive approach to optimizing data center operations, focusing on reducing environmental impact. Also, holistic optimization of the data centers and other reinforcement learning optimizations have been explored in Naug et al. (2024); Sarkar et al. (2024a,b,c, 2023a,b,c,d); Naug et al. (2023a,b). 3 System Description Our proposed system optimizes workload distribution across a geographically distributed Data Centers cluster (DCC), leveraging a hierarchical multi-agent framework. This system is structured to dynamically adjust to real-time operational conditions, effectively managing workload shifts to reduce carbon footprint. It can be extended to reduce energy consumption, water usage, energy cost, etc. 3.1 Hierarchical Framework The architecture employs a hierarchical framework to optimize workload distribution across geograph- ically dispersed data centers. It comprises a top-level (global) agent and multiple lower-level (local) agents. The top-level agent manages global decision-making, strategically distributing workloads between data centers based on factors like weather, and carbon intensity, which can be extended to energy cost. Figure 2 illustrates this geographic load shifting strategy, where tasks are dynamically moved between data centers to optimize resource utilization and reduce environmental impact. Lower- level agents handle localized operations within each data center, focusing on temporal workload scheduling. Figure 2 also shows the temporal load shifting strategy, where non-critical tasks are deferred using a Deferred Task Queue (DTQ) for processing during more favorable conditions. This combined approach of geographic and temporal load shifting enables our approach to optimize data center operations’ sustainability and efficiency at both global and local levels. 3.2 Dynamic Bandwidth Cost Model We use a dynamic model that defines the cost of transmitting data across data centers in a DCC. Traditional models primarily use geographical distance as a basis for calculating transmission costs. However, such models often lead to trivial solutions and suboptimal resource utilization, as they may promote workload migration to data centers with minimal carbon intensity, disregarding other operational variables and costs, leading to resource underutilization Guo et al. (2013); Wei et al. (2022). To address these challenges, we introduce a cost model, bij(t), that\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nReducing the environmental impact of cloud computing requires efficient workload distribution across geographically dispersed Data Center Clusters (DCCs). This paper introduces Green-DCC, a reinforcement learning-based hierarchical controller technique to dynamically optimize temporal and geographical workload distribution between data centers within the same DCC. The environment models non-uniform external weather, carbon intensity, computing resources, cooling capabilities, and dynamic bandwidth costs, which provide constraints and interdependencies.\n\nWe adapted and evaluated various reinforcement learning approaches, comparing their aggregate carbon emissions across the DCC, demonstrating Green-DCC's effectiveness in controlling and testing advanced data center control algorithms for sustainability. The proposed system optimizes workload distribution across a geographically distributed DCC, leveraging a hierarchical multi-agent framework that dynamically adjusts to real-time operational conditions to reduce carbon footprint.\n\n**Figure 1: Spatio-temporal hierarchical workload distribution**\n\nMajor cloud service providers and corporations often have geographically distributed data centers, enabling enterprise users to schedule large-scale batch workloads across various locations. These decisions are influenced by factors such as weather, grid carbon intensity, workload characteristics, compute instance availability, latency requirements, and energy costs. This paper aims to develop a hierarchical machine learning framework for distributing large-scale batch workloads from multiple enterprise users across data centers to reduce sustainability metrics like carbon emissions.\n\nThe dynamic nature of workload requirements, weather patterns, carbon intensity fluctuations, energy pricing, and system states with workload reassignments renders static solutions ineffective, motivating the use of reinforcement learning (RL) algorithms for our workload distribution framework. Our main contributions include:\n\n1. Hierarchical control using RL algorithms to simultaneously shift load in real-time within and across data centers for sustainability goals.\n2. A customizable data center cluster simulator.\n\n**2. Related Work**\n\nGeographical workload distribution strategies vary based on factors like workload type, energy sources, and data center resources. Prior approaches, such as Stochastic Cost Minimization and Mixed Integer Linear Programming, often rely on static solutions or relax constraints, limiting their effectiveness in dynamic environments. Our work addresses these limitations by incorporating a holistic, reinforcement learning-based approach that considers bandwidth, delay models, carbon footprint, and heterogeneous data center specifications in an online setting.\n\nBy prioritizing sustainability alongside performance, our work provides a more comprehensive approach to optimizing data center operations, focusing on reducing environmental impact. Other reinforcement learning optimizations have been explored in various studies.\n\n**3. System Description**\n\nOur proposed system optimizes workload distribution across a geographically distributed DCC, leveraging a hierarchical multi-agent framework that dynamically adjusts to real-time operational conditions to reduce carbon footprint. It can be extended to reduce energy consumption, water usage, energy cost, etc.\n\n**3.1 Hierarchical Framework**\n\nThe architecture employs a hierarchical framework to optimize workload distribution across geographically dispersed data centers. It comprises a top-level (global) agent and multiple lower-level (local) agents. The top-level agent manages global decision-making, strategically distributing workloads between data centers based on factors like weather and carbon intensity. Figure 2 illustrates this geographic load shifting strategy, where tasks are dynamically moved between data centers to optimize resource utilization and reduce environmental impact.\n\nLower-level agents handle localized operations within each data center, focusing on temporal workload scheduling. Figure 2 also shows the temporal load shifting strategy, where non-critical tasks are deferred using a Deferred Task Queue (DTQ) for processing during more favorable conditions.\n\n**3.2 Dynamic Bandwidth Cost Model**\n\nWe use a dynamic model that defines the cost of transmitting data across data centers in a DCC. Traditional models primarily use geographical distance as a basis for calculating transmission costs, which may lead to trivial solutions and suboptimal resource utilization. To address these challenges, we introduce a cost model, bij(t), that considers other operational variables and costs."
}