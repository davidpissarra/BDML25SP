{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" the surface using an autonomous surface vehicle (ASV). Increase in the ocean temperatures has resulted in widespread coral bleaching at an ever-increasing rate (Hoegh-Guldberg, 1999) (Fig. 1a). Improved monitoring would enhance the currently poor understanding of the spatial and temporal dynamics of coral bleaching. Since we are sampling from the surface, higher information gain is provided in shallower regions where visibility is better. We present an anytime (Zilberstein & Russell, 1993) adap- tive sampling technique that generates paths to efﬁciently measure and then mathematically model a scalar ﬁeld by performing non-uniform measurements in a given region of interest. In particular, the class of scalar ﬁeld we are interested is some physical or virtual parameter that varies with location, such as depth of the sea ﬂoor or algae blooms or suspended particles in air. As the measurements are collected at each sampling location, we can compute an estimate of the large-scale variation of the phenomenon of interest. We compute a sampling path that minimizes the ex- pected time to accurately model the phenomenon of interest by visiting high information regions (hotspots) using non- myopic path generation based on reinforcement learning. Exhaustively sampling each point of an unknown survey region (Xu et al., 2011; Choset & Pignon, 1998) can be 1Mobile Robotics Lab (MRL), McGill University, Montreal, Canada2Amsterdam Machine Learning Lab (AMLAB), University of Amsterdam, Amsterdam, Netherlands. (a) Effect of global warming on Corals (Vevers) (b) System overview of our sampling approach. Figure 1. tedious and impractical if the survey space is large and/or the phenomenon of interest has only a few regions with important information ( hotspots ). Also it has been observed that sampling rates far below the Nyquist rate can still be information preserving (Venkataramani & Bresler, 2000). This is the key guiding principle behind active and non- uniform sampling (Manjanna & Dudek, 2017; Low et al., 2008; Rahimi et al., 2005; Sadat et al., 2015). In our approach (Fig.1b), a continuous two-dimensional sampling region is discretized into uniform grid-cells, such that the robot’s position xcan be represented by a pair of integers x2Z2. Each grid-cell (i;j)is assigned a score q(i;j)indicating the expected goodness of the visual data in that cell. The goal is to maximize the total accumulated scoreJover a trajectory \u001cwithin a ﬁxed amount of time T. To specify the robot’s behavior we use a parametrized policy\u0019\u0012(s;a) =p(ajs;\u0012)that maps the current state sof sampling to a distribution over possible actions a. Our aim will be to automatically ﬁnd good parameters \u0012, after which the policy can be deployed without additional training on new problems.Policy Search with Non-uniform State Representations for Environmental Sampling (a)  (b)  (c)  (d)  (e) Figure 2. Robot-centric feature space aggregations. (a) Uniform-grid feature aggregation. (b) 4-feature aggregation. (c) 8-feature aggregation. (d) 24-feature aggregation. (e) Multi-resolution feature aggregation. 2. Technical Approach Our algorithm gets trained with a generic score-map ( q) generated by the satellite data from areas that exemplify the target environments, for example images of coral reefs (Fig.1b). The system is trained to achieve paths that prefer- entially cover hotspots at the earlier stages of exploration. These learned parameters then deﬁne a policy\u0019(in the sense of reinforcement learning) that is then used on the satellite image or any other sensor map of the target coral reef (Fig.1b) to generate an explicit action plan. Thus, the policy does not need to be re-trained for each new reef map. This property is a key feature of our approach. In our approach, we formalize the sampling problem as a Markov Decision Process (MDP). We consider the state sto include the position of the robot xas well as the map qcon- taining the per-location score for the visual data, s= (x;q). The action space Aconsists of four actions (move North, East, South, or West) and the transitions are deterministic. Once the visual data at the current cell (i;j)is sampled, the score q(i;j)is reduced to 0(Manjanna et al., 2018). The discounted reward function is deﬁned as  tq(x), with a discount factor 0\u0014 \u00141encouraging the robot to sample cells with high scores in early time steps t. 2.1. Policy Gradient Method In policy gradient methods\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nThe surface of the ocean is being monitored using an autonomous surface vehicle (ASV). Rising ocean temperatures have led to widespread coral bleaching at an increasing rate (Hoegh-Guldberg, 1999). Improved monitoring would enhance the current understanding of the spatial and temporal dynamics of coral bleaching. Since sampling is done from the surface, more information is gained in shallower regions where visibility is better. We present an adaptive sampling technique that efficiently measures and mathematically models a scalar field by performing non-uniform measurements in a given region of interest. The class of scalar fields we are interested in includes physical or virtual parameters that vary with location, such as sea floor depth, algae blooms, or suspended particles in the air.\n\nAs measurements are collected at each sampling location, an estimate of the large-scale variation of the phenomenon of interest can be computed. We compute a sampling path that minimizes the expected time to accurately model the phenomenon of interest by visiting high information regions (hotspots) using non-myopic path generation based on reinforcement learning. Exhaustively sampling each point in an unknown survey region can be tedious and impractical if the survey space is large and/or the phenomenon of interest has only a few regions with important information (hotspots). It has been observed that sampling rates below the Nyquist rate can still be information-preserving (Venkataramani & Bresler, 2000). This is the key guiding principle behind active and non-uniform sampling (Manjanna & Dudek, 2017; Low et al., 2008; Rahimi et al., 2005; Sadat et al., 2015).\n\nIn our approach, a continuous two-dimensional sampling region is discretized into uniform grid cells, where the robot's position can be represented by a pair of integers. Each grid cell is assigned a score indicating the expected goodness of the visual data in that cell. The goal is to maximize the total accumulated score over a trajectory within a fixed amount of time. To specify the robot's behavior, we use a parametrized policy that maps the current state of sampling to a distribution over possible actions. Our aim is to automatically find good parameters for the policy, after which it can be deployed without additional training on new problems.\n\nOur algorithm is trained with a generic score map generated by satellite data from areas that exemplify the target environments, such as images of coral reefs. The system is trained to preferentially cover hotspots at the earlier stages of exploration. These learned parameters define a policy that is then used on the satellite image or any other sensor map of the target coral reef to generate an explicit action plan. This property is a key feature of our approach. In our approach, we formalize the sampling problem as a Markov Decision Process (MDP). We consider the state to include the position of the robot and the map containing the per-location score for the visual data. The action space consists of four actions (move North, East, South, or West), and the transitions are deterministic. Once the visual data at the current cell is sampled, the score is reduced to 0. The discounted reward function is defined as the score at time t, with a discount factor encouraging the robot to sample cells with high scores in early time steps.\n\nWe use the policy gradient method to optimize the policy."
}