{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" curat- ing a massive and diverse dataset, which is an expensive and time-consuming process that can span from weeks to months. Searching for relevant examples in over 40 petabytes of un- labelled data requires researchers to manually hunt for such images, much like finding a needle in a haystack. We present a no-code end-to-end pipeline, Curator1, which dramatically minimizes the time taken to curate an exhaustive labeled dataset. Curator is able to search massive amounts of unla- belled data by combining self-supervision, scalable nearest neighbor search, and active learning to learn and differentiate image representations. The pipeline can also be readily ap- plied to solve problems across different domains. Overall, the pipeline makes it practical for researchers to go from just one reference image to a comprehensive dataset in a diminutive span of time. Introduction One of the initial steps for a scientific study related to cli- mate change and natural disasters, including wildfires, oil spills, hurricanes, dust storms, etc., involves scientists gath- ering a large number of relevant examples from satellite imagery. Locating an exhaustive set of examples requires painstakingly inspecting 197 million square miles of satel- lite imagery each day across more than 20 years. While such an effort can produce a valuable trove of data, the act of man- ually searching is laborious, expensive, and often imprac- tical - grounding many scientific studies before they could ever take off. While one of the approaches to solving this is building an image similarity search, several challenges arise when ap- plying similarity search to raw satellite imagery: • The data is unlabelled, preventing attempts to train con- ventional supervised models which could have generated meaningful representations. *These authors contributed equally. Copyright © 2022, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. 1We release all instructions, trained models and code for Cura- tor : https://www.github.com/spaceml-org• Pretrained ImageNet (Deng et al. 2009) models fail to transfer representations and generalize to this data - es- pecially for larger areas that are usually without sharp edges including clouds as well as multi-spectral data. • Climate phenomena can have vastly different physical sizes - from few miles for wildfires to 300+ miles for hurricanes. • Vast data imbalances inherently present in the data. • The engineering challenges that come with the sheer scale of our data. We propose Curator, a modular toolkit that aims to take a user from one reference image to an exhaustive set of rel- evant examples for any large unlabelled image data source. It solves the core issue of data inaccessibility by discovering relevant samples from sizeable collections while minimizing human labeling effort. This pipeline combines several indi- vidually tested, high-performance components built for spe- cific tasks - from downloading data, training self-supervised models, large-scale similarity search, active learning, and crowd-sourced labeling. This open source project, built by citizen scientists (Koul et al. 2020), aims to enable a re- searcher to accomplish all of this without writing a single line of code or possessing any prerequisite AI knowledge. This ease of usage further reduces barriers to entry and hope- fully catalyzes research involving climate science. Previous Methods We demonstrate a specific use case of our pipeline that aims to solve a previously unsolved problem - building a curated dataset for any natural phenomenon by intelligently index- ing over 897 satellite imagery sources via the Global Im- agery Browse Services (GIBS) portal. To the best of our knowledge, the widely accepted solution in practice which also acts as the baseline is a manual approach involving vi- sual inspection of data from the GIBS portal for multiple layers over a region for a period of time, where each layer provides information overlays based on science disciplines, hazards, and disaster categories, downloading the requisite data, and then manually annotating it. Research in this field includes manually labeling, semi-supervised learning like in (Kim et al. 2019), or using text mining and NLP techniques to extract images and their labels from multiple large data stores. Our method involves annotating a negligible numberFigure 1: The Curator pipeline of images in comparison, and then relies on active learning to generate weak labels for the rest of the dataset. Pipeline Our key goal is to let a scientist use a single query image (say of a climate event) to ultimately identify every potential ex- ample of the same category in a large image collection (like satellite imagery). A scalable way to do this is by evaluating each image with a classifier tuned to the user’s needs. Train- ing such a supervised classifier requires enough positive and negative examples for training. Getting to this training set can be achieved in four steps - (1)training a self-supervised model on unlabeled data, in order to learn semantically rel- evant representations. (2)generating embeddings\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nCurating a massive and diverse dataset is an expensive and time-consuming process that can take weeks to months. Searching for relevant examples in over 40 petabytes of unlabeled data requires researchers to manually hunt for such images, much like finding a needle in a haystack. We present a no-code end-to-end pipeline, Curator, which dramatically minimizes the time taken to curate an exhaustive labeled dataset. Curator searches massive amounts of unlabeled data by combining self-supervision, scalable nearest neighbor search, and active learning to learn and differentiate image representations. The pipeline can be applied to solve problems across different domains, making it practical for researchers to go from one reference image to a comprehensive dataset in a short span of time.\n\nOne of the initial steps for a scientific study related to climate change and natural disasters involves gathering a large number of relevant examples from satellite imagery. Locating an exhaustive set of examples requires inspecting 197 million square miles of satellite imagery each day across more than 20 years. While this effort can produce valuable data, the manual search process is laborious, expensive, and often impractical, grounding many scientific studies before they can take off.\n\nApplying similarity search to raw satellite imagery poses several challenges:\n\n* The data is unlabeled, preventing the use of conventional supervised models that could generate meaningful representations.\n* Pretrained ImageNet models fail to transfer representations and generalize to this data, especially for larger areas without sharp edges, including clouds and multi-spectral data.\n* Climate phenomena can have vastly different physical sizes, from a few miles for wildfires to over 300 miles for hurricanes.\n* Vast data imbalances are inherently present in the data.\n* The sheer scale of the data presents engineering challenges.\n\nWe propose Curator, a modular toolkit that takes a user from one reference image to an exhaustive set of relevant examples for any large unlabeled image data source. It solves the core issue of data inaccessibility by discovering relevant samples from sizeable collections while minimizing human labeling effort. The pipeline combines several individually tested, high-performance components for specific tasks, including downloading data, training self-supervised models, large-scale similarity search, active learning, and crowd-sourced labeling. This open-source project, built by citizen scientists, enables researchers to accomplish all of this without writing a single line of code or possessing any prerequisite AI knowledge, reducing barriers to entry and hopefully catalyzing research involving climate science.\n\nWe demonstrate a specific use case of our pipeline that aims to solve a previously unsolved problem: building a curated dataset for any natural phenomenon by intelligently indexing over 897 satellite imagery sources via the Global Imagery Browse Services (GIBS) portal. The widely accepted solution in practice is a manual approach involving visual inspection of data from the GIBS portal for multiple layers over a region for a period of time, downloading the requisite data, and then manually annotating it. Research in this field includes manually labeling, semi-supervised learning, or using text mining and NLP techniques to extract images and their labels from multiple large data stores. Our method involves annotating a negligible number of images and then relying on active learning to generate weak labels for the rest of the dataset.\n\nOur pipeline consists of four steps:\n\n1. Training a self-supervised model on unlabeled data to learn semantically relevant representations.\n2. Generating embeddings for the images in the dataset.\n3. Evaluating each image with a classifier tuned to the user's needs.\n4. Using active learning to generate weak labels for the rest of the dataset."
}