{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" ride-hailing EV agent, with the goal of reducing costs and emissions while increasing transporta- tion service provided. We introduce a data-driven simulation of a ride-hailing EV agent that provides transportation service and charges energy at congested charging infrastructure. We then formulate a test case for the sequential driving and charging decision making problem of the agent and apply DRL to optimize the agent’s decision making policy. We evaluate the performance against hand-written policies and show that our agent learns to act competitively without any prior knowledge. 1 Introduction The transportation sector is responsible for a signiﬁcant share of all CO 2emissions from human activity, representing 37% of emissions produced by California [ 1] and 29% of U.S. emissions [ 5]. Looking toward the future, the U.S. Department of Energy research shows that the proliferation of new forms of mobility, such as connected and autonomous vehicles (CA Vs), could result in a 200% increase in energy consumption in the U.S. transportation sector by 2050 relative to their baseline [14]. The state of California has identiﬁed transportation electriﬁcation in tandem with transitioning to carbon free sources of electric power generation as a key strategy for reducing emissions from the transportation sector [1]. However, electric vehicles (EV) face many obstacles to reaching high adoption levels, especially for use in ride-hailing for transportation network companies (TNCs) such as Lyft and Uber or in future CA V based services. EVs have less driving range and longer refueling times than internal combustion powered vehicles, as well as limited charging infrastructure. A ride-hailing EV driver should consider many spatially and temporally varying factors when deciding when and where to charge. This includes time-varying electric energy prices and emissions, ride-hailing surge pricing, demand for transportation service, trafﬁc conditions, and queues at charging stations. Each of these items can exhibit strong seasonality with time of day, geographic patterns, and randomness, creating opportunities to optimize timing and location of charging. An optimal driving and charging strategy (known as a policy ) could help ride-hailing EVs to charge at the times of day when electric energy costs and emissions are lowest, to reduce time spent waiting in queues, and maximize a driver’s revenues, reducing the economic barrier to adoption of EVs in ride-hailing and CA V ﬂeets. Such a policy could be implemented as a decision support tool for human drivers, or as part of a CA V ﬂeet management system. In this feasibility study, we focus on the case of optimizing the policy of a single agent as a ﬁrst step, with its environment taken as exogenous and given. 33rd NeurIPS 2019, Workshop on Tackling Climate Change with Machine Learning, Vancouver, Canada.2 Approach & Results To investigate the potential to optimize policies for operation of ride-hailing EVs, we developed a data-driven simulation of a ride-hailing EV driving for a TNC. The simulation includes modules representing the TNC, EV, the network of EV charging stations, and the electric grid. The TNC randomly generates requested trips depending on the time of day and the EV’s current location. Trip information includes destination, distance driven, time elapsed while driving, and transportation revenue earned. The probabilistic model of trips is estimated from the open source New York City taxicab dataset for the year 20151. In our simulation, we consider trips within Manhattan, and have discretized the map into a grid of 120 zones. The EV tracks the battery state of charge, depleting the battery when driving or recharging the battery when charging, and has a battery capacity of 100kWh. The EV charging station network deﬁnes the locations of charging stations and models exogenous EV use of charging stations with an assumed pattern for queuing wait times. Charging stations are assumed to have a ﬁxed charging power of 100kW. The electric grid determines the energy prices and CO 2emissions produced per energy unit charged, depending on the time of day. California values are used for electric energy prices2and electric grid emissions3. The driving and charging decision making of a ride-hailing EV can be formulated as a Markov Decision Process (MDP) [ 9]. In the MDP framework, an agent receives some observation about the state of their environment, chooses an action, and receives some reward. Solving an MDP is equivalent to ﬁnding the decision making policy that maximizes an expected reward over the lifetime of the agent. Our simulation environment is based on OpenAI Gym [ 3] with a discrete action space, where the EV agent chooses between charging the vehicle or accepting a new ride at every step. The agent’s observation space is a vector consisting of the EV battery level, time of day, expected battery usage if the agent were to perform a ride, expected charging cost if the agent were to charge, expected emissions produced by choosing to charge\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nThe goal of our ride-hailing electric vehicle (EV) agent is to reduce costs and emissions while increasing transportation service. We introduce a data-driven simulation of a ride-hailing EV agent that provides transportation service and charges energy at congested charging infrastructure. We then formulate a test case for the sequential driving and charging decision-making problem of the agent and apply deep reinforcement learning (DRL) to optimize the agent's decision-making policy. We evaluate the performance against hand-written policies and show that our agent learns to act competitively without prior knowledge.\n\nThe transportation sector is responsible for a significant share of all CO2 emissions from human activity, representing 37% of emissions produced by California and 29% of U.S. emissions. The U.S. Department of Energy research shows that the proliferation of new forms of mobility, such as connected and autonomous vehicles, could result in a 200% increase in energy consumption in the U.S. transportation sector by 2050 relative to the baseline. The state of California has identified transportation electrification in tandem with transitioning to carbon-free sources of electric power generation as a key strategy for reducing emissions from the transportation sector. However, electric vehicles face many obstacles to reaching high adoption levels, especially for use in ride-hailing for transportation network companies (TNCs) such as Lyft and Uber or in future connected and autonomous vehicle (CAV) based services. Electric vehicles have less driving range and longer refueling times than internal combustion-powered vehicles, as well as limited charging infrastructure. A ride-hailing EV driver should consider many spatially and temporally varying factors when deciding when and where to charge, including time-varying electric energy prices and emissions, ride-hailing surge pricing, demand for transportation service, traffic conditions, and queues at charging stations. Each of these factors can exhibit strong seasonality with time of day, geographic patterns, and randomness, creating opportunities to optimize timing and location of charging.\n\nAn optimal driving and charging strategy (known as a policy) could help ride-hailing EVs to charge at the times of day when electric energy costs and emissions are lowest, to reduce time spent waiting in queues, and maximize a driver's revenues, reducing the economic barrier to adoption of EVs in ride-hailing and CAV fleets. Such a policy could be implemented as a decision support tool for human drivers, or as part of a CAV fleet management system. In this feasibility study, we focus on the case of optimizing the policy of a single agent as a first step, with its environment taken as exogenous and given.\n\nTo investigate the potential to optimize policies for operation of ride-hailing EVs, we developed a data-driven simulation of a ride-hailing EV driving for a TNC. The simulation includes modules representing the TNC, EV, the network of EV charging stations, and the electric grid. The TNC randomly generates requested trips depending on the time of day and the EV's current location. Trip information includes destination, distance driven, time elapsed while driving, and transportation revenue earned. The probabilistic model of trips is estimated from the open-source New York City taxicab dataset for the year 2015. In our simulation, we consider trips within Manhattan, and have discretized the map into a grid of 120 zones. The EV tracks the battery state of charge, depleting the battery when driving or recharging the battery when charging, and has a battery capacity of 100 kWh. The EV charging station network defines the locations of charging stations and models exogenous EV use of charging stations with an assumed pattern for queuing wait times. Charging stations are assumed to have a fixed charging power of 100 kW. The electric grid determines the energy prices and CO2 emissions produced per energy unit charged, depending on the time of day. California values are used for electric energy prices and electric grid emissions. The driving and charging decision-making of a ride-hailing EV can be formulated as a Markov Decision Process (MDP). In the MDP framework, an agent receives some observation about the state of their environment, chooses an action, and receives some reward. Solving an MDP is equivalent to finding the decision-making policy that maximizes an expected reward over the lifetime of the agent. Our simulation environment is based on OpenAI Gym with a discrete action space, where the EV agent chooses between charging the vehicle or accepting a new ride at every step. The agent's observation space is a vector consisting of the EV battery level, time of day, expected battery usage if the agent were to perform a ride, expected charging cost if the agent were to charge, and expected emissions produced by choosing to charge."
}