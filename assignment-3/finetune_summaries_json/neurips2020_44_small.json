{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" Learning (FL) have emerged. Perhaps unexpectedly, FL in particular is starting to be deployed at a global scale by companies that must adhere to new legal demands and policies originating from governments and the civil society for privacy protection. However, the potential environmental impact related to FL remains unclear and unexplored. This paper offers the ﬁrst-ever systematic study of the carbon footprint of FL. First, we propose a rigorous model to quantify the carbon footprint, hence facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. Our ﬁndings show FL, despite being slower to converge, can be a greener technology than data center GPUs. Finally, we highlight and connect the reported results to the future challenges and trends in FL to reduce its environmental impact, including algorithms efﬁciency, hardware capabilities, and stronger industry transparency. 1 Introduction Atmospheric concentrations of carbon dioxide, methane, and nitrous oxide are at unprecedented levels not seen in the last 800;000years [ 1], and are extremely likely to have been the dominant cause of the observed global warming since the mid-20thcentury [ 2,3]. Unfortunately, deep learning (DL) algorithms keep growing in complexity, and numerous “state-of-the-art\" models continue to emerge, each requiring a substantial amount of computational energy, resulting in clear environmental costs [ 4]. However, such a trend is not going to end soon as [ 5] shows that the amount of compute used by machine learning training has grown by more than 300;000\u0002from 2012 to 2018. This is an observation that forces us to seriously consider the carbon footprint of deep learning methods. The data centers that enable DL research and commercial operations are not often accompanied by visual signs of pollution, but they are still responsible for signiﬁcant carbon footprint. Each year data centers use 200TWh energy, which accounts for 0:3%of global CO 2emissions. In comparison, the entire information and communications technology ecosystem accounts for only 2%[6]. Fortunately, alternatives to data center-based DL (and other forms of machine learning) are emerging. The most prominent of these to date is Federated Learning (FL) proposed by [ 7]. Under FL, the training of models primarily occurs across a large number of typically user-owned-and-controlled personal devices, such as smartphones. Devices collaboratively learn a shared prediction model but do so without uploading to a data center any of the locally stored raw sensitive data. While FL is still a maturing technology, it is already being used by millions of users on a daily basis; for example, Google uses FL to train models for: predictive keyboard, device setting recommendation, and hot keyword personalization on phones [8]. Tackling Climate Change with Machine Learning workshop at NeurIPS 2020.Whilst the carbon footprint for centralized learning has been studied in many previous works [ 9–12], the energy consumption and carbon footprint related to FL remain unexplored. To this extent, this paper proposes to overcome this issue by giving a ﬁrst look into the carbon analysis of FL. SOTA results in deep learning are usually determined by standard accuracy metrics, such as the accuracy of a given model, while energy efﬁciency or privacy concerns are often overlooked. Whilst accuracy remains crucial, we hope to encourage researchers to also focus on other metrics that are in line with the increasing interest of the civil society for global warming. By quantifying carbon emissions for FL, we encourage the integration of the released CO 2as a crucial metric to the FL deployment. In this paper, we ﬁrstly provide a ﬁrst-of-its-kind quantitative CO 2emissions estimation method for FL (Section 2). Then a carbon sensitivity analysis is conducted with this method on real FL hardware with the CIFAR10 dataset (Section 3). Furthermore, we provide a comprehensive analysis and discussion of the results to highlight the challenges and future research directions to develop an environmentally-friendly federated learning. 2 Quantifying CO 2emissions Two major steps can be followed to quantify the environmental cost of training deep learning models either in data centers or on edge. First, we perform an analysis of the energy required by the method (Section 2.1), mostly accounting for the total amount of energy consumed by the hardware. Then, the latter amount is converted to CO 2emissions based on geographical locations (Section 2.2). Currently we are not accounting for energy of WAN networking overhead of either centralized or FL, and will investigate this in future research. 2.1 Total Energy Consumption First, we need to consider the energy consumption coming from GPU and CPU, which can be measured by sampling GPU and CPU power consumption at training time [ 13]. Alternatively, we can use the ofﬁcial hardware power speciﬁcation or TDP, assuming a full GPU utilization. It is not realistic as a GPU is rarely used\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nLearning (FL) has emerged as a significant technology. Notably, FL is being deployed at a global scale by companies that must adhere to new legal demands and policies for privacy protection. However, the potential environmental impact of FL remains unclear and unexplored. This paper offers the first systematic study of the carbon footprint of FL. First, we propose a rigorous model to quantify the carbon footprint, facilitating the investigation of the relationship between FL design and carbon emissions. Then, we compare the carbon footprint of FL to traditional centralized learning. Our findings show that FL, despite being slower to converge, can be a greener technology than data center GPUs. Finally, we highlight and connect the reported results to the future challenges and trends in FL to reduce its environmental impact, including algorithm efficiency, hardware capabilities, and industry transparency.\n\nAtmospheric concentrations of carbon dioxide, methane, and nitrous oxide are at unprecedented levels, not seen in the last 800,000 years. Deep learning (DL) algorithms continue to grow in complexity, resulting in significant environmental costs. The amount of compute used by machine learning training has grown by more than 300,000 from 2012 to 2018. This trend forces us to consider the carbon footprint of deep learning methods. Data centers that enable DL research and commercial operations are responsible for significant carbon footprint, using 200 TWh of energy annually, which accounts for 0.3% of global CO2 emissions. Alternatives to data center-based DL are emerging, with Federated Learning (FL) being the most prominent. Under FL, models are trained across a large number of user-owned and controlled personal devices, such as smartphones, without uploading sensitive data to a data center. While FL is still a maturing technology, it is already being used by millions of users daily, for example, Google uses FL to train models for predictive keyboard, device setting recommendation, and hot keyword personalization on phones.\n\nThe carbon footprint of centralized learning has been studied in previous works, but the energy consumption and carbon footprint related to FL remain unexplored. This paper proposes to overcome this issue by providing a first look into the carbon analysis of FL. Standard accuracy metrics, such as model accuracy, are often prioritized over energy efficiency or privacy concerns. We aim to encourage researchers to focus on other metrics that align with the increasing interest in global warming. By quantifying carbon emissions for FL, we encourage the integration of CO2 emissions as a crucial metric in FL deployment.\n\nWe provide a quantitative CO2 emissions estimation method for FL (Section 2) and conduct a carbon sensitivity analysis on real FL hardware with the CIFAR10 dataset (Section 3). We also provide a comprehensive analysis and discussion of the results to highlight the challenges and future research directions to develop an environmentally-friendly federated learning.\n\nTwo major steps can be followed to quantify the environmental cost of training deep learning models: analyzing the energy required by the method and converting it to CO2 emissions based on geographical locations. We are not currently accounting for energy overhead of WAN networking in either centralized or FL, and will investigate this in future research.\n\nFirst, we consider the energy consumption coming from GPU and CPU, which can be measured by sampling power consumption at training time or using official hardware power specifications. However, it is not realistic to assume full GPU utilization, as GPUs are rarely used at maximum capacity."
}