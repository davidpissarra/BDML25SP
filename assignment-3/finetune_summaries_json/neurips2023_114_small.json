{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" provides vector embeddings summarizing the characteristics of a given location for convenient usage in diverse downstream tasks. We show that SatCLIP embeddings, pretrained on multi-spectral Sentinel-2 satellite data, can be used for various predictive out-of-domain tasks, including temperature predic- tion and animal recognition in imagery, and outperform existing competing ap- proaches. SatCLIP embeddings also prove helpful in overcoming geographic do- main shift. This demonstrates the potential of general-purpose location encoders and opens the door to learning meaningful representations of our planet from the vast, varied, and largely untapped modalities of geospatial data. 1 Introduction Much of the world’s data is geospatial. From images taken with a cellphone to the movement tra- jectories of taxis, different modalities live in the same geometric space: planet Earth. Geographic features, the characteristics describing any location on our planet, are commonly used in climate- related predictive modeling tasks, e.g., by using satellite imagery for crop yield prediction [9, 4]. Much of the existing work in geospatial machine learning does not directly encode spatial infor- mation as an input in the modeling process, even though this information is often readily available. Instead, indirect contextual information is used, such as images taken close to the location. New ap- proaches aim to represent relevant features of locations directly in an implicit neural representation by embedding contextual information in location encoder model weights [8, 17, 12]. The location information within georeferenced images lends itself conveniently to contrastive, self-supervised pretraining objectives: given a dataset that includes satellite images and their corresponding loca- tions, we can devise a pretraining task that matches images to locations or more precisely, image embeddings to location embeddings. This is analogous to the text-image pretraining deployed in the popular CLIP model [10]. Satellite imagery can be freely acquired for all land mass of our Planet at regular intervals, e.g. using the Sentinel-2 satellite. This enables pretraining a location encoder which, for any given location on the planet, returns contextual vectors encoding the characteristics of this location captured by satellite imagery. Location representations obtained from this encoder can be deployed in a range of different downstream tasks, as we show in this paper. We first outline the intuition for pretrain- ing general-purpose geographic location encoders and deploying them in real-world tasks (see also Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.[40.91, -75.53] [23.31, 19.18] [-3.01, -63.54] [lon, lat] [lon, lat][lon, lat]image encoder location encodercontrastive learning objectiveimg. emb.loc. emb. location encoder [lon, lat] [lon, lat][lon, lat] down- stream learnerdownstream objective pretraining downstream task loc. emb.global locationsdownstream locations location encoderFigure 1: Pretraining (left) to downstream task deployment (right) pipeline of general-purpose, global-coverage geographic location encoders. Trained using large, unlabeled geographic data (e.g., satellite imagery), the learned location encoder can then be queried for contextual embed- dings at downstream locations and–in combination with other relevant features–used for geospatial predictive modeling tasks. Figure 1). We then introduce a new model from this family: SatCLIP, a pretrained geographic loca- tion encoder leveraging recent advances in geospatial coordinate encoding with spherical harmonics functions [12]. We train SatCLIP using a new dataset, S2-100K, compiled from uniformly dis- tributed Sentinel-2 satellite imagery. Lastly, we compare SatCLIP to existing location encoders on different real-world tasks, highlighting how SatCLIP embeddings are highly effective for predictive modeling across geospatial tasks. 2 Approach 2.1 Pretrained geographic location encoders The inputs to a geographic location encoder are longitude/latitude coordinate pairs ci= [λi, ϕi]at locations iand on the spherical surface S. For each location i, we have corresponding contextual data, e.g. an image Ii∈Rm×n×cwithcchannels. The this context describes the characteristics of that location, in the case of SatCLIP we use satellite images. We now define two encoders, a location encoder Enc loc(ci, θloc) :S2→Rdthat takes in 2-dimensional coordinates and returns a d-dimensional latent embedding and a context encoder Enc cont(xi, θcont) :Rm→Rdthat takes in images Iiand returns a d-dimensional latent embedding. We can then define a loss that matches context vector embeddings and their correpsonding geoloca- tion embeddings and contrasts them to others in a training batch. This contrastive self-supervised learning transfers location clues from the context encoder to the location encoder weights during pretraining. The goal is for the location encoder and context encoder to both capture the\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nSatCLIP embeddings provide a convenient way to summarize the characteristics of a given location, making them useful for various downstream tasks. We demonstrate that SatCLIP embeddings, pretrained on multi-spectral Sentinel-2 satellite data, can be used for tasks such as temperature prediction and animal recognition in imagery, and outperform existing approaches. SatCLIP embeddings also help overcome geographic domain shift. This shows the potential of general-purpose location encoders and opens the door to learning meaningful representations of our planet from geospatial data.\n\nMuch of the world's data is geospatial, including images taken with cellphones and taxi movement trajectories. Geographic features, which describe any location on our planet, are commonly used in climate-related predictive modeling tasks, such as using satellite imagery for crop yield prediction. However, much of the existing work in geospatial machine learning does not directly encode spatial information as input in the modeling process, even though this information is often readily available. Instead, indirect contextual information is used, such as images taken close to the location. New approaches aim to represent relevant features of locations directly in an implicit neural representation by embedding contextual information in location encoder model weights.\n\nThe location information within georeferenced images lends itself conveniently to contrastive, self-supervised pretraining objectives. Given a dataset that includes satellite images and their corresponding locations, we can devise a pretraining task that matches images to locations or, more precisely, image embeddings to location embeddings. This is analogous to the text-image pretraining deployed in the popular CLIP model. Satellite imagery can be freely acquired for all landmass of our planet at regular intervals, using the Sentinel-2 satellite. This enables pretraining a location encoder that, for any given location on the planet, returns contextual vectors encoding the characteristics of this location captured by satellite imagery.\n\nLocation representations obtained from this encoder can be deployed in a range of different downstream tasks, as we show in this paper. We first outline the intuition for pretraining general-purpose geographic location encoders and deploying them in real-world tasks. We then introduce a new model from this family: SatCLIP, a pretrained geographic location encoder leveraging recent advances in geospatial coordinate encoding with spherical harmonics functions. We train SatCLIP using a new dataset, S2-100K, compiled from uniformly distributed Sentinel-2 satellite imagery. Lastly, we compare SatCLIP to existing location encoders on different real-world tasks, highlighting how SatCLIP embeddings are highly effective for predictive modeling across geospatial tasks.\n\n2 Approach\n\n2.1 Pretrained geographic location encoders\n\nThe inputs to a geographic location encoder are longitude/latitude coordinate pairs, ci = [λi, ϕi], at locations i on the spherical surface S. For each location i, we have corresponding contextual data, such as an image Ii ∈ Rm×n×c with c channels. The context describes the characteristics of that location, in the case of SatCLIP, we use satellite images. We define two encoders: a location encoder Encloc(ci, θloc) : S2 → Rd that takes in 2-dimensional coordinates and returns a d-dimensional latent embedding, and a context encoder Enccont(xi, θcont) : Rm → Rd that takes in images Ii and returns a d-dimensional latent embedding. We can then define a loss that matches context vector embeddings and their corresponding location embeddings and contrasts them to others in a training batch. This contrastive self-supervised learning transfers location clues from the context encoder to the location encoder weights during pretraining. The goal is for the location encoder and context encoder to both capture the characteristics of a location."
}