{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" diverse buildings. This paper introduces HV AC-DPT, a Decision-Pretrained Transformer using in-context Reinforcement Learning (RL) for multi-zone HV AC control. HV AC-DPT frames HV AC control as a sequential prediction task, training a causal transformer on inter- action histories generated by diverse RL agents. This approach enables HV AC-DPT to refine its policy in-context, without modifying network parameters, allowing for deployment across different buildings without the need for additional training or data collection. HV AC-DPT reduces energy consumption in unseen buildings by 45% compared to the baseline controller, offering a scalable and effective approach to mitigating the increasing environmental impact of HV AC systems. 1 Introduction and related work Advanced controllers have the potential to significantly reduce HV AC energy consumption [ 4], but most buildings continue to rely on inefficient, rule-based systems. Although various model-based, data-driven, and learning-based HV AC control strategies have been proposed [ 4,5], it remains a significant challenge to scale these methods across diverse building types. Model Predictive Control is limited by its reliance on precise and building-specific models, while RL requires extensive training, lasting months or years [ 6], which often leads to suboptimal performance and occupant discomfort during the learning phase [ 7]. RL also suffers from severe sample inefficiency, demanding significant amounts of sensor data and requiring retraining for each new building. Even with transfer learning, significant data collection and customisation is still needed to address the variability in building structures and thermal dynamics between the buildings used for training and new target buildings [8, 9]. The transformer architecture [ 10] has been widely adopted in key areas of machine learning. One major feature of transformers is in-context learning, which makes it possible for them to adapt to new tasks after extensive pretraining [ 11]. Recent research, such as the Decision-Pretrained Transformer (DPT) by Lee et al. [12] and Algorithm Distillation by Laskin et al. [13], effectively uses transformer- based in-context learning for sequential decision-making. These methods predict actions based on a query state and historical environment dynamics without the need for weight updates after the initial pretraining phase. Additionally, recent work demonstrates that transformers pretrained on diverse datasets can generalise to new RL tasks in-context, offering a promising approach for extracting Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.generalist policies from offline RL data [ 14,15,16]. Nevertheless, the application of in-context RL to HV AC control remains unexplored. In response, we introduce HV AC-DPT, a pretrained decision transformer that uses in-context RL to optimise HV AC systems across multiple building zones without requiring prior data or additional training for new buildings. HV AC-DPT overcomes the limitations of existing control methods by enabling scalable, data-efficient, and generalisable deployment across diverse building types, removing the need for retraining and pre-deployment data collection in new environments. In a year-long evaluation using EnergyPlus [ 17], HV AC-DPT reduced HV AC energy consumption by 45% compared to baseline operations, demonstrating its transformative potential to reduce the carbon footprint of building operations. Figure 1: Schematic overview of the proposed methodology. 2 Problem definition Table 1: State variables and actions for each agent. State Unit Zone mean temperature °C Zone mean humidity % Zone occupancy Binary Outdoor temperature °C Solar radiation W Hour of the day Integer Action Unit V A V minimum damper position %HV AC systems consist of one or more air han- dling units (AHUs) and variable air volume (V A V) systems, as illustrated in Figure 3. Opti- mising HV AC control can be framed as a sequen- tial decision-making problem, where an agent interacts with the building, adjusts controls (e.g., V A V actuators), and receives rewards to learn control policies. While a single agent could manage the entire building, this approach limits policy adaptability to buildings with different state-action spaces, such as those with varying numbers of V A V sys- tems. Consequently, we model HV AC control as a multi-agent reinforcement learning (MARL) task, where each agent controls a single zone, enabling independent management across zones, similar to the approach in [6]. The multi-agent Markov decision process is defined as a tuple (N;S;Ai;Ri;T;H), where Ndenotes the number of agents, Sis the state space, Aiis the action space for agent i,R:S×A→∆(R)is the reward function, T:S×A→∆(S)is the transition function, and His the horizon. The state space S, observed by all agents, includes six sensor readings, detailed in Table 1. Each agent’s action space Aicorresponds to the minimum damper position in their V A V system, ranging from 0 (closed) to 1 (fully open). The reward Rifor\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nThis paper introduces HV AC-DPT, a decision-pretrained transformer using in-context reinforcement learning (RL) for multi-zone HV AC control. HV AC-DPT frames HV AC control as a sequential prediction task, training a causal transformer on interaction histories generated by diverse RL agents. This approach enables HV AC-DPT to refine its policy in-context, without modifying network parameters, allowing for deployment across different buildings without the need for additional training or data collection. HV AC-DPT reduces energy consumption in unseen buildings by 45% compared to the baseline controller, offering a scalable and effective approach to mitigating the increasing environmental impact of HV AC systems.\n\nAdvanced controllers have the potential to significantly reduce HV AC energy consumption, but most buildings continue to rely on inefficient, rule-based systems. Although various model-based, data-driven, and learning-based HV AC control strategies have been proposed, it remains a significant challenge to scale these methods across diverse building types. Model Predictive Control is limited by its reliance on precise and building-specific models, while RL requires extensive training, lasting months or years, which often leads to suboptimal performance and occupant discomfort during the learning phase. RL also suffers from severe sample inefficiency, demanding significant amounts of sensor data and requiring retraining for each new building. Even with transfer learning, significant data collection and customization is still needed to address the variability in building structures and thermal dynamics between the buildings used for training and new target buildings.\n\nThe transformer architecture has been widely adopted in key areas of machine learning. One major feature of transformers is in-context learning, which makes it possible for them to adapt to new tasks after extensive pretraining. Recent research, such as the Decision-Pretrained Transformer (DPT) by Lee et al. and Algorithm Distillation by Laskin et al., effectively uses transformer-based in-context learning for sequential decision-making. These methods predict actions based on a query state and historical environment dynamics without the need for weight updates after the initial pretraining phase. Additionally, recent work demonstrates that transformers pretrained on diverse datasets can generalize to new RL tasks in-context, offering a promising approach for extracting generalist policies from offline RL data.\n\nHowever, the application of in-context RL to HV AC control remains unexplored. In response, we introduce HV AC-DPT, a pretrained decision transformer that uses in-context RL to optimize HV AC systems across multiple building zones without requiring prior data or additional training for new buildings. HV AC-DPT overcomes the limitations of existing control methods by enabling scalable, data-efficient, and generalizable deployment across diverse building types, removing the need for retraining and pre-deployment data collection in new environments. In a year-long evaluation using EnergyPlus, HV AC-DPT reduced HV AC energy consumption by 45% compared to baseline operations, demonstrating its transformative potential to reduce the carbon footprint of building operations.\n\nHV AC systems consist of one or more air handling units (AHUs) and variable air volume (VAV) systems. Optimizing HV AC control can be framed as a sequential decision-making problem, where an agent interacts with the building, adjusts controls (e.g., VAV actuators), and receives rewards to learn control policies. We model HV AC control as a multi-agent reinforcement learning (MARL) task, where each agent controls a single zone, enabling independent management across zones. The multi-agent Markov decision process is defined as a tuple (N, S, A, R, T, H), where N denotes the number of agents, S is the state space, A is the action space for agent i, R is the reward function, T is the transition function, and H is the horizon. The state space S includes six sensor readings, including zone mean temperature, zone mean humidity, zone occupancy, outdoor temperature, solar radiation, and hour of the day. Each agent's action space A corresponds to the minimum damper position in their VAV system, ranging from 0 (closed) to 1 (fully open). The reward function R for each agent is based on the energy consumption and occupant comfort."
}