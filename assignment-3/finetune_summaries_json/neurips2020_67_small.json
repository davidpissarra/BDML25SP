{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"old@bf.uzh.ch Abstract We introduce CLIMATE -FEVER, a new publicly available dataset for veriﬁcation of climate change-related claims. By providing a dataset for the research community, we aim to facilitate and encourage work on improving algorithms for retrieving evidential support for climate-speciﬁc claims, addressing the underlying language understanding challenges, and ultimately help alleviate the impact of misinforma- tion on climate change. We adapt the methodology of FEVER [1], the largest dataset of artiﬁcially designed claims, to real-life claims collected from the Internet. While during this process, we could rely on the expertise of renowned climate scientists, it turned out to be no easy task. We discuss the surprising, subtle complexity of modeling real-world climate-related claims within the FEVER framework, which we believe provides a valuable challenge for general natural language understanding. We hope that our work will mark the beginning of a new exciting long-term joint effort by the climate science and AIcommunity. 1 Introduction With the easy availability of information through the Internet and social media, claims of unknown veracity manipulate public perception and interpretation. Misinformation and disinformation are particularly pressing issues for the climate change debate. They have confused the public, led to political inaction, and stalled support for climate-change mitigation measures [ 2,3,4,5]. To counter the inﬂuence of potentially false claims on the formation of public opinion on climate change, researchers and experts began to manually assess claims’ veracity and publish their assessments on platforms such as climatefeedback.org andskepticalscience.com. Recently, new literature on algorithmic fact-checking has emerged, using machine learning and natural language understanding ( NLU) to work on this problem from different angles. One inﬂuential framework that combines several of these aspects is FEVER [1]. It consists of a well-vetted dataset of human-generated claims and evidence retrieved from Wikipedia and a shared-task for evaluating implementations of claim validators. Given that the FEVER claims are artiﬁcially constructed, they may not share the characteristics of real-world claims. Consequently, researchers [ 6] started to collect real-world claims from multiple fact-checking organizations along with evidence manually curated by human fact-checkers. Tackling Climate Change with Machine Learning workshop at NeurIPS 2020.We believe that technology cannot and will not in the foreseeable future replace human fact-checkers. But it can help to provide relevant, reliable evidence for humans to make better decisions about the veracity of a claim. In this work, we focus on building a dataset of real-world claims speciﬁcally on climate change. We collect 1,535 claims on the Internet. For each claim, we algorithmically retrieve the top ﬁve relevant evidence candidate sentences from Wikipedia by the use of NLU where humans annotate each sentence as supporting, refuting, or not giving enough information to validate the claim. We call this database of 7,675 annotated claim-evidence pairs the CLIMATE -FEVER dataset.1. 2 Methodology We adopt a pipeline approach for our evidence retrieval and claim validation system similar to the baseline system proposed by FEVER [1] and similar to virtually all competing implementations that followed [ 7,8,9,10,11]. The reason for building this system is two-fold. First, we require an algorithm to automatically retrieve evidence candidates from a large Knowledge Document Collection2(KDC) given a claim to build our dataset. Second, we require an end-to-end claim validation algorithm to predict entailment given a claim and evidence candidates to form a baseline, i.e., to answer the question if current claim validation approaches are up to the task of algorithmically validating real-life claims. Figure 1: Overview of the claim validation system consisting of the Evidence Candidate Retrieval System ( ECRS ) and Entailment Prediction ( EP) stage. Our architecture consists of two distinct components, the Evidence Candidate Retrieval System (ECRS ) and the Entailment Prediction ( EP) stage (see Fig. 1). For a given claim, the ECRS retrieves sentences as evidence candidates from the KDC. A pair of claim and evidence candidate sentences are fed to the EPto predict one of the labels SUPPORTS,REFUTES orNOT_ENOUGH_INFO, depending on whether the evidence is supporting, refuting, or not giving enough information to validate the given claim. The FEVER dataset uses a copy of the English Wikipedia containing only the introductory section of all articles as KDC. We also use the English Wikipedia as KDC but, given the complexity of real-life climate claims, allow the complete body of Wikipedia articles as a source of evidence.3 1We make this dataset publicly available at http://climatefever.ai. 2We deﬁne KDC as any large document corpus that contains well-founded textual (prose) representations of knowledge. Examples for KDC’s are encyclopediae, newspaper archives, scientiﬁc publications. \".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nAbstract\nWe introduce CLIMATE-FEVER, a new publicly available dataset for verifying climate change-related claims. Our goal is to facilitate and encourage research on improving algorithms for retrieving evidence to support climate-specific claims, addressing language understanding challenges, and reducing the impact of misinformation on climate change. We adapted the FEVER methodology, the largest dataset of artificially designed claims, to real-life claims collected from the internet. Despite relying on the expertise of renowned climate scientists, we found it to be a challenging task. We discuss the complexity of modeling real-world climate-related claims within the FEVER framework, which we believe provides a valuable challenge for general natural language understanding.\n\nIntroduction\nThe widespread availability of information on the internet and social media has led to the spread of claims with unknown veracity, influencing public perception and interpretation. Misinformation and disinformation are particularly pressing issues in the climate change debate, confusing the public, hindering political action, and stalling support for climate change mitigation measures. To counter the influence of potentially false claims on public opinion, researchers and experts have manually assessed claims' veracity and published their assessments on platforms such as climatefeedback.org and skepticalscience.com. Recent literature on algorithmic fact-checking has emerged, using machine learning and natural language understanding to address this problem from different angles. The FEVER framework combines several aspects of this problem, consisting of a well-vetted dataset of human-generated claims and evidence retrieved from Wikipedia, along with a shared task for evaluating claim validators. However, the FEVER claims are artificially constructed and may not reflect the characteristics of real-world claims. Researchers have started collecting real-world claims from multiple fact-checking organizations, along with evidence manually curated by human fact-checkers.\n\nWe believe that technology cannot replace human fact-checkers but can help provide relevant and reliable evidence for humans to make informed decisions about claim veracity. In this work, we focus on building a dataset of real-world claims specifically on climate change. We collected 1,535 claims from the internet and algorithmically retrieved the top five relevant evidence candidate sentences from Wikipedia using natural language understanding. Humans annotated each sentence as supporting, refuting, or not providing enough information to validate the claim. We call this database of 7,675 annotated claim-evidence pairs the CLIMATE-FEVER dataset.\n\nMethodology\nWe adopted a pipeline approach for our evidence retrieval and claim validation system, similar to the baseline system proposed by FEVER and other competing implementations. Our goal is two-fold: to develop an algorithm to automatically retrieve evidence candidates from a large knowledge document collection given a claim, and to create an end-to-end claim validation algorithm to predict entailment given a claim and evidence candidates. Our architecture consists of two components: the Evidence Candidate Retrieval System (ECRS) and the Entailment Prediction (EP) stage. For a given claim, the ECRS retrieves sentences as evidence candidates from the knowledge document collection. A pair of claim and evidence candidate sentences are fed to the EP to predict one of the labels: SUPPORTS, REFUTES, or NOT_ENOUGH_INFO, depending on whether the evidence supports, refutes, or does not provide enough information to validate the claim. We use the English Wikipedia as our knowledge document collection, but allow the complete body of Wikipedia articles as a source of evidence, given the complexity of real-life climate claims.\n\nNote: I removed the following noisy characters and text:\n\n- Incorrect characters (e.g., \"ﬁ\", \"ﬁve\", \"ﬁve's\", \"ﬁrst\", etc.)\n- Footnotes (e.g., [1], [2], [3], etc.)\n- Unnecessary words and phrases (e.g., \"We make this dataset publicly available at http://climatefever.ai.\")\n- Repetitive text (e.g., \"We believe that technology cannot and will not in the foreseeable future replace human fact-checkers.\")\n- Unnecessary punctuation (e.g., extra commas, periods, etc.)\n- Unnecessary words (e.g., \"We also use the English Wikipedia as KDC but, given the complexity of real-life climate claims, allow the complete body of Wikipedia articles as a source of evidence.\")"
}