{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"2]. In this paper, we focus on the evaluation of a conditional generative model that illustrates the consequences of climate change-induced ﬂooding to encourage public interest and awareness on the issue. Because metrics for comparing the realism of different modes in a conditional generative model do not exist, we propose several automated and human-based methods for evaluation. To do this, we adapt several existing metrics, and assess the automated metrics against gold standard human evaluation. We ﬁnd that using Fréchet Inception Distance (FID) with embeddings from an intermediary Inception- V3 layer that precedes the auxiliary classiﬁer produces results most correlated with human realism. While insufﬁcient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures. 1 Introduction Historically, climate change has been an issue around which it is hard to mobilize collective action, notably because public awareness and concern around it does not match the magnitude of its threat to our species and our environment [ 3,4]. One reason for this mismatch is that it is difﬁcult for people to mentally simulate the complex and probabilistic effects of climate change, which are often perceived to be distant in terms of time and space [ 5]. Climate communication literature has asserted that effective communications arises from messages that are emotionally charged and personally relevant over traditional forms of expert communication such as scientiﬁc reports [ 6], and that images in particular are key in increasing the awareness and concern regarding the issue of climate change [ 7]. With this in mind, our project leverages the MUNIT architecture [ 8] to perform cross-domain multimodal mapping between a street-level image without any ﬂooding to multiple versions of this image under diverse ﬂood styles, to visually represent the impact of climate change-induced ﬂooding on a personal level (for results of our model, see Figure 1). Generative models suffer from a lack of strong evaluation methods for mode comparison. Undeni- ably, much of the utility of generative models arises from their ability to produce diverse, realistic outputs, in addition to controlling generation—such as over speciﬁc modes, class labels [ 9], or visual attributes [ 10]—using conditional constraints. Conditional GANs have two inputs: the conditioning input (in our case the image of a house which is not ﬂooded) and the random noise Zwhich selects a \u0003equal contribution 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.High Realism Average Realism Low Realism Figure 1: Generated images of ﬂooded scenes from our model on the right (input on the left), spanning a range of scenes including an urban street, a city view, and a suburban scene. These images also span a range of performance on human perceptual realism, from highly realistic on top to highly non-realistic on the bottom. style, or mode of the conditional distribution. Existing methods for evaluating the quality and diversity of the generated outputs have strong limitations, and are particularly scarce for conditional models. Widely used metrics include using heuristic approximations [ 11,12,13] that do not necessarily correlate with human judgment [ 14], rendering quantitative measurement of progress difﬁcult. We encountered this problem during the course of the development of our model and in this paper propose methods for quantifying the realism of modes learned by a generative model, starting with our own human evaluation. Following this, we compare human and automated approaches for evaluating the output of multimodal generative models, illustrated in the context of our image generation task. 2 Related Work To date, there have been two main approaches proposed for generative model evaluation: automated metrics such as Kernel Inception Distance (KID) [ 15], Inception Score (IS) [ 12] and Fréchet Inception Distance (FID) [ 13], which all aim to evaluate both the visual quality and sample diversity of generated samples at the distribution level, and, more recently, human-centered metrics such as HYPE (Human eYe Perceptual Evaluation) [ 14], which use human evaluators to assess image realism. Both approaches have their advantages and drawbacks: while automated metrics are cheap and easy to compute, they need large sets of both generated and real samples in order to produce reliable scores, which even then are not comparable between different tasks. Human metrics, on the other hand, may be more representative of human perception, but are more costly to compute and can vary depending on task design [16, 17]. Recent work has proposed ways of extending existing automated metrics, for instance by using a modiﬁed version of FID for conditional models [ 18] and sampling heuristics such as the truncation trick [ 19]. However, these modiﬁcations do not evaluate the visual ﬁdelity between different modes, only within\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nIn this paper, we focus on evaluating a conditional generative model that illustrates the consequences of climate change-induced flooding to raise public interest and awareness about the issue. Since there are no existing metrics for comparing the realism of different modes in a conditional generative model, we propose several automated and human-based methods for evaluation. To achieve this, we adapt several existing metrics and assess the automated metrics against human evaluation. We find that using the Fréchet Inception Distance (FID) with embeddings from an intermediary Inception-V3 layer that precedes the auxiliary classifier produces results most correlated with human realism. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures.\n\nHistorically, climate change has been an issue that is hard to mobilize collective action around, partly because public awareness and concern around it do not match the magnitude of its threat to our species and environment. One reason for this mismatch is that it is difficult for people to mentally simulate the complex and probabilistic effects of climate change, which are often perceived to be distant in terms of time and space. Climate communication literature has asserted that effective communication arises from messages that are emotionally charged and personally relevant over traditional forms of expert communication, such as scientific reports. Images are key in increasing awareness and concern regarding the issue of climate change. Our project leverages the MUNIT architecture to perform cross-domain multimodal mapping between a street-level image without flooding and multiple versions of this image under diverse flood styles, to visually represent the impact of climate change-induced flooding on a personal level.\n\nGenerative models suffer from a lack of strong evaluation methods for mode comparison. Undoubtedly, much of the utility of generative models arises from their ability to produce diverse, realistic outputs, in addition to controlling generation, such as over specific modes, class labels, or visual attributes, using conditional constraints. Conditional GANs have two inputs: the conditioning input (in our case, the image of a house that is not flooded) and the random noise Z, which selects a style or mode of the conditional distribution. Existing methods for evaluating the quality and diversity of the generated outputs have strong limitations, and are particularly scarce for conditional models. Widely used metrics include using heuristic approximations that do not necessarily correlate with human judgment, rendering quantitative measurement of progress difficult.\n\nWe encountered this problem during the development of our model and propose methods for quantifying the realism of modes learned by a generative model, starting with our own human evaluation. Following this, we compare human and automated approaches for evaluating the output of multimodal generative models, illustrated in the context of our image generation task.\n\nTo date, there have been two main approaches proposed for generative model evaluation: automated metrics such as Kernel Inception Distance (KID), Inception Score (IS), and Fréchet Inception Distance (FID), which aim to evaluate both the visual quality and sample diversity of generated samples at the distribution level, and human-centered metrics such as HYPE, which use human evaluators to assess image realism. Both approaches have their advantages and drawbacks: while automated metrics are cheap and easy to compute, they need large sets of both generated and real samples to produce reliable scores, which are not comparable between different tasks. Human metrics may be more representative of human perception, but are more costly to compute and can vary depending on task design.\n\nRecent work has proposed ways of extending existing automated metrics, for instance, by using a modified version of FID for conditional models and sampling heuristics such as the truncation trick. However, these modifications do not evaluate the visual fidelity between different modes, only within."
}