{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" neural networks (DNNs). Answering the call for sustainability, conven- tional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, moving jobs to other locations may not al- ways be feasible due to large dataset sizes or data regulations. Moreover, postpon- ing training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. In this work, we present a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Specifically, our solution observes real-time car- bon intensity shifts during training and controls the energy consumption of GPUs, thereby reducing carbon footprint while maintaining training performance. Fur- thermore, in order to proactively adapt to shifting carbon intensity, we propose a lightweight machine learning algorithm that predicts the carbon intensity of the upcoming time frame. Our solution, Chase, reduces the total carbon footprint of training ResNet-50 on ImageNet by 13.6% while only increasing training time by 2.5%. 1 I NTRODUCTION The growth of Deep Learning has led to a significant increase in energy consumption and carbon emissions from the use of GPUs for training DNNs (Anderson et al., 2022; Wu et al., 2021; Patterson et al., 2021), and enhancing the carbon efficiency of DNN training became a pressing and urgent problem. Concretely, training large DNNs such as GPT-3 (Brown et al., 2020), generates 552 metric tons of CO 2emissions (Patterson et al., 2021). However, not all Joules are born equal; Carbon intensity is a measure of electricity production, and is calculated by considering the number of grams of carbon dioxide emissions produced per kilowatt-hour (kWh) of electricity generated ( g·CO 2/kWh ). Naturally, carbon intensity can vary significantly depending on time and location. For instance, a region that relies heavily on coal for electricity generation would have a higher carbon intensity (Miller et al., 2022) than one that relies on carbon-free energy sources such as nuclear, solar, or wind (Google, 2018). Additionally, carbon intensity can also vary across time of day or season, as many renewable energy sources depend on natural phenomena. In this work, we demonstrate that by forecasting and exploiting shifts in real-time carbon intensity, we can enhance the carbon efficiency of DNN training. That is, when carbon intensity increases, we slow down training to draw less electricity; on the other hand, when carbon intensity decreases, we speed up training to make more progress. Chase makes these decisions automatically and provides large reductions in carbon emissions while increasing training time marginally. Chase will be open- sourced. 2 R ELATED WORK Carbon-aware job scheduling utilizes the variation of carbon intensity based on time (Li et al., 2016; Haghshenas et al., 2022) and location (Moghaddam, 2014; Berl et al., 2009) in order to reduce the 1Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2023 carbon emissions of DNN training. But due to various constraints such as large datasets (Caesar et al., 2019; Dai et al., 2017; Deng et al., 2009), data regulations (GDPR, 2018), and availability of resources, moving jobs to greener geographical locations is not always viable. Moreover, deferring training jobs to greener times may not be an option either, since DNNs must be trained with the latest data and quickly deployed to production for the highest service quality. In contrast, our solution does not migrate nor postpone training jobs. Rather, as the training job runs as requested, we transparently adjust its speed and energy consumption so that it automatically chases low-carbon electricity. Optimizing the energy consumption of DNN training can naturally lower carbon emissions due to the linear relationship between carbon and energy. GPUs, the primary hardware used for training DNNs, allow users to set their power limit through software (Nvidia, 2022). Exploiting this technique, Zeus (You et al., 2023) jointly optimizes energy and time consumed to reach a target validation accuracy by automatically configuring power limit and batch size over multiple re-training jobs. However, Zeus focuses on the time and energy consumption of training jobs and is not aware of carbon intensity nor the time-varying nature thereof. To proactively react to changes in carbon intensity, having carbon intensity forecasts for the next time window is necessary. Recent approaches (Maji et al., 2022a;b) have achieved high forecasting performance, but the use of DNNs consumes GPU resources and can offset the amount of carbon footprint reduction from subsequent optimization techniques. On the other hand, there are com- mercial services (WattTime, 2022; ElectricityMaps, 2022) that\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nDeep neural networks (DNNs) have a significant carbon footprint due to their energy consumption during training. To address this issue, conventional solutions have attempted to move training jobs to locations or time frames with lower carbon intensity. However, this approach may not be feasible due to large dataset sizes or data regulations. Moreover, delaying training can negatively impact application service quality because the DNNs backing the service are not updated in a timely fashion. This work presents a practical solution that reduces the carbon footprint of DNN training without migrating or postponing jobs. Our solution, Chase, observes real-time carbon intensity shifts during training and controls the energy consumption of GPUs to reduce carbon footprint while maintaining training performance. To proactively adapt to shifting carbon intensity, we propose a lightweight machine learning algorithm that predicts the carbon intensity of the upcoming time frame.\n\nThe growth of deep learning has led to a significant increase in energy consumption and carbon emissions from the use of GPUs for training DNNs. Enhancing the carbon efficiency of DNN training has become a pressing problem. Training large DNNs, such as GPT-3, generates a substantial amount of CO2 emissions. However, carbon intensity varies significantly depending on time and location. For instance, regions that rely heavily on coal for electricity generation have a higher carbon intensity than those that rely on carbon-free energy sources. Carbon intensity can also vary across time of day or season, as many renewable energy sources depend on natural phenomena.\n\nWe demonstrate that by forecasting and exploiting shifts in real-time carbon intensity, we can enhance the carbon efficiency of DNN training. When carbon intensity increases, we slow down training to draw less electricity; when carbon intensity decreases, we speed up training to make more progress. Chase makes these decisions automatically and provides large reductions in carbon emissions while increasing training time marginally.\n\nCarbon-aware job scheduling utilizes the variation of carbon intensity based on time and location to reduce the carbon emissions of DNN training. However, moving jobs to greener geographical locations is not always viable due to various constraints such as large datasets, data regulations, and availability of resources. Deferring training jobs to greener times may not be an option either, since DNNs must be trained with the latest data and quickly deployed to production for the highest service quality. In contrast, our solution does not migrate nor postpone training jobs. Rather, as the training job runs as requested, we transparently adjust its speed and energy consumption so that it automatically chases low-carbon electricity.\n\nOptimizing the energy consumption of DNN training can naturally lower carbon emissions due to the linear relationship between carbon and energy. GPUs allow users to set their power limit through software. Recent approaches have achieved high forecasting performance for carbon intensity, but the use of DNNs consumes GPU resources and can offset the amount of carbon footprint reduction from subsequent optimization techniques. Our solution, Chase, will be open-sourced."
}