{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" training (CLIP) model, which is pretrained to understand the relationship between images and text, demonstrated remarkable performance in the zero-shot recycla- bility classification task, with an accuracy of 89%. Our results underscore the potential of general vision-language models in addressing real-world challenges, such as automated waste sorting, by harnessing the inherent associations between visual and textual information. 1 Introduction Waste management, facilitated by proper recycling, is paramount in fostering sustainability, as it helps in the conservation of natural resources, mitigation of environmental harm, and re-utilization of recyclable materials instead of their disposal. The repercussions of inadequate waste disposal are severe, e.g., leaching toxic chemicals from batteries can pollute groundwater and decomposing organic waste in landfills can emit methane, a potent greenhouse gas. Since the 1960s, the per capita generation of garbage has doubled to ∼5 lbs/person/day [ 5]. A major obstacle to recycling is the widespread ambiguity surrounding the recyclability of certain items. The current classification bins labeled with generic categories such as paper, plastic, and metal often fall short of providing clear guidance. For instance, while cardboard falls under paper and is thus recyclable, does this include cardboard affixed with tape? Similarly, plastic bottles are generally recyclable, but what about those containing liquid? Machine learning models offer a viable solution to these challenges. However, the absence of large-scale, labeled datasets annotated with recyclability is a fundamental obstacle. Additionally, the multifaceted nature of materials and their varying conditions, such as differing states of cardboard, necessitate models capable of discerning a potentially limitless array of categories. Thus, models that grasp detailed information about both the substance and its state are pivotal. Our study explores the zero-shot classification capabilities of CLIP[ 4], a vision-language model trained on 400 million (image, text) pairs collected from the internet. Given its extensive and diverse training dataset, CLIP possesses significant potential to comprehend nuanced relationships between images and texts and exhibits proficiency in tasks for which it was not explicitly trained. Our findings reveal that CLIP can accurately classify materials according to their recyclability status, showcasing 37th Conference on Neural Information Processing Systems (NeurIPS 2023).Example 1:  Item - correct  Expected: “Boat Outboard Motor” - Private Disposal  Output: “Boat Outboard Motor” - Private Disposal  Conﬁdence: 99. 994%  Example 3:  Item is narrowly incorrect;     disposal method is incorrect  Expected: “Cereal Boxes” - Recycling  Output: “Cereals” - Curbside Food Waste Collection  Conﬁdence: 62. 199%  Example 4:   Item is clearly incorrect;     disposal method is incorrect  Expected: “Shoe box ( cardboard)” - Recycling  Output: “Metal” - Private Disposal   Conﬁdence: 8. 978%  Example 2:  Item is incorrect     but disposal method is the same  Expected: “Wine Bottle” - Recycling  Output: “Shampoo Bottle” - Recycling  Conﬁdence: 49.030%  Expected Predicted Expected Predicted  Boston.gov Guidelines  (~1100 items linked to  15 disposal methods ) Curated database  (998 items linked to  9 material categories,  11 disposal methods ) Curated Images  Creative Cloud Licenses  ( 1155 images )  Individual Image OpenCLIP  ViT -B/32  LAION- 2B  >2.3B text-image pairs CLIP-generated  item prediction Evaluation  ( sensitivity / recall/ F1  per disposal method,  category )  Comparison to  correct disposal  method Figure 1: Overview: A database of 998 items and linked images was curated based on the City of Boston guidelines. CLIP-predicted item labels were converted to the disposal method based on the database and compared to the disposal method of the original image. Four examples are shown. its potential as a valuable tool in offering informed guidance in waste management. Fig. 1 shows the overview of our approach. 2 Related Work Prior work has explored the application of various machine learning methods for waste material classification. In 2016, Yang and G. [6]applied a convolutional neural network (CNN) on a custom dataset of ∼2500 images in 6 categories (paper, glass, plastic, metal, cardboard, trash). However, they found that the performance of their naive CNN performed poorly (27% using a 70/30 split), worse than SVM (63% accuracy). Several studies have subsequently employed variants of CNN to classify materials. Narayan [3]implemented multiple CNN models to categorize images into compostable, recyclable, and landfill divisions. For model training, the authors amassed 1218 images and initiated the models with weights pre-trained on ImageNet, achieving accuracies ranging from 0.\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nThe CLIP model, pre-trained to understand the relationship between images and text, demonstrated remarkable performance in the zero-shot recyclability classification task with an accuracy of 89%. Our results highlight the potential of general vision-language models in addressing real-world challenges, such as automated waste sorting, by leveraging the inherent associations between visual and textual information.\n\nIntroduction\nWaste management, facilitated by proper recycling, is crucial for fostering sustainability, as it helps conserve natural resources, mitigate environmental harm, and reutilize recyclable materials instead of disposing of them. The consequences of inadequate waste disposal are severe, including the leaching of toxic chemicals from batteries into groundwater and the emission of methane, a potent greenhouse gas, from decomposing organic waste in landfills. Since the 1960s, the per capita generation of garbage has doubled to approximately 5 pounds per person per day.\n\nA significant obstacle to recycling is the widespread ambiguity surrounding the recyclability of certain items. Current classification bins labeled with generic categories such as paper, plastic, and metal often fail to provide clear guidance. For instance, while cardboard falls under paper and is thus recyclable, it is unclear whether cardboard affixed with tape is also recyclable. Similarly, plastic bottles are generally recyclable, but it is unclear whether those containing liquid are also recyclable. Machine learning models offer a viable solution to these challenges. However, the absence of large-scale, labeled datasets annotated with recyclability is a fundamental obstacle. Additionally, the multifaceted nature of materials and their varying conditions, such as differing states of cardboard, necessitate models capable of discerning a potentially limitless array of categories. Thus, models that grasp detailed information about both the substance and its state are pivotal.\n\nOur study explores the zero-shot classification capabilities of CLIP, a vision-language model trained on 400 million (image, text) pairs collected from the internet. Given its extensive and diverse training dataset, CLIP possesses significant potential to comprehend nuanced relationships between images and texts and exhibits proficiency in tasks for which it was not explicitly trained. Our findings reveal that CLIP can accurately classify materials according to their recyclability status, showcasing its potential as a valuable tool in offering informed guidance in waste management.\n\nExample 1: Item - correct\nExpected: \"Boat Outboard Motor\" - Private Disposal\nOutput: \"Boat Outboard Motor\" - Private Disposal\nConfidence: 99.994%\n\nExample 3: Item is narrowly incorrect; disposal method is incorrect\nExpected: \"Cereal Boxes\" - Recycling\nOutput: \"Cereals\" - Curbside Food Waste Collection\nConfidence: 62.199%\n\nExample 4: Item is clearly incorrect; disposal method is incorrect\nExpected: \"Shoe box (cardboard)\" - Recycling\nOutput: \"Metal\" - Private Disposal\nConfidence: 8.978%\n\nExample 2: Item is incorrect but disposal method is the same\nExpected: \"Wine Bottle\" - Recycling\nOutput: \"Shampoo Bottle\" - Recycling\nConfidence: 49.030%\n\nOur study used the following datasets:\n- Boston.gov Guidelines (~1100 items linked to 15 disposal methods)\n- Curated database (998 items linked to 9 material categories, 11 disposal methods)\n- Curated Images (Creative Cloud Licenses, 1155 images)\n- Individual Image OpenCLIP\n- ViT-B/32\n- LAION-2B (>2.3B text-image pairs)\n- CLIP-generated item prediction\n\nEvaluation (sensitivity, recall, F1 per disposal method, category)\nComparison to correct disposal method\nFigure 1: Overview\nA database of 998 items and linked images was curated based on the City of Boston guidelines. CLIP-predicted item labels were converted to the disposal method based on the database and compared to the disposal method of the original image. Four examples are shown.\n\nRelated Work\nPrior work has explored the application of various machine learning methods for waste material classification. In 2016, Yang and G. applied a convolutional neural network (CNN) on a custom dataset of approximately 2500 images in 6 categories (paper, glass, plastic, metal, cardboard, trash). However, they found that the performance of their naive CNN performed poorly (27% using a 70/30 split), worse than SVM (63% accuracy). Several studies have subsequently employed variants of CNN to classify materials. Narayan implemented multiple CNN models to categorize images into compostable, recyclable, and landfill divisions. For model training, the authors amassed 1218 images and initiated the models with weights pre-trained on ImageNet, achieving accuracies ranging from 0%."
}