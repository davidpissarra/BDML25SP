{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\") Santander, Spain iheredia@ifca.unican.es Abstract Deep Learning has recently emerged as a “perfect” prognosis downscaling tech- nique to compute high-resolution ﬁelds from large-scale coarse atmospheric data. Despite their promising results to reproduce the observed local variability, they are based on the estimation of independent distributions at each location, which leads to deﬁcient spatial structures, especially when downscaling precipitation. This study proposes the use of generative models to improve the spatial consistency of the high-resolution ﬁelds, very demanded by some sectoral applications (e.g., hydrology) to tackle climate change. 1 Motivations for generative models in “perfect” prognosis downscaling Global Climate Models (GCMs) are the main tools used nowadays to study the evolution of climate at different time-scales. They numerically solve a set of equations describing the dynamics of the climate system over a three-dimensional grid (latitude-longitude-height). In climate change modeling, these models are utilized to produce possible future pathways of the climate system based on different natural and anthropogenic forcings. However, due to computational limitations these models present a coarse spatial resolution —between 1\u000eand3\u000e,— which leads to a misrepresentation of important phenomena occurring at ﬁner scales. The generation of high-resolution climate projections is crucial for important socio-economic activities (e.g., the energy industry), and they are routinely used to elaborate mitigation and adaptation politics to climate change at a regional scale. Statistical Downscaling (SD) is used to bridge the scale-gap between the coarse model outputs and the local-scale by learning empirical relationships between a set of large-scale variables (predictors) and the regional variable of interest (predictands) based on large simulated/observational historical data records [ 1]. In this study we focus on a speciﬁc type of SD, named the “Perfect” Prognosis (PP) approach. PP downscaling leans on observational datasets to learn empirical relationships linking the predictors and the predictands. For the former, reanalysis data —a global dataset which combines observations with short-range forecasts through data assimilation,— is typically used, whilst for the latter either high-resolution grids or station-scale records can be employed. Once the relationship is established in these “perfect” conditions, we feed the model/algorithm with the equivalent GCM predictor variables to obtain high-resolution climate projections. A wide variety of Tackling Climate Change with Machine Learning: workshop at NeurIPS 2021.statistical techniques have been deployed to establish these links, such as (generalized) linear models [2], support vector machines [ 3], random forests [ 4], classical neural networks [ 5], and more recently deep learning (DL). In particular, DL has recently emerged as a promising PP technique, showing capabilities to reproduce the observed local climate [ 6,7,8], whilst showing plausible climate change projections of precipitation and temperature ﬁelds over Europe [ 9]. Nonetheless, currently the regression-based nature of most of the existing PP methods, leads to an underestimation of the extremes when the predictors lack from sufﬁcient informative power —i.e., given a particular predictor conﬁguration there are many possible predictand situations,— since they output the conditional mean [10]. To account for the uncertainty describing the possible extremes is crucial for some activities, and the community has driven its attention to probabilistic regression-based modeling. The probabilistic models used mostly estimate the parameters of selected probability distributions conditioned to the large-scale atmospheric situation. The choice of the distribution depends on the variable of interest to be modeled —for instance, the temperature follows a Gaussian distribution, whilst wind or precipitation ﬁelds present a heavy-tailed structure which better ﬁts with Gamma, Poisson or log-normal density functions,— and the regression-based models are trained to optimize the negative log-likelihood of the selected distribution at each site [ 5,7,11,12,13]. To model the spatial dependencies among sites, ideally we would estimate multivariate distributions representing the whole predictand domain, instead of predicting independent probability functions at each predictand site. Nonetheless, this was in practice computationally intractable, and very few procedures aimed to downscale over low-dimensional predictand spaces have been successfully deployed [14, 15, 16]. Recently, deep generative models have been developed that seek to approximate high-dimensional distributions through DL topologies. Based on previous merits in other disciplines, such as image- super-resolution (see e.g., [ 17,18]), some studies have searched for an analogy between this task and downscaling, deploying Generative Adversarial Networks (GAN, [ 19,20]) to obtain stochastic samples of high-resolution precipitation and temperature ﬁelds conditioned to their counterpart low-resolution ones. Despite these ﬁrst studies are far from the PP approach, —since they lean on surface variables in their predictor set, which are not well represented by GCMs (see [ 1,21] for\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nSantander, Spain (iheredia@ifca.unican.es)\n\nAbstract\n\nDeep learning has recently emerged as a promising downscaling technique to compute high-resolution fields from large-scale coarse atmospheric data. Despite their promising results in reproducing local variability, they are based on estimating independent distributions at each location, leading to deficient spatial structures, especially when downscaling precipitation. This study proposes the use of generative models to improve the spatial consistency of high-resolution fields, which is highly demanded by some sectoral applications, such as hydrology, to tackle climate change.\n\nMotivations for generative models in \"perfect\" prognosis downscaling Global Climate Models (GCMs) are the main tools used to study climate evolution at different time-scales. They numerically solve a set of equations describing the climate system over a three-dimensional grid (latitude-longitude-height). In climate change modeling, these models are used to produce possible future pathways of the climate system based on different natural and anthropogenic forcings. However, due to computational limitations, these models present a coarse spatial resolution (between 1° and 3°), which leads to a misrepresentation of important phenomena occurring at finer scales. The generation of high-resolution climate projections is crucial for socio-economic activities, such as the energy industry, and are used to elaborate mitigation and adaptation policies to climate change at a regional scale.\n\nStatistical Downscaling (SD) is used to bridge the scale-gap between the coarse model outputs and the local-scale by learning empirical relationships between a set of large-scale variables (predictors) and the regional variable of interest (predictands) based on large simulated/observational historical data records. In this study, we focus on a specific type of SD, named the \"Perfect\" Prognosis (PP) approach. PP downscaling relies on observational datasets to learn empirical relationships linking the predictors and the predictands. For the former, reanalysis data (a global dataset combining observations with short-range forecasts through data assimilation) is typically used, while for the latter, either high-resolution grids or station-scale records can be employed. Once the relationship is established in these \"perfect\" conditions, we feed the model/algorithm with the equivalent GCM predictor variables to obtain high-resolution climate projections.\n\nA wide variety of statistical techniques have been deployed to establish these links, such as linear models, support vector machines, random forests, classical neural networks, and more recently, deep learning (DL). In particular, DL has recently emerged as a promising PP technique, showing capabilities to reproduce observed local climate and plausible climate change projections of precipitation and temperature fields over Europe. However, currently, the regression-based nature of most existing PP methods leads to an underestimation of extremes when the predictors lack sufficient informative power, since they output the conditional mean. To account for the uncertainty describing possible extremes is crucial for some activities, and the community has driven its attention to probabilistic regression-based modeling.\n\nThe probabilistic models used mostly estimate the parameters of selected probability distributions conditioned to the large-scale atmospheric situation. The choice of the distribution depends on the variable of interest to be modeled (e.g., temperature follows a Gaussian distribution, while wind or precipitation fields present a heavy-tailed structure that better fits with Gamma, Poisson, or log-normal density functions). The regression-based models are trained to optimize the negative log-likelihood of the selected distribution at each site.\n\nTo model spatial dependencies among sites, ideally, we would estimate multivariate distributions representing the whole predictand domain, instead of predicting independent probability functions at each predictand site. However, this was computationally intractable, and very few procedures aimed to downscale over low-dimensional predictand spaces have been successfully deployed. Recently, deep generative models have been developed that seek to approximate high-dimensional distributions through DL topologies. Based on previous merits in other disciplines, such as image super-resolution, some studies have searched for an analogy between this task and downscaling, deploying Generative Adversarial Networks (GANs) to obtain stochastic samples of high-resolution precipitation and temperature fields conditioned to their counterpart low-resolution ones. Despite these first studies are far from the PP approach, since they lean on surface variables in their predictor set, which are not well represented by GCMs."
}