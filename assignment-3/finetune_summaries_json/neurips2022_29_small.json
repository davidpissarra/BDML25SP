{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" means the rest of the high-resolution data is thrown out. We use a transfer learning approach, which can be applied to a range of machine learning models, to leverage all the high-resolution data. We use three chaotic systems to show it stabilises training, gives improved general- isation performance and results in better forecasting skill. Our code is at https://github.com/raghul-parthipan/dont_waste_data. 1 Introduction Accurate weather and climate models are key to climate science and decision-making. Often we have a high-resolution physics-based model which we trust, and want to use that to create a lower-cost (lower-resolution) emulator of similar accuracy. There has been much work using machine learning (ML) to learn such models from data [ 1,2,3,4,5,6,7,8,9,10,11,12,13,14], due to the difficulty in manually specifying them. The naive approach is to use coarse-grained high-resolution model data as training data. The high- resolution data is averaged onto the lower-resolution grid and treated as source data. The goal is to match the evolution of the coarse-grained high-resolution model using the lower-resolution one. Such procedures have been used successfully [ 2,3,8,12,15,16]. This has several benefits over using observations, including excellent spatio-temporal coverage. But it has a key downside — the averaging procedure means much high-resolution data is thrown away. Our novelty is showing that climate model emulation can be framed as a transfer learning task. We can do better by using all of the high-resolution data as an auxiliary task to help learn the low- resolution emulator. And we can do this without any further cost at simulation time. As far as we know, this has not yet been reported in the climate literature. This results in improved generalization performance and forecasting ability, and we demonstrate this on three chaotic dynamical systems. Related Work. Transfer learning (TL) has been successfully used for fine-tuning models, including sequence models, in various domains such as natural language processing (NLP) and image classi- fication. There are various methods used such as (1) fine-tuning on an auxiliary task and then the target task [17, 18, 19]; (2) multi-task learning, where fine-tuning is done on the target task and one or more auxiliary tasks simultaneously [ 20,21,22,23,24,25,26]; and mixtures of the two. Our Tackling Climate Change with Machine Learning workshop at NeurIPS 2022.approach is most similar to the first one. However, our models are not pre-trained as is standard in NLP. Despite this, we show our approach remains successful. Climate Impact. A major source of inaccuracies in weather and climate models arises from ‘unresolved’ processes (such as those relating to convection and clouds) [ 27,28,29,30,31,32]. These occur at scales smaller than the resolution of the climate model but have key effects on the overall climate. For example, most of the variability in how much global surface temperatures increase after CO 2concentrations double is due to the representation of clouds [ 27,29,33]. There will always be processes too costly to be explicitly resolved by our current operational models. The standard approach to deal with these unresolved processes is to model their effects as a function of the resolved ones. This is known as ‘parameterization’ and there is much ML work on this [1,2,3,4,5,6,7,8,9,10,11,12,13,14]. We propose that by using all available high-resolution data, better ML parameterization schemes and therefore better climate models can be created. 2 Methods Our approach is a two-step process: first, we train our model on the high-resolution data, and second, we fine-tune it on the low-resolution (target) data. We denote the low-resolution data at time tasXt∈Rd. The goal is to create a sequence model for the evolution of Xtthrough time, whilst only tracking Xt. We denote the high-resolution data at time tasYt∈Rdm. In parameterization, Xtis often a temporal and/or spatial averaging of Yt. We wish to use Ytto learn a better model of Xt. A range of ML models for sequences may be used, but we suggest they should contain both shared and task-specific layers. We first model Yt, training in the standard teacher-forcing way for ML sequence models. We use the framework of probability, and so train by maximising the log-likelihood of Yt,log Pr( y1,y2,...,yn). Informally, the likelihood measures how likely Ytis to be generated by our sequence model. Next, the weights of the shared layers are frozen and the weights of the target-specific layers are trained to model\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nAccurate weather and climate models are crucial for climate science and decision-making. We often have a high-resolution physics-based model that we trust and want to use to create a lower-cost (lower-resolution) emulator of similar accuracy. There has been significant work using machine learning (ML) to learn such models from data, due to the difficulty in manually specifying them. The naive approach is to use coarse-grained high-resolution model data as training data. However, this approach has a key downside: much high-resolution data is discarded.\n\nOur novelty is showing that climate model emulation can be framed as a transfer learning task. We can leverage all the high-resolution data as an auxiliary task to help learn the low-resolution emulator without any additional cost at simulation time. This approach has not been reported in the climate literature before. Our results show improved generalization performance and forecasting ability, demonstrated on three chaotic dynamical systems.\n\nTransfer learning has been successfully used for fine-tuning models in various domains, including natural language processing and image classification. Our approach is similar to fine-tuning on an auxiliary task and then the target task. However, our models are not pre-trained, unlike standard practice in natural language processing. Despite this, we show that our approach remains successful.\n\nClimate models are prone to inaccuracies due to 'unresolved' processes, such as those related to convection and clouds, which occur at scales smaller than the model resolution but have significant effects on the overall climate. For example, most of the variability in global surface temperature increase after CO2 concentrations double is due to the representation of clouds. The standard approach to deal with these unresolved processes is to model their effects as a function of the resolved ones, known as 'parameterization'. We propose that using all available high-resolution data can lead to better ML parameterization schemes and therefore better climate models.\n\nOur approach is a two-step process: first, we train our model on the high-resolution data, and second, we fine-tune it on the low-resolution (target) data. We denote the low-resolution data at time t as Xt∈Rd and the high-resolution data at time t as Yt∈Rdm. The goal is to create a sequence model for the evolution of Xt through time, while only tracking Xt. We first model Yt, training in the standard teacher-forcing way for ML sequence models. We use the framework of probability and train by maximizing the log-likelihood of Yt, log Pr(y1,y2,...,yn). The weights of the shared layers are frozen, and the weights of the target-specific layers are trained to model Xt."
}