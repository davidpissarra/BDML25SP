{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" Climate Change (IPCC) reports, labeled with their associated confidence levels. Using this dataset, we show that recent LLMs can classify human expert confidence in climate-related statements, especially in a few-shot learning setting, but with limited (up to 47%) accuracy. Overall, models exhibit consistent and significant over-confidence on low and medium confidence statements. We highlight implications of our results for climate communication, LLMs evaluation strategies, and the use of LLMs in information retrieval systems. 1 Introduction The wide deployment of Large Language Models (LLMs) as question-answering tools calls for nuanced evaluation of their outputs across knowledge domains. This is especially important in climate science, where the quality of the information sources shaping public opinion, and ultimately public policy, could determine whether the world succeeds or fails in tackling climate change. This paper aims to evaluate the reliability of LLM outputs in the climate science and policy domain. We introduce the Expert Confidence in Climate Statements ( CLIMATE X) dataset [10], a novel, curated, expert-labeled, natural language dataset of 8094 statements sourced from the three most recent Intergovernmental Panel on Climate Change Assessment Reports (IPCC AR6) — along with their associated confidence levels (low, medium, high, or very high) that were assessed by climate scientists based on the quantity and quality of available evidence and agreement among their peers. CLIMATE X is available on HuggingFace and source code for experiments is available on Github. We use this dataset to evaluate how accurately recent LLMs assess the confidence which human experts associate with climate science statements. Although OpenAI’s GPT-3.5-turbo andGPT-4 assess the true confidence level with better-than-random accuracy and higher performance than non- expert humans, even in a zero-shot setting, they, and other models we tested, consistently overstate the certainty level associated with low and medium confidence labeled statements. With LLMs poised to become increasingly significant sources of public information, the reliability of their outputs in the climate domain is critical for avoiding misinformation and garnering support for effective climate policy. We hope the CLIMATE Xdataset provides a valuable tool for benchmarking the trustworthiness of LLM outputs in the climate domain, highlights the need for further work in this area, and aids efforts to develop models that accurately convey climate knowledge. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.2 Prior Work 2.1 LLMs and Uncertainty Recent literature demonstrates that linguistic statements of certainty impact LLM performance on calibrated NLP tasks. For example, Zhou et al. (2023) [ 24] find that appending ‘weakeners’ (i.e. “A: I think it’s...”) or ‘strengtheners’ (i.e. “A: I’m certain it’s...”) to zero-shot QA prompts can significantly impact GPT-3’s performance on common QA datasets like TriviaQA. They find that strengtheners surprisingly result in a lower accuracy across their datasets (40%) than weakeners (47%), and suggest that there may be unique challenges with reliably interpreting linguistic cues of high confidence. To address this issue, and the more general over-confidence problem, recent work attempts to train LLMs to accurately express their own uncertainty. Lin et al. (2022) [ 12] fine-tune [ 6] GPT-3 to express how confidently it answers different arithmetic tasks, using both categorical certainty (e.g. “high confidence”) and numeric certainty (e.g. “90%”). While they obtain relative success and promising results, the authors note concerns such as over-fitting to their training set distribution. Kadavath et al. (2022) [ 8] show that LLMs are capable of ‘self-evaluation,’ i.e. evaluating their own answers as true or false relative to accepted human knowledge with few-shot learning. They also fine-tune models to predict the probability that they can accurately answer a particular question. The authors train and evaluate many different-sized models using multiple pre-existing generation datasets such as TriviaQA, arithmetic, and code generation. They find that models are initially poor at self-evaluation, but improve with few-shot learning (up to 20 demonstrations). Prior work has also explored how to evaluate “what LLMs know.” Chang et al. (2023) [ 3] carry what they describe as a “data archaeology” investigation to infer books that LLMs have been trained on, using a “name cloze membership inference query,” the task of predicting a masked name in a sentence based on the context surrounding it. Importantly, the human baseline on this task is 0%. The authors find a clear correlation between the frequency at which books appear in datasets over which LLMs are known to have been trained, and their performance on the related cloze task. Finally, prior work has also included more wholistic evaluations of how LLMs communicate climate- change related information\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nClimate Change (IPCC) reports, labeled with their associated confidence levels, are used to evaluate the performance of recent Large Language Models (LLMs) in classifying human expert confidence in climate-related statements. The results show that LLMs can classify human expert confidence with limited accuracy, up to 47%, especially in a few-shot learning setting. However, models consistently overstate the certainty level associated with low and medium confidence statements.\n\nThis paper aims to evaluate the reliability of LLM outputs in the climate science and policy domain. A novel dataset, Expert Confidence in Climate Statements (CLIMATE X), is introduced, which consists of 8094 statements sourced from the three most recent Intergovernmental Panel on Climate Change Assessment Reports (IPCC AR6) along with their associated confidence levels (low, medium, high, or very high) assessed by climate scientists. The dataset is available on HuggingFace, and the source code for experiments is available on Github.\n\nThe CLIMATE X dataset is used to evaluate how accurately recent LLMs assess the confidence associated with climate science statements. Although OpenAI's GPT-3.5-turbo and GPT-4 assess the true confidence level with better-than-random accuracy and higher performance than non-expert humans, even in a zero-shot setting, they consistently overstate the certainty level associated with low and medium confidence labeled statements.\n\nThe reliability of LLM outputs in the climate domain is critical for avoiding misinformation and garnering support for effective climate policy. The CLIMATE X dataset is hoped to provide a valuable tool for benchmarking the trustworthiness of LLM outputs in the climate domain and aid efforts to develop models that accurately convey climate knowledge.\n\nPrior work has demonstrated that linguistic statements of certainty impact LLM performance on calibrated NLP tasks. For example, appending weakeners or strengtheners to zero-shot QA prompts can significantly impact GPT-3's performance on common QA datasets. Recent work attempts to train LLMs to accurately express their own uncertainty, but concerns such as over-fitting to the training set distribution remain.\n\nOther prior work has explored how to evaluate what LLMs know and how they communicate climate-change related information. This includes investigating the books that LLMs have been trained on and their performance on related tasks. The results highlight the need for further work in this area to develop models that accurately convey climate knowledge and communicate climate-change related information effectively."
}