{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" the VQA system. However, the main challenge is the delay caused by the generation of labels in the assessment of the affected areas. To tackle this, we deployed pre-trained CLIP model, which is trained on visual-image pairs. however, we empirically see that the model has poor zero-shot performance. Thus, we instead use pre-trained embeddings of text and image from this model for our supervised training and surpass previous state-of-the-art results on the FloodNet dataset. We expand this to a continual setting, which is a more real-life scenario. We tackle the problem of catastrophic forgetting using various experience replay methods.2 1 Introduction Climate change is one of the most pressing issues of our times, with increased greenhouse gases emission and urbanization. One of the most destructive effects of climate change is flooding, disrupting people’s lives and causing damage to properties. During floods, most disaster response systems rely on aerial data to assess the affected areas and provide immediate help. In applications, like a deployed disaster response system, where labels and data are going to be generated in real time and the model is deployed for predictions, the most important thing is to save time and deploy rescue teams as quick as possible based on the model predictions. There are going to be two types of major delays for such a system: labelling the entire data at once and training the model on the labeled data. The latter problem specific to FloodNet VQA was addressed by Kane and Khose (2022), which reduced the training time by 12x and inference time by 4x on the entire data. However, the former is yet not addressed, as labelling takes a lot of time. To address the delay caused by label generation, we investigate the direct evaluation of large pre- trained multimodal models on the downstream VQA dataset, studying the effects of zero-shot transfer, proposed by Song et al. (2022). We analyze CLIP’s zero-shot ability on the FloodNet VQA dataset. These models do not perform well on the downstream aerial VQA task without finetuning the entire dataset. We, therefore, use pre-trained CLIP embeddings of the images and the questions from these models for our supervised training. Previous works like Sarkar and Rahnemoonfar (2021) solve this problem using VGG-19 and LSTM features, but their results are inferior to modern architectures like ResNetHe et al. (2015) and BERT-based Devlin et al. (2018) models. ∗Indicates equal contribution 2Our training runs are available at: https://wandb.ai/compyle/continual_vqa_final Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.Our main contribution in this paper is to tackle this real-time setting delay with the help of continual learning for visual question answering by training our models on a continuous stream of data using experience replay. The models are trained on different tasks in a sequential fashion without substantial forgetting the previous tasks. This way, the need for complete data labelling is eliminated, reducing training time and compute requirements. We provide a thorough analysis of the task-order permutations and compare the same with supervised benchmarks. 2 Methodology We use the FloodNet dataset Rahnemoonfar et al. (2020) for our experiments. It is a VQA dataset based on aerial images of flooded areas. We have elaborated the dataset in Appendix A. In the ideal case, pretrained multimodal models like CLIP Radford et al. (2021) should correctly answer questions related to image due to the shared latent space between image and text features. Bearing that in mind, we experiment with an out-of-the-box CLIP predictor to answer questions. Concretely, we evaluate the performance of CLIP on the FloodNet VQA task without finetuning. We convert the question text to prompts containing the probable labels for our text input. We then pass these inputs to the CLIP model, which returns the posterior probability of every possible label word. However, this ideal scenario is unlikely, more so in the case of specialized aerial images. The following best approach is task-wise lifelong learning. Here, the data is available through a stream of tasks, meaning a task once trained on cannot be trained on again. This more closely depicts the real-world scenario where the labels are not available all at once. Thus, we divide the available dataset into three tasks: Yes/No, Road condition recognition and Image condition recognition. We use experience replay Chaudhry et al. (2019) to further enhance the continual learning abilities of the model. Given that image and text features from models like CLIP are more coherent due to the shared latent space, we use these features for continual training. We use the training strategy proposed by Kane and Khose (2022). We also provide results on this model trained on the complete dataset and task-wise training results as the upper bound for the continual training setup. 3 Experiments and Results 3.1 Zero shot setup We evaluate CLIP’s zero\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy and irrelevant content removed:\n\nThe main challenge in the VQA system is the delay caused by generating labels for affected areas. To address this, we deployed a pre-trained CLIP model, which is trained on visual-image pairs. However, we observed poor zero-shot performance. Instead, we used pre-trained embeddings of text and image from this model for supervised training, achieving state-of-the-art results on the FloodNet dataset. We extended this to a continual setting, simulating a real-life scenario. To tackle catastrophic forgetting, we employed various experience replay methods.\n\nClimate change is a pressing issue, with increased greenhouse gas emissions and urbanization. Flooding is a destructive effect, disrupting lives and causing property damage. During floods, disaster response systems rely on aerial data to assess affected areas and provide immediate help. In deployed disaster response systems, time is critical, and models must make predictions quickly to deploy rescue teams. Two major delays occur: labeling data and training models on labeled data. The latter was addressed by Kane and Khose (2022), reducing training and inference times. However, labeling data remains a significant challenge.\n\nTo address the delay caused by label generation, we investigated direct evaluation of pre-trained multimodal models on the downstream VQA dataset. We analyzed CLIP's zero-shot ability on the FloodNet VQA dataset and found that it does not perform well without fine-tuning the entire dataset. We used pre-trained CLIP embeddings of images and questions for supervised training. Previous works, such as Sarkar and Rahnemoonfar (2021), used VGG-19 and LSTM features, but their results were inferior to modern architectures like ResNet and BERT-based models.\n\nOur main contribution is to tackle the real-time setting delay using continual learning for visual question answering. We trained models on a continuous stream of data using experience replay, eliminating the need for complete data labeling and reducing training time and compute requirements. We provided a thorough analysis of task-order permutations and compared them with supervised benchmarks.\n\nWe used the FloodNet dataset (Rahnemoonfar et al., 2020) for our experiments. In an ideal scenario, pre-trained multimodal models like CLIP should correctly answer questions related to images due to the shared latent space between image and text features. We evaluated the performance of CLIP on the FloodNet VQA task without fine-tuning and found that it did not perform well. We then divided the dataset into three tasks: Yes/No, Road condition recognition, and Image condition recognition, and used experience replay to enhance the continual learning abilities of the model.\n\nWe used the training strategy proposed by Kane and Khose (2022) and provided results on the complete dataset and task-wise training results as the upper bound for the continual training setup.\n\nIn the zero-shot setup, we evaluated CLIP's performance on the FloodNet VQA task and found that it did not perform well. We then used pre-trained CLIP embeddings of images and questions for supervised training, achieving state-of-the-art results on the FloodNet dataset."
}