{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"RL) frameworks for effectively learn- ing RL agents to prevent cascading failures. A curriculum-based approach with reward tuning is incorporated into the training procedure by modifying the environ- ment using the network physics. Our procedure is tested on an actor-critic-based agent on the IEEE 14-bus test system using the RL environment developed by RTE, the French transmission system operator (TSO). We observed that naively training the RL agent without the curriculum approach failed to prevent cascading for most test scenarios, while the curriculum based RL agents succeeded in most test scenarios, illustrating the importance of properly integrating domain knowledge of physical systems for real-world RL applications. 1 Motivation & Introduction The electric power generation is a major contributor of green house emissions. The trend of electri- fication of transportation and gas infrastructure [1] will further increase this contribution. Thus, in order to achieve the emission goals of the Paris agreement [2], to transition to clean energy and to mitigate the already escalating global climate change, it is necessary to integrate renewables into the power grids and to utilize the existing grid infrastructure to its maximum capacity, while ensuring grid operations are within limits. A key operational constraint in the power grid is the current flow limit in a transmission line. If left unattended or an appropriate response from the grid operator is delayed, then overloaded lines will be disconnect (trip) and the current that was flowing in these lines will be re-routed through other lines, which can in turn lead to further line tripping. This sequence of events is called a cascade [3]. During times of high demand, cascading can lead to regions in the power grid to be disconnected Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.from generators, leading to a blackout [4] (Appendix A). The introduction of renewables that are inherently uncertain is going to exacerbate this phenomenon and so transmission system operators (TSOs) are actively researching methods to prevent cascades. TSOs prefer an economical and flexible solution like dynamic topology reconfiguration over the other solutions like load shedding, peak shaving and curtailment [5-7] that increase operational costs. A dynamic topology reconfiguration (Appendix B) essentially alters the connectivity of nodes and edges in the power grid through closing and opening switches in many substations (nodes) of the grid. The discrete combinatorial nature of the reconfiguration combined with the non-linear relation between load demand, generation and line flows [8] implies that estimating the topologies to prevent cascading as the load demand varies over the course of a day is still beyond the state-of-the-art. Contribution. We leverage deep reinforcement learning (RL) methods to learn topology controls that prevent cascading failures. We propose an efficient physics-inspired curriculum based deep reinforcement learning approach and demonstrate the superior performance of this approach over the conventional RL methods on the IEEE-14 bus system. We show that the same deep-RL structure is able to learn with our approach but fails when trained using conventional RL approaches. 1.1 Related Work [9] proposed an expert system-based approach that incorporates both transmission line switching and bus splitting/merging operations. This expert system-based approach is sufficiently fast but suffers from accuracy issues at times, and also, it cannot account for the impact of an optimal control action over a time horizon [10]. [11] includes the time horizon concept but uses a mixed-integer nonlinear optimization method which takes longer times to solve. [12] proposed to learn curriculum strategies as a part of the overall ML-approach for classification or regression. [13] uses curriculum learning to accelerate training of RL agents. However, it has not been explored for controlling network flows. 2 Dynamic Topology Reconfiguration Formulation There are two ways to dynamically reconfigure the grid topology: 1) transmission line switching and 2) node splitting/merging using the switches within a node. The objective of a dynamic topology controller for the power grid involves identifying the optimal topology grid configuration that minimizes the total line loading on the power grid, given the load demand and generator supply. The sequential planning problem is shown below: min τ1...τtt=nX t=1X ∀p∈E\u0012Ip,t Ip,max\u0013, (1) sub. to :f(τt, xt) = 0; ∀t={1,2,···, n}, (2) τt∈A(τt−1, xt, ρS, ρC, ρH);∀t={2,···, n}, (3) The line loading is the sum of the ratio of the current at time t(Ip,t) and the current thermal limits, Ip,max,∀p∈Ewhere Eis the set of all transmission lines in the power grid. This ratio is also referred to as the normalized current in a line. The aim of the topology controller is to minimize the total line loading on the grid over a time horizon t={1,2,..., n\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy and irrelevant content removed:\n\n**Preventing Cascading Failures in Power Grids using Reinforcement Learning**\n\nTo prevent cascading failures in power grids, we propose a curriculum-based approach with reward tuning, which is incorporated into the training procedure by modifying the environment using network physics. Our method is tested on an actor-critic-based agent on the IEEE 14-bus test system using the RL environment developed by RTE, the French transmission system operator.\n\nWe observed that naively training the RL agent without the curriculum approach failed to prevent cascading for most test scenarios, while the curriculum-based RL agents succeeded in most test scenarios, highlighting the importance of integrating domain knowledge of physical systems for real-world RL applications.\n\n**Motivation and Introduction**\n\nThe electric power generation is a significant contributor to greenhouse gas emissions. The trend of electrification of transportation and gas infrastructure will further increase this contribution. To achieve the emission goals of the Paris agreement, it is necessary to integrate renewables into the power grids and utilize the existing grid infrastructure to its maximum capacity while ensuring grid operations are within limits.\n\nA key operational constraint in the power grid is the current flow limit in a transmission line. If left unattended or an appropriate response from the grid operator is delayed, overloaded lines will trip, and the current will be re-routed through other lines, leading to further line tripping, a sequence of events known as a cascade. During times of high demand, cascading can lead to regions in the power grid being disconnected from generators, resulting in a blackout.\n\n**Contribution**\n\nWe leverage deep reinforcement learning (RL) methods to learn topology controls that prevent cascading failures. We propose an efficient physics-inspired curriculum-based deep reinforcement learning approach and demonstrate its superior performance over conventional RL methods on the IEEE-14 bus system. Our approach shows that the same deep-RL structure can learn with our approach but fails when trained using conventional RL approaches.\n\n**Related Work**\n\nPrevious studies have proposed expert system-based approaches that incorporate transmission line switching and bus splitting/merging operations. However, these approaches suffer from accuracy issues and cannot account for the impact of optimal control actions over a time horizon. Other studies have used mixed-integer nonlinear optimization methods, which take longer times to solve.\n\n**Dynamic Topology Reconfiguration Formulation**\n\nThere are two ways to dynamically reconfigure the grid topology: transmission line switching and node splitting/merging using switches within a node. The objective of a dynamic topology controller for the power grid is to identify the optimal topology grid configuration that minimizes the total line loading on the power grid, given the load demand and generator supply.\n\nThe sequential planning problem can be formulated as:\n\nmin τ1...τtt=nX t=1X ∀p∈E\u0012Ip,t Ip,max\u0013, (1)\nsub. to :f(τt, xt) = 0; ∀t={1,2,···, n}, (2)\nτt∈A(τt−1, xt, ρS, ρC, ρH);∀t={2,···, n}, (3)\n\nwhere the line loading is the sum of the ratio of the current at time t (Ip,t) and the current thermal limits, Ip,max, ∀p∈E, where E is the set of all transmission lines in the power grid. The aim of the topology controller is to minimize the total line loading on the grid over a time horizon t={1,2,..., n}."
}