{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" to cli- mate change conditions remains questionable, mainly due to the stationarity as- sumption. We propose deep ensembles as a simple method to improve the uncer- tainty quantification of statistical downscaling models. By better capturing uncer- tainty, statistical downscaling models allow for superior planning against extreme weather events, a source of various negative social and economic impacts. Since no observational future data exists, we rely on a pseudo reality experiment to as- sess the suitability of deep ensembles for quantifying the uncertainty of climate change projections. Deep ensembles allow for a better risk assessment, highly demanded by sectoral applications to tackle climate change. 1 I NTRODUCTION One of the most popular and extended tools for simulating the spatio-temporal evolution of cli- mate systems are General Circulation Models (GCMs). Unfortunately, due to the complexity of the physical processes underlying climate dynamics, they usually incur in a high computational cost, resulting in coarse resolution climate fields. This hampers their applicability in the regional-to-local scale, where climate projections are crucial for some sectorial applications to elaborate adaptation and mitigation plans to climate change. Statistical Downscaling (SD) (Maraun & Widmann, 2018) aims to generate high-resolution fields from coarse climate model outputs by learning a statistical relationship between a set of predictors (inputs) and predictands (outputs) representing the coarse and high resolution fields, respectively. Deep learning (DL) techniques (Goodfellow et al., 2016) have recently emerged as promising tools for downscaling coarse climate fields, showing excellent results in reproducing the observed local climate (Pan et al., 2019; Ba ˜no-Medina et al., 2020; Sun & Lan, 2021). For SD models to reliably compute high-resolution fields in future scenarios, it is crucial that they generalize to unseen conditions (stationary assumption), such as those of climate change. Most DL models adapted to the SD task minimize losses such as the mean squared error, which leads models to fit the mean of the training data, no providing information about the confidence in their pre- dictions. Taking this uncertainty into account is extremely important when applying these models to future scenarios influenced by climate change, where the stationarity assumption may be vio- lated. Also, being able to capture extremes is key for planning against extreme weather events (e.g., droughts, heat waves, extreme heat) whose frequency is expected to grow due to climate change. By improving the modeling of the uncertainty of SD models we can better capture these events when downscaling climate fields under climate change conditions. To quantify uncertainty in SD, in (Ba ˜no-Medina et al., 2020) authors model the parameters of a distribution over each gridpoint of the downscaled field, relying on information about the climate variable to choose the appropriate distribution (e.g., Gaussian for temperature). Subsequent work 1(Quesada-Chac ´on et al., 2022) follows the same procedure as a way to improve the distributional properties of the predictions. In (Vandal et al., 2018) authors rely on Monte Carlo Dropout (Gal & Ghahramani, 2016), an approximation to Bayesian inference through the use of dropout at test time, to quantify uncertainty in the downscaling of precipitation, obtaining acceptable results. Recently, several works explore generative modeling for the statistical downscaling task (Price & Rasp, 2022; Harris et al., 2022; Accarino et al., 2021), obtaining benefits from its stochastic properties. Among these works, only (Ba ˜no-Medina et al., 2022) quantifies the uncertainty in climate change projec- tions. A simpler way to achieve uncertainty quantification is through deep ensembles. It consists of training several models in parallel on the same data and then combining the predictions through an aggre- gation operation (e.g. mean), which generally improves performance compared to a single model (Dietterich, 2000). In (Lakshminarayanan et al., 2017) authors show how this technique also allows for a better predictive uncertainty estimates, however, its application to SD has not yet been studied. In this work we explore a natural extension of (Ba ˜no-Medina et al., 2020) through the use of deep ensembles, a simple technique that does not require tuning hyperparameters or modifying the model topology. This leads to a better quantification of uncertainty of climate change projections in future scenarios. 2 E XPERIMENTAL FRAMEWORK Due to the lack of observations in future periods, we rely on a pseudo reality experiment to eval- uate the effectiveness of deep ensembles for SD. This experiment is necessary if we need to re- veal whether the relationship learned in a historical period is able to extrapolate to future scenarios (Maraun & Widmann, 2018\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nClimate change projections remain uncertain, mainly due to the assumption of stationarity. We propose using deep ensembles as a simple method to improve the uncertainty quantification of statistical downscaling models. By better capturing uncertainty, these models allow for superior planning against extreme weather events, which have various negative social and economic impacts. Since no observational future data exists, we rely on a pseudo-reality experiment to assess the suitability of deep ensembles for quantifying the uncertainty of climate change projections. Deep ensembles enable a better risk assessment, highly demanded by sectoral applications to tackle climate change.\n\nIntroduction\n\nGeneral Circulation Models (GCMs) are popular tools for simulating the spatio-temporal evolution of climate systems. However, due to the complexity of climate dynamics, they incur a high computational cost, resulting in coarse resolution climate fields. This hampers their applicability in the regional-to-local scale, where climate projections are crucial for sectorial applications to develop adaptation and mitigation plans to climate change. Statistical Downscaling (SD) aims to generate high-resolution fields from coarse climate model outputs by learning a statistical relationship between predictors (inputs) and predictands (outputs) representing coarse and high-resolution fields, respectively. Deep learning techniques have recently emerged as promising tools for downscaling coarse climate fields, showing excellent results in reproducing observed local climate.\n\nFor SD models to reliably compute high-resolution fields in future scenarios, it is crucial that they generalize to unseen conditions (stationarity assumption), such as those of climate change. Most deep learning models adapted to the SD task minimize losses such as the mean squared error, which leads models to fit the mean of the training data, providing no information about the confidence in their predictions. Taking this uncertainty into account is essential when applying these models to future scenarios influenced by climate change, where the stationarity assumption may be violated. Capturing extremes is key for planning against extreme weather events (e.g., droughts, heat waves, extreme heat) whose frequency is expected to grow due to climate change. By improving the modeling of the uncertainty of SD models, we can better capture these events when downscaling climate fields under climate change conditions.\n\nTo quantify uncertainty in SD, previous studies have modeled the parameters of a distribution over each grid point of the downscaled field, relying on information about the climate variable to choose the appropriate distribution. Recent works have explored generative modeling for the statistical downscaling task, obtaining benefits from its stochastic properties. However, only one study has quantified the uncertainty in climate change projections. A simpler way to achieve uncertainty quantification is through deep ensembles, which consists of training several models in parallel on the same data and then combining the predictions through an aggregation operation, generally improving performance compared to a single model.\n\nIn this work, we explore a natural extension of previous studies through the use of deep ensembles, a simple technique that does not require tuning hyperparameters or modifying the model topology. This leads to a better quantification of uncertainty of climate change projections in future scenarios.\n\nExperimental Framework\n\nDue to the lack of observations in future periods, we rely on a pseudo-reality experiment to evaluate the effectiveness of deep ensembles for SD. This experiment is necessary to reveal whether the relationship learned in a historical period is able to extrapolate to future scenarios."
}