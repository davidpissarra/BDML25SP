{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" unseen species. To overcome these limitations, we propose leveraging bottom-up, open- domain learning frameworks as a resilient, scalable solution for image and video analysis in marine applications. Our preliminary demonstration uses pretrained vision-language models (VLMs) combined with retrieval-augmented generation (RAG) as grounding, leaving the door open for numerous architectural, training and engineering optimizations. We validate this approach through a preliminary application in classifying fish from video onboard fishing vessels, demonstrating impressive emergent retrieval and prediction capabilities without domain-specific training or knowledge of the task itself. 1 Introduction Reliable and timely data is critical for addressing climate challenges driving global biodiversity collapse, and threatening 3 billion livelihoods that depend on healthy oceans [1]. Thousands of cameras are deployed around the world underwater, on drones, on fishing vessels, and via satellite to monitor marine life and inform critical management tasks. The scale of the data they produce is already impossible for humans to handle. In fisheries monitoring alone, thousands of hours of video are generated per boat, making its manual review crushingly slow, not scalable, and unaffordable. [2]. Automated processing of visual data brings key insights about marine activity at an unprecedented scale. Traditional vision approaches, such as CNNs and basic vision transformers [3, 4], require extensive retraining for new environments and perform poorly when faced with rare or unseen species. These top-down methods struggle with generalization, domain transfer, and handling long-tailed distributions, and are infeasible for scalable ocean applications. Generalizable species identification stands to revolutionize marine management by providing comprehensive data on marine life distribution and their responses to climate change - vital for management of sustainable fisheries, invasive species, carbon sequestration, and ecosystem health. To overcome these challenges, we propose leveraging bottom-up, open-domain learning frameworks as a scalable and resilient solution, where knowledge about tuna species or oil spills are connected in a modular way, even after deployment. In this proposal, we show one accessible method for grounded, open-domain vision and highlight the promise of vision-language models (VLMs) combined with retrieval-augmented generation (RAG) [5–9]. This approach enhances generalization across diverse ∗Corresponding author: sepand@ondeck-ai.com. Website: https://ondeck.fish Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.environments and enables accurate identification of unseen species by integrating external knowledge during inference. 2 Related Work Traditional vision methods such as convolutional neural networks (CNNs), single-stage detectors, and vision transformers [4, 10, 11] have been the cornerstone of visual recognition. These top-down methods have shown reliable performance in closed-set, uni-domain climate applications, including environmental monitoring [12–14]. However, their effectiveness diminishes in generalized climate applications where data is often non-IID, and the distribution of objects and events is long-tailed or unexpected [15–18]. Recent advances in bottom-up, multi-modal approaches offer promising solutions to these challenges. Vision-language models (VLMs) based on CLIP and BLIP-2 [19, 20] learn representations of the world through contrastive learning and show emergent capabilities like detection, retrieval, question- answering and broader domain transfer. Note that this effectively bridges the modality gap between visual and textual data. These models excel in open-domain tasks such as visual question-answering (VQA) and retrieval [21, 22], which are crucial for climate monitoring and conservation [23]. The integration of grounding and retrieval-augmented generation (RAG) into these models further enhances their performance by allowing them to access and incorporate external knowledge during inference [5, 6, 24–26]. For instance, grounded CLIP [27] showcases emergent detection and identification capabilities that compete and surpass SOTA methods. Interestingly, Shen et al. [6] empirically indicates that biodiversity-related domains benefit most from transferable vision models injected with external knowledge (e.g. Flowers102 and OxfordPets). Grounding is key for open-ended VQA tasks, where external knowledge is required [21], but can also alleviate difficulties of class imbalance, domain shift and transfer, and interpretability. These advances make VLMs combined with RAG particularly promising for addressing the complex challenges posed by climate-related monitoring tasks. Despite these advancements, there is still limited work that systematically composes, refines, and scales these approaches across diverse real-world applications. Our work addresses this gap by exploring the application of these methods in marine conservation, where the ability to generalize and adapt to new environments is critical for effective and scalable species identification and marine monitoring. 3 Method Current vision research has produced powerful methods that can extrapolate and perform new tasks even without explicit training on them. While extremely generalizable, they cannot yet produce specialized outputs they have never seen in training, such as object classification to an unseen class. Thus grounding vision is critical as it provides access to external knowledge, and this is our focus for this preliminary work. We\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nTo address the limitations of current methods, we propose using bottom-up, open-domain learning frameworks as a scalable and resilient solution for image and video analysis in marine applications. Our preliminary demonstration utilizes pre-trained vision-language models (VLMs) combined with retrieval-augmented generation (RAG) as a grounding mechanism, leaving room for various architectural, training, and engineering optimizations. We validate this approach through a preliminary application in classifying fish from video footage captured on fishing vessels, demonstrating impressive emergent retrieval and prediction capabilities without domain-specific training or knowledge of the task itself.\n\nReliable and timely data is crucial for addressing climate challenges driving global biodiversity collapse, threatening 3 billion livelihoods that depend on healthy oceans. Thousands of cameras are deployed worldwide to monitor marine life and inform critical management tasks. The scale of the data they produce is already overwhelming for humans to handle. In fisheries monitoring alone, thousands of hours of video are generated per boat, making manual review slow, unscalable, and unaffordable. Automated processing of visual data brings key insights about marine activity at an unprecedented scale.\n\nTraditional vision approaches, such as CNNs and basic vision transformers, require extensive retraining for new environments and perform poorly when faced with rare or unseen species. These top-down methods struggle with generalization, domain transfer, and handling long-tailed distributions, making them infeasible for scalable ocean applications. Generalizable species identification has the potential to revolutionize marine management by providing comprehensive data on marine life distribution and their responses to climate change, which is vital for managing sustainable fisheries, invasive species, carbon sequestration, and ecosystem health.\n\nTo overcome these challenges, we propose leveraging bottom-up, open-domain learning frameworks as a scalable and resilient solution, where knowledge about tuna species or oil spills can be connected in a modular way, even after deployment. In this proposal, we showcase one accessible method for grounded, open-domain vision and highlight the promise of vision-language models (VLMs) combined with retrieval-augmented generation (RAG). This approach enhances generalization across diverse environments and enables accurate identification of unseen species by integrating external knowledge during inference.\n\nTraditional vision methods, such as convolutional neural networks (CNNs), single-stage detectors, and vision transformers, have been the cornerstone of visual recognition. However, their effectiveness diminishes in generalized climate applications where data is often non-IID, and the distribution of objects and events is long-tailed or unexpected. Recent advances in bottom-up, multi-modal approaches offer promising solutions to these challenges. Vision-language models (VLMs) based on CLIP and BLIP-2 learn representations of the world through contrastive learning and show emergent capabilities like detection, retrieval, question-answering, and broader domain transfer. These models excel in open-domain tasks such as visual question-answering (VQA) and retrieval, which are crucial for climate monitoring and conservation.\n\nThe integration of grounding and retrieval-augmented generation (RAG) into these models further enhances their performance by allowing them to access and incorporate external knowledge during inference. For instance, grounded CLIP showcases emergent detection and identification capabilities that compete and surpass state-of-the-art methods. Grounding is key for open-ended VQA tasks, where external knowledge is required, but can also alleviate difficulties of class imbalance, domain shift, and transfer, and interpretability.\n\nOur work addresses the gap in systematically composing, refining, and scaling these approaches across diverse real-world applications. We explore the application of these methods in marine conservation, where the ability to generalize and adapt to new environments is critical for effective and scalable species identification and marine monitoring."
}