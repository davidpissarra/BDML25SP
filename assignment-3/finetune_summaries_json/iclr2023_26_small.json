{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" paper proposes a global-local policy search method to train a reinforcement learning (RL) based controller which op- timizes building operation during both normal hours and demand response (DR) events. Experiments on a simulated five-zone commercial building demonstrate that by adding a local fine-tuning stage to the evolution strategy policy training process, the control costs can be further reduced by 7.55% in unseen testing sce- narios. Baseline comparison also indicates that the learned RL controller outper- forms a pragmatic linear model predictive controller (MPC), while not requiring intensive online computation. 1 I NTRODUCTION With the acceleration and exacerbation of global climate change, human society is in dire need of technologies for decarbonization and sustainable development to avoid any irreversible conse- quences caused to the earth. Grid-interactive efficient building (GEB) control enables resources at grid edge to be harnessed, and with the provided flexibility, power systems can be operated in a cleaner manner with higher efficiency. Thus, GEB is gaining traction in recent years. Direct load control (DLC), see (San Diego Gas & Electric, 2023) for an example, allows utility companies to directly control customers’ devices and is straightforward to implement. However, DLC does not explicitly consider specific building thermal condition and thus might jeopardize occupant comfort. Unlike DLC, MPC can combine building-centric and grid service objectives and achieves multi- objective optimal control, see (Drgo ˇna et al., 2020) for an extensive review. Despite the advantages of MPC, its massive deployment can be challenging. One of the reasons is that MPC requires on- demand computation to solve optimization problems during real time control (Zhang et al., 2021). Reinforcement learning (RL) policies, on the other hand, can be pre-trained offline and only require computationally cheap policy evaluation during real-time control. As a result, domain scientists are investigating using RL for optimal building control. For example, RL algorithms, including the deep Q-network (DQN) and asynchronous advantage actor-critic (A3C), are utilized for energy-saving while maintaining indoor comfort (Wei et al., 2017; Zhang et al., 2019). However, the discrete ac- tion spaces they employed usually require careful discretization to achieve good control performance and are more susceptible to the “curse of dimensionality” when applied to multi-zone control (Wei et al., 2017, Section 3.3). To use continuous action space, which greatly increases the policy search space and problem complexity, a Zap Q-learning method is leveraged in (Raman et al., 2020), though its application does not consider multi-zone building control. In addition, most prior building-RL studies are building-centric and do not consider enabling a building to be grid-interactive. In this paper, we investigate developing an RL controller for the most complex single building control problem studied in RL-building literature. To achieve this, a global-local policy search method is proposed, which strategically combines merits of two different types of RL algorithm, to allow policy to converge to a better performing local optimum in the non-convex policy searching process. A full version of this paper is published in Zhang et al. (2022). © [2023] IEEE. Reprinted, with permission, from “Two-Stage Reinforcement Learning Policy Search for Grid-Interactive Building Control,” IEEE Transactions on Smart Grid (V olume: 13, Issue: 3, May 2022). 12 G LOBAL -LOCAL POLICY SEARCH In deep RL, a policy network πθ(at|st)parameterized by θis trained to maximize its control per- formance J(θ) :=Eat∼πθ(P t∈Tγtrt). The evolution strategies (ES) algorithm (Salimans et al., 2017) achieves this by maximizing a Gaussian smoothed version of the original objective: V(ˆθ) :=Eθ∼N(ˆθ,σ2I)J(θ) =Eϵ∼N(0,I)J(ˆθ+σϵ), where θfollows an isotropic multivariate Gaussian distribution with fixed covariance, i.e., θ∼ N(ˆθ, σ2I), and ˆθis the mean parameter vector to be learned and σis a standard deviation, which controls the smoothness. ES updates ˆθviaˆθk+1=ˆθk+α∇V(ˆθ), and estimates ∇V(ˆθ)using zero-order gradient estimation (ZOE) (Nesterov & Spokoiny, 2017): ∇V(ˆθ)≈1 σEϵ∼N(0,I)[ϵ·J(ˆθ+σϵ)]. (1) According to Salimans et al. (2017), without the need for backpropagation, ES is highly scalable and requires less computation per episode\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nThis paper proposes a global-local policy search method to train a reinforcement learning (RL) based controller that optimizes building operation during both normal hours and demand response (DR) events. Experiments on a simulated five-zone commercial building demonstrate that by adding a local fine-tuning stage to the evolution strategy policy training process, the control costs can be further reduced by 7.55% in unseen testing scenarios. A baseline comparison also indicates that the learned RL controller outperforms a pragmatic linear model predictive controller (MPC), while not requiring intensive online computation.\n\nIntroduction\n\nWith the acceleration and exacerbation of global climate change, human society is in dire need of technologies for decarbonization and sustainable development to avoid irreversible consequences caused to the earth. Grid-interactive efficient building (GEB) control enables resources at the grid edge to be harnessed, and with the provided flexibility, power systems can be operated in a cleaner manner with higher efficiency. Thus, GEB is gaining traction in recent years. Direct load control (DLC) allows utility companies to directly control customers' devices and is straightforward to implement. However, DLC does not explicitly consider specific building thermal conditions and thus might jeopardize occupant comfort. Unlike DLC, MPC can combine building-centric and grid service objectives and achieves multi-objective optimal control. Despite the advantages of MPC, its massive deployment can be challenging due to the need for on-demand computation to solve optimization problems during real-time control.\n\nReinforcement learning (RL) policies can be pre-trained offline and only require computationally cheap policy evaluation during real-time control. As a result, domain scientists are investigating using RL for optimal building control. For example, RL algorithms, including the deep Q-network (DQN) and asynchronous advantage actor-critic (A3C), are utilized for energy-saving while maintaining indoor comfort. However, the discrete action spaces they employed usually require careful discretization to achieve good control performance and are more susceptible to the \"curse of dimensionality\" when applied to multi-zone control. To use continuous action space, which greatly increases the policy search space and problem complexity, a Zap Q-learning method is leveraged, though its application does not consider multi-zone building control. In addition, most prior building-RL studies are building-centric and do not consider enabling a building to be grid-interactive.\n\nIn this paper, we investigate developing an RL controller for the most complex single building control problem studied in RL-building literature. To achieve this, a global-local policy search method is proposed, which strategically combines the merits of two different types of RL algorithms, to allow the policy to converge to a better-performing local optimum in the non-convex policy searching process.\n\nGlobal-Local Policy Search\n\nIn deep RL, a policy network π(θ|s) parameterized by θ is trained to maximize its control performance J(θ) = E_{t∈T}γ^{t}r_{t}. The evolution strategies (ES) algorithm achieves this by maximizing a Gaussian smoothed version of the original objective: V(θ) = E_{θ∼N(θ,σ^{2}I)}J(θ) = E_{ϵ∼N(0,I)}J(θ + σϵ), where θ follows an isotropic multivariate Gaussian distribution with fixed covariance, i.e., θ ∼ N(θ, σ^{2}I), and θ is the mean parameter vector to be learned and σ is a standard deviation, which controls the smoothness. ES updates θ via θ_{k+1} = θ_{k} + α∇V(θ), and estimates ∇V(θ) using zero-order gradient estimation (ZOE): ∇V(θ) ≈ 1/σ E_{ϵ∼N(0,I)}[ϵ·J(θ + σϵ)]. According to Salimans et al. (2017), without the need for backpropagation, ES is highly scalable and requires less computation per episode."
}