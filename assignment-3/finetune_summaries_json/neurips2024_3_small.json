{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"ichard} @carnegiece.com Abstract Ocean wave energy is a reliable form of clean, renewable energy that has been under-explored compared to solar and wind. Wave Energy Converters (WEC) are devices that convert wave energy to electricity. To achieve a competitive Levelized Cost of Energy (LCOE), WECs require complex controllers to maximize the absorbed energy. Traditional engineering controllers, like spring-damper, cannot anticipate incoming waves, missing vital information that could lead to higher energy capture. Reinforcement Learning (RL) based controllers can instead optimize for long-term gains by being informed about the future waves. Prior works have utilized incoming wave information, achieving significant gains in energy capture. However, this has only been done via simulated waves (perfect prediction), making them impractical in real-life deployment. In this work, we develop a neural network based model for wave prediction. While prior works use auto-regressive techniques, we predict waves using information available on-device like position, acceleration, etc. We show that replacing the simulated waves with the wave predictor model can still maintain the gain in energy capture achieved by the RL controller in simulations. 1 Introduction Burning fossil fuel is a leading cause of climate change responsible for 75% of global greenhouse gas emissions UN (2023). To move away from fossil fuels, we need renewable and clean energy sources like wind, solar, and wave energy. Ocean waves are a reliable source of energy. Wave Energy Converters can harness this energy to produce electricity. These devices can either be submerged a few meters under the ocean, or can be deployed as floating devices. In this work, we focus on the CETO configuration Wikipedia contributors (2024). CETOs are submerged under the water and tethered to the ocean floor with mooring legs. The purpose of the mooring legs is two-fold: ensure CETO doesn’t drift with waves, but also capture power from the waves. As waves roll over the device, they apply forces to it. In order to allow relative motion between the seabed and the device, the the mooring legs have to extend to retract. This process is controlled by each leg’s respective Power Take Off (PTO). Onboard the device, the mooring legs terminate onto a drum, which is connected to an electrical generator. The generator, by varying the torque it applies to the ∗Corresponding author. †These authors contributed equally. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.drum, controls how much and how fast its mooring leg extends or retracts, which dictates how the BA moves. The power generated by each leg is proportional to the product between the leg’s extension / retraction velocity, and the tension in the leg. The power generated by CETO is the sum of each leg’s generated power. Passive controllers rely on hourly to half-hourly average values of the wave height and time period (Tp) to perform this task, which means that the controller cannot anticipate and react to optimise the devices’ response to individual waves. Figure 1: The CETO 6 configuration of WEC.Deep Reinforcement Learning has been widely and successfully used for continuous control tasks. RL controllers have already been devel- oped for wave energy converters (Sarkar et al., 2023, 2022a,b,c, 2021), (Anderlini et al., 2016), (Anderlini et al., 2020). In this work, we follow the design in (Sarkar et al., 2022a), which uses a RL controller trained with Proximal Policy Opti- mization Schulman et al. (2017) with the GTrXL Parisotto et al. (2020) transformer architecture as the policy network. Our contribution is as follows: •We analyze the change in performance with and without knowledge of incom- ing waves. •We then develop a wave predictor model. •We show that using the predictions from the model causes negligible loss in the RL controller performance. 2 RL Control with Wave Predictions 2.1 Reinforcement Learning for WEC A standard reinforcement learning setup consists of an agent interacting with an environment in discrete timesteps. At each step, the agent receives an observation, takes an action, and receives a scalar reward. The agent’s behaviour is defined by a policy. The goal in reinforcement learning is to maximize the sum of rewards over a time horizon. In the WEC environment, the observation space consists of variables that define the movement of the device in 6 degrees of freedom, and information about the waves. The action space is the controller force applied on the three legs. The reward is the sum of instantaneous power generated in the three legs. The RL agent learns to take actions to maximize the generated power. The RL agent is typically trained in a simulator where it learns from thousands of episodes over different wave states. Along with the position, velocity, acceleration of the device, and the extension the legs, (Sarkar\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nRichard @carnegie.com Abstract\nOcean wave energy is a reliable form of clean, renewable energy that has been under-explored compared to solar and wind energy. Wave Energy Converters (WECs) are devices that convert wave energy to electricity. To achieve a competitive Levelized Cost of Energy (LCOE), WECs require complex controllers to maximize the absorbed energy. Traditional engineering controllers, like spring-damper systems, cannot anticipate incoming waves, missing vital information that could lead to higher energy capture. Reinforcement Learning (RL) based controllers can instead optimize for long-term gains by being informed about the future waves. Prior works have utilized incoming wave information, achieving significant gains in energy capture. However, this has only been done via simulated waves (perfect prediction), making them impractical in real-life deployment. In this work, we develop a neural network-based model for wave prediction. While prior works use auto-regressive techniques, we predict waves using information available on-device, such as position, acceleration, etc. We show that replacing simulated waves with the wave predictor model can still maintain the gain in energy capture achieved by the RL controller in simulations.\n\nIntroduction\nBurning fossil fuel is a leading cause of climate change, responsible for 75% of global greenhouse gas emissions (UN, 2023). To move away from fossil fuels, we need renewable and clean energy sources like wind, solar, and wave energy. Ocean waves are a reliable source of energy. Wave Energy Converters can harness this energy to produce electricity. These devices can be submerged a few meters under the ocean or deployed as floating devices. In this work, we focus on the CETO configuration (Wikipedia contributors, 2024). CETO devices are submerged under the water and tethered to the ocean floor with mooring legs. The purpose of the mooring legs is two-fold: to ensure CETO doesn't drift with waves and to capture power from the waves. As waves roll over the device, they apply forces to it. To allow relative motion between the seabed and the device, the mooring legs have to extend and retract. This process is controlled by each leg's respective Power Take Off (PTO). Onboard the device, the mooring legs terminate onto a drum, which is connected to an electrical generator. The generator, by varying the torque it applies to the drum, controls how much and how fast its mooring leg extends or retracts, which dictates how the device moves. The power generated by each leg is proportional to the product between the leg's extension/retraction velocity and the tension in the leg. The power generated by CETO is the sum of each leg's generated power. Passive controllers rely on hourly to half-hourly average values of the wave height and time period (Tp) to perform this task, which means that the controller cannot anticipate and react to optimize the device's response to individual waves.\n\nFigure 1: The CETO 6 configuration of WEC.\n\nDeep Reinforcement Learning has been widely and successfully used for continuous control tasks. RL controllers have already been developed for wave energy converters (Sarkar et al., 2023, 2022a, b, c, 2021), (Anderlini et al., 2016), (Anderlini et al., 2020). In this work, we follow the design in (Sarkar et al., 2022a), which uses a RL controller trained with Proximal Policy Optimization (Schulman et al., 2017) with the GTrXL Parisotto et al. (2020) transformer architecture as the policy network. Our contribution is as follows:\n\n* We analyze the change in performance with and without knowledge of incoming waves.\n* We develop a wave predictor model.\n* We show that using the predictions from the model causes negligible loss in the RL controller performance.\n\n2 RL Control with Wave Predictions\n\n2.1 Reinforcement Learning for WEC\nA standard reinforcement learning setup consists of an agent interacting with an environment in discrete timesteps. At each step, the agent receives an observation, takes an action, and receives a scalar reward. The agent's behavior is defined by a policy. The goal in reinforcement learning is to maximize the sum of rewards over a time horizon. In the WEC environment, the observation space consists of variables that define the movement of the device in 6 degrees of freedom, and information about the waves. The action space is the controller force applied on the three legs. The reward is the sum of instantaneous power generated in the three legs. The RL agent learns to take actions to maximize the generated power. The RL agent is typically trained in a simulator where it learns from thousands of episodes over different wave states. Along with the position, velocity, acceleration of the device, and the extension of the legs,"
}