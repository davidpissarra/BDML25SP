{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\". However, identifying species calls in large datasets by hand is an expensive task, leading to the development of machine learning techniques to reduce cost. Due to a lack of annotated datasets of sound- scape recordings, these models are often trained on large databases of community created focal recordings. A challenge of training on such data is that clips are given a “weak label,” a single label that represents the whole clip. This includes segments with background noise, anthropogenic sounds, and difference species that are labeled as calls in the training data, reducing model performance. Heuristic methods exist to convert clip-level labels to “strong” call-specific labels, where the label tightly bounds the temporal length of the call and better identifies bird vocalizations. Our work improves on the current weakly to strongly labeled method used on the training data for BirdNET, the current most popular model for audio species classification. We utilize an existing RNN-CNN hybrid, resulting in a precision improvement of 12% (going to 90% precision) against our new strongly hand-labeled dataset of Peruvian bird species. 1 Introduction Climate change threatens to cause devastating biodiversity loss in regions such as the Amazon, which hosts about a quarter of all global biodiversity and more than half of the world’s remaining tropical forests. To monitor the biodiversity of such vast and dense regions, ecologists are relying more on passive acoustic monitoring [1, 2]. Compared to traditional techniques such as transects, point counts, and captures, acoustic monitoring scales better with large datasets, is less invasive, and is cheaper thanks to autonomous recording units such as AudioMoths [3 –10]. Therefore, acoustic monitoring presents a better way to monitor the effects of climate change. However, the volume of data produced makes manual labeling of audio data impractical and expensive. In order to greatly reduce the cost for researchers using passive acoustic monitoring to study bird communities, we can use automated techniques such as deep learning [3, 11, 12]. One of the most popular models for automatic bird species classification is BirdNET, a convolutional neural network (CNN) capable of classifying more than 6,000 species and trained on large bird 1University of California San Diego, La Jolla, California, USA 2University of California Merced, Merced, California, USA 3Demining Research Community, New York, New York, USA 4Population Sustainability, San Diego Zoo Wildlife Alliance, Escondido, California, USA Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.species focal-recording databases, such as xeno-canto and the Macaulay Library [13 –15]. The major downside of these databases is that files are weakly labeled, meaning an entire audio file is labeled with its species. There is no indication of how much of the file contains vocalizations of the species of interest. Thus, training data will frequently include extraneous noise, such as other vocalizations, anthropogenic sounds, or environmental noise. This is in contrast to strongly labeled data, for which the exact start and end times of the call are labeled, allowing the removal of segments where the bird is not calling. To handle weakly labeled data, it has become common practice in bioacoustics to feed training data through a detection algorithm to convert weak labels into strong labels before presenting them to the classifier [16]. However, most of these detection algorithms (discussed in section 2) are rooted in signal processing, causing reliance on signal-to-noise ratio (SNR) or other static features. This in turn leads to false detections of non-bioacoustic events. We propose a prepossessing technique using deep learning to convert focal recordings into strongly labeled annotations for training data that improves upon currently used signal processing techniques. 2 Related Work The weak-to-strong label (WTS) technique used by BirdNET is background foreground separation, a digital signal processing method based on the premise that focal recordings have a high SNR [17 –19]. Assuming the peaks of a given signal are when the weakly labeled species vocalizes, identifying signal peaks will output strong labels for the bird calls. Other techniques also depend on amplitude or other such static features for bioacoustic sound event detection. One approach employs a threshold for amplitude and duration for detecting bird note onsets and offsets [20]. In another, a wavelet peak detector was used to find frames with high energy, which each had a 6-second window drawn around it. The 5 windows with the highest peaks were then extracted [21]. However, given the crowdsourced nature of these focal recordings, such assumptions may not always hold during other loud sound events, such as a tree branch falling, gunshots, or human vocalizations. These cases create false positives in the training data which may lead to downstream errors and inefficiencies during training. 3 Methodology Following the same assumptions as BirdNET’s signal strength estimator, we assume the weakly labeled bird is the most prominent bird in the audio recording. Therefore, any positive bird detection by the W\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy parts removed:\n\nIdentifying species calls in large datasets by hand is an expensive task, leading to the development of machine learning techniques to reduce costs. However, due to a lack of annotated datasets of soundscapes, these models are often trained on large databases of community-created focal recordings. A challenge of training on such data is that clips are given a \"weak label,\" a single label that represents the whole clip, including segments with background noise, anthropogenic sounds, and different species labeled as calls in the training data, which reduces model performance.\n\nHeuristic methods exist to convert clip-level labels to \"strong\" call-specific labels, where the label tightly bounds the temporal length of the call and better identifies bird vocalizations. Our work improves on the current weakly to strongly labeled method used on the training data for BirdNET, the current most popular model for audio species classification. We utilize an existing RNN-CNN hybrid, resulting in a precision improvement of 12% (to 90% precision) against our new strongly hand-labeled dataset of Peruvian bird species.\n\nClimate change threatens to cause devastating biodiversity loss in regions such as the Amazon, which hosts about a quarter of all global biodiversity and more than half of the world's remaining tropical forests. To monitor the biodiversity of such vast and dense regions, ecologists are relying more on passive acoustic monitoring. Compared to traditional techniques such as transects, point counts, and captures, acoustic monitoring scales better with large datasets, is less invasive, and is cheaper thanks to autonomous recording units such as AudioMoths. Therefore, acoustic monitoring presents a better way to monitor the effects of climate change.\n\nHowever, the volume of data produced makes manual labeling of audio data impractical and expensive. In order to greatly reduce the cost for researchers using passive acoustic monitoring to study bird communities, we can use automated techniques such as deep learning. One of the most popular models for automatic bird species classification is BirdNET, a convolutional neural network (CNN) capable of classifying more than 6,000 species and trained on large bird species focal-recording databases, such as xeno-canto and the Macaulay Library.\n\nThe major downside of these databases is that files are weakly labeled, meaning an entire audio file is labeled with its species, with no indication of how much of the file contains vocalizations of the species of interest. This leads to training data that frequently includes extraneous noise, such as other vocalizations, anthropogenic sounds, or environmental noise. This is in contrast to strongly labeled data, for which the exact start and end times of the call are labeled, allowing the removal of segments where the bird is not calling.\n\nTo handle weakly labeled data, it has become common practice in bioacoustics to feed training data through a detection algorithm to convert weak labels into strong labels before presenting them to the classifier. However, most of these detection algorithms are rooted in signal processing, causing reliance on signal-to-noise ratio (SNR) or other static features. This leads to false detections of non-bioacoustic events. We propose a preprocessing technique using deep learning to convert focal recordings into strongly labeled annotations for training data that improves upon currently used signal processing techniques.\n\nThe weak-to-strong label (WTS) technique used by BirdNET is background foreground separation, a digital signal processing method based on the premise that focal recordings have a high SNR. However, this assumption may not always hold during other loud sound events, such as a tree branch falling, gunshots, or human vocalizations, which create false positives in the training data and may lead to downstream errors and inefficiencies during training.\n\nOur methodology follows the same assumptions as BirdNET's signal strength estimator, assuming the weakly labeled bird is the most prominent bird in the audio recording. Therefore, any positive bird detection by the WTS technique is considered a strong label."
}