{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"- ability by using a large-scale battery. In this paper, we propose a novel strat- egy called error compensable forecasting. We switch the objective of forecast- ing from reducing errors to making errors compensable by leveraging a battery. Speciﬁcally, we propose a deep reinforcement learning based framework having forecasting in the loop of control. Extensive simulations show that the proposed one-hour ahead forecasting achieves zero error for more than 98% of time while reducing the operational expenditure by up to 44%. 1 I NTRODUCTION The Paris Agreement has recently stressed the necessity of using renewable energy instead of fos- sil fuels to prevent climate change. As a result, global penetration of renewable energy rapidly increases, but renewable power outputs heavily depend on weather conditions such as clouds, tem- perature, and humidity. This causes substantial uncertainties, which brings adverse effects on eco- nomic beneﬁt and the stability of power grids. Thus, accurate renewable energy forecasting tech- niques are required to integrate the renewable energy into the power grids and ultimately prevent climate change. Recently, deep learning based renewable generation forecasting techniques have been proposed (Cardona et al., 2019; Mathe et al., 2019; Jeong & Kim, 2019) and show signiﬁcantly improved performances compared to conventional machine learning based schemes. However, fore- casting always induces errors, and large-scale energy storages such as lithium-ion batteries are used to compensate forecasting errors (Bae et al., 2016; Gholami et al., 2018; Bae et al., 2019). The basic idea is such that over-forecasting errors are compensated by discharging energy from the battery while under-forecasting errors are resolved by charging excessive generation into the battery. Traditional forecasting methods commonly aimed to minimize the forecasting errors. They used the mean squared error (MSE) as an objective function in training process. Since there is a squared term in the MSE, training is processed without considering whether the errors are positive or negative. However, reducing errors does not necessarily imply compensable errors. For example, suppose that the battery is empty, i.e., discharging is not possible but charging battery is possible. Then, over- forecasting is not allowed because the battery cannot compensate error by discharging. By contrast, under-forecasting is compensable because excessive generation can be stored in the battery. For the same reason, under-forecasting is not allowed when the battery is fully charged. Nevertheless, existing forecasting algorithms do not consider whether the errors are positive or negative but just reduce the distance between forecasting and real values. Consequently, none of the previous works considered error compensability by using the battery. In this regard, we propose a novel strategy called error compensable forecasting (ECF). We switch the objective of forecasting from reducing errors to making compensable errors. The challenging part of developing ECF lies in that the stored energy at current time is affected by the previous forecasting result. Hence, time-coupling exists between forecasting and battery control, and fore- casting should be in the loop of sequential decision making. We tackle this problem by leveraging reinforcement learning (RL) that has interaction between an agent and the environment where ac- 1Published as a conference paper at ICLR 2020 tions of the agent affect the subsequent data it receives (Sutton et al., 1998). In our framework, an action is a continuous forecasted value, and it requires a continuous action space. To enable the continuous control, we leverage the state-of-the-art deep reinforcement learning (DRL) algorithm called proximal policy optimization (PPO), which is known to be simpler to implement than other DRL algorithms with outstanding performance (Schulman et al., 2017). Our extensive simulations with real solar and wind power generation data conﬁrm that the proposed framework outperforms the traditional forecasting and achieves zero error for more than 98% of time for one-hour ahead forecasting when the maximum battery capacity is 0.5 p.u, i.e., a half of the installed generation capacity. 2 M ETHODS 2.1 B ATTERY OPERATION In this section, we present a practical battery model for ECF applications. For simplicity we focus on one-hour ahead forecasting because renewable energy providers can resubmit their bids one- hour ahead of the operation hour in a renewable energy market (Bae et al., 2019). Since battery degradation is known to be severe at both ends of the state-of-charge (SoC), i.e., either empty or full, the stored energy denoted by Etat time slottshould be constrained by (Choi & Kim, 2016) Emax\u0001SoC min\u0014Et\u0014Emax\u0001SoC max; (1) where SoC minand SoC maxdenote the minimum and maximum SoC of the battery, and Emaxde- notes the maximum battery capacity. From (1), the charging and discharging power limitation\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nThe ability to use a large-scale battery is proposed in this paper. We introduce a novel strategy called error compensable forecasting. The objective of forecasting is shifted from reducing errors to making errors compensable by leveraging a battery. Specifically, we propose a deep reinforcement learning-based framework that incorporates forecasting into the control loop. Extensive simulations show that the proposed one-hour ahead forecasting achieves zero error for more than 98% of the time while reducing operational expenditure by up to 44%.\n\nThe Paris Agreement has emphasized the need to use renewable energy instead of fossil fuels to prevent climate change. As a result, the global penetration of renewable energy is increasing rapidly, but renewable power outputs heavily depend on weather conditions such as clouds, temperature, and humidity. This causes substantial uncertainties, which have adverse effects on economic benefits and the stability of power grids. Thus, accurate renewable energy forecasting techniques are required to integrate renewable energy into power grids and prevent climate change.\n\nRecently, deep learning-based renewable generation forecasting techniques have been proposed and show significantly improved performances compared to conventional machine learning-based schemes. However, forecasting always induces errors, and large-scale energy storage systems such as lithium-ion batteries are used to compensate for forecasting errors. The basic idea is that over-forecasting errors are compensated by discharging energy from the battery, while under-forecasting errors are resolved by charging excessive generation into the battery.\n\nTraditional forecasting methods commonly aimed to minimize forecasting errors. They used the mean squared error (MSE) as an objective function in the training process. However, reducing errors does not necessarily imply compensable errors. For example, if the battery is empty, over-forecasting is not allowed because the battery cannot compensate for the error by discharging. By contrast, under-forecasting is compensable because excessive generation can be stored in the battery. Existing forecasting algorithms do not consider whether the errors are positive or negative but just reduce the distance between forecasting and real values.\n\nIn this regard, we propose a novel strategy called error compensable forecasting (ECF). We shift the objective of forecasting from reducing errors to making compensable errors. The challenging part of developing ECF lies in that the stored energy at the current time is affected by the previous forecasting result. Hence, time-coupling exists between forecasting and battery control, and forecasting should be in the loop of sequential decision making.\n\nWe tackle this problem by leveraging reinforcement learning (RL) that has interaction between an agent and the environment where actions of the agent affect the subsequent data it receives. In our framework, an action is a continuous forecasted value, and it requires a continuous action space. To enable the continuous control, we leverage the state-of-the-art deep reinforcement learning (DRL) algorithm called proximal policy optimization (PPO), which is known to be simpler to implement than other DRL algorithms with outstanding performance.\n\nOur extensive simulations with real solar and wind power generation data confirm that the proposed framework outperforms traditional forecasting and achieves zero error for more than 98% of the time for one-hour ahead forecasting when the maximum battery capacity is 0.5 p.u., i.e., a half of the installed generation capacity.\n\n2.1 Battery Operation\n\nIn this section, we present a practical battery model for ECF applications. For simplicity, we focus on one-hour ahead forecasting because renewable energy providers can resubmit their bids one-hour ahead of the operation hour in a renewable energy market. Since battery degradation is known to be severe at both ends of the state-of-charge (SoC), the stored energy denoted by Et at time slot t should be constrained by:\n\nEmax - SoC_min <= Et <= Emax - SoC_max\n\nwhere SoC_min and SoC_max denote the minimum and maximum SoC of the battery, and Emax denotes the maximum battery capacity.\n\nFrom this constraint, the charging and discharging power limitations can be derived."
}