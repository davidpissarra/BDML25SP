{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" programs — have the potential to benefit from the use of large-scale datasets or simulations together with intelligent algorithms. These could, if designed and deployed in a way that is well grounded on scientific evidence, enable a more comprehensive, faster, and rigorous approach to policy making. Integrated Assessment Models (IAM) is a broad umbrella covering scientific models that attempt to link main features of society and economy with the biosphere into one modelling framework. At present, these systems are probed by policy makers and advisory groups in a hypothesis-driven manner. In this paper, we empirically demonstrate that modern Reinforcement Learning can be used to probe IAMs and explore the space of solutions in a more principled manner. While the implication of our results are modest since the environment is simplistic, we believe that this is a stepping stone towards more ambitious use cases, which could allow for effective exploration of policies and understanding of their consequences and limitations. 1 Introduction Climate is a high dimensional dynamical system with strong inter-dependent components and long time dependencies, all of which interact to produce highly non-linear responses and behavior. Climate is also highly conditioned on human behavior – another greatly complex system – such that is now necessary to reason about climate change from a socio-climatic perspective [Moore et al., 2022]. To make progress towards achieving some kind of solution in the face of extreme consequences, policymakers and advisory groups employ Integrated Assessment Models (IAMs), state of the art models for climate change that combine knowledge about human development (such as economical theories) together with planetary sciences such as ecology and geophysics [Parson and Fisher-Vanden, 1997]. Exploring and analysing the properties of the IAMs employed for large scale assessments [Pörtner et al., 2022] - to e.g. measure fidelity against the real world – is generally intractable from a computational perspective, which leads researchers to implement poor simplifying assumptions and decrease their effectiveness [Asefi-Najafabady et al., 2021]. Smaller IAM models aim to provide an alternative by employing fewer state variables and simpler sets of dynamics, making them amenable to mathematical probing and analysis [Kittel et al., 2017, Nitzbon et al., 2017]. The Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.literature commonly explores these models with ODE solvers; however recently [Strnad et al., 2019] has shown that it is possible to employ them as environments in standard Reinforcement Learning (RL) [Sutton and Barto, 2020], and explore models using trained policies. These can be used to understand the system, and provide upwards insight towards improving more complex IAMs or even our understanding of climate change policies. We build on this work, testing more RL algorithms, reward functions, as well as different experimental setups. Among others, we show that (a) modern RL can learn effective policies with a variety of reward functions in this environment (b) that different agents and reward functions generate a significantly diverse set of solutions, thus exploring the IAM in different manners, and (c) that it is necessary to apply care when designing reward functions as they show different success rate in reaching the desirable state for different initialisation points, finally - (d) that RL helps us gain a deeper understanding of the properties and limitations of the applied models. 2 AYS environment, RL reward functions and agents We employ the AYS model [Kittel et al., 2017], which we use to create a Markov Decision Process following Strnad et al. [2019]. AYS is governed by three coupled differential equations: dA dt=1 1 + (S/σ)ρY ϕϵ−A/τA,dY dt=βY−θAY,dS dt=\u0012 1−1 1 + (S/σ)ρ\u0013Y ϵ−S/τS,(1) where Ais the excess atmospheric carbon, Ysociety’s economic output, and Sthe renewable knowl- edge stock. At each step t, the agent observes a vector (At, Yt, St), which is a partial representation of the state of the system. This dynamical system contains two attractors, a green fixed point, where renewable knowledge and economic output grow forever, and a black fixed point, where the economic output is stagnant and there is a large amount of excess carbon in the atmosphere (Figure 1). Broadly speaking, the former is a positive end state (and effectively encodes the high level goal), while the latter is undesirable. See Appendix A for a more detailed technical aspects of the environment. The agent can take four actions, each corresponding to a high level government policy decision and changes to the dynamics of the environment (See Table 1 for a mathematical and textual description of the actions). Reward functions The reward signal that agents receives from taking actions in this environment is based on planetary boundaries (PBs), quantitative physical and eco- nomical\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy and dirty parts removed:\n\nPrograms have the potential to benefit from the use of large-scale datasets or simulations combined with intelligent algorithms. These could enable a more comprehensive, faster, and rigorous approach to policy making if designed and deployed based on scientific evidence. Integrated Assessment Models (IAMs) is a broad framework that links societal and economic features with the biosphere. Currently, these systems are probed by policymakers and advisory groups in a hypothesis-driven manner. This paper empirically demonstrates that modern Reinforcement Learning can be used to probe IAMs and explore the space of solutions in a more principled manner.\n\nClimate is a high-dimensional dynamical system with strong interdependent components and long-term dependencies, producing highly non-linear responses and behavior. Climate is also highly conditioned on human behavior, requiring a socio-climatic perspective to reason about climate change. Policymakers and advisory groups employ IAMs, state-of-the-art models for climate change that combine knowledge about human development and planetary sciences. Exploring and analyzing the properties of IAMs used for large-scale assessments is computationally intractable, leading researchers to implement simplifying assumptions that decrease their effectiveness.\n\nSmaller IAM models aim to provide an alternative by using fewer state variables and simpler dynamics, making them amenable to mathematical probing and analysis. Recent work has shown that IAMs can be employed as environments in standard Reinforcement Learning, allowing for the exploration of models using trained policies. These can be used to understand the system and provide insights towards improving more complex IAMs or our understanding of climate change policies.\n\nWe build on this work, testing more RL algorithms, reward functions, and experimental setups. Our results show that (a) modern RL can learn effective policies with various reward functions in this environment, (b) different agents and reward functions generate a diverse set of solutions, exploring the IAM in different manners, (c) it is necessary to apply care when designing reward functions as they show different success rates in reaching the desirable state for different initialisation points, and (d) RL helps gain a deeper understanding of the properties and limitations of the applied models.\n\n2. AYS Environment, RL Reward Functions, and Agents\n\nWe employ the AYS model, which we use to create a Markov Decision Process. AYS is governed by three coupled differential equations:\n\ndA/dt = 1 + (S/σ)ρY ϕϵ - A/τA\ndY/dt = βY - θAY\ndS/dt = 1 - 1 + (S/σ)ρY ϵ - S/τS\n\nwhere A is the excess atmospheric carbon, Y is society's economic output, and S is the renewable knowledge stock. At each step t, the agent observes a vector (At, Yt, St), which is a partial representation of the system's state. This dynamical system contains two attractors: a green fixed point, where renewable knowledge and economic output grow forever, and a black fixed point, where the economic output is stagnant and there is a large amount of excess carbon in the atmosphere.\n\nThe agent can take four actions, each corresponding to a high-level government policy decision and changes to the environment's dynamics. The reward signal that agents receive from taking actions in this environment is based on planetary boundaries, quantitative physical, and economic indicators."
}