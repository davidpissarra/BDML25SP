{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" been transformational to the ﬁeld of natural language processing. While similar mod- els have also been trained on large corpuses of im- ages, they are not well suited for remote sensing data. To stimulate the development of foundation models for Earth monitoring, we propose to de- velop a new benchmark comprised of a variety of downstream tasks related to climate change. We believe that this can lead to substantial improve- ments in many existing applications and facilitate the development of new applications. This pro- posal is also a call for collaboration with the aim of developing a better evaluation process to mit- igate potential downsides of foundation models for Earth monitoring. 1. Introduction Earth monitoring with machine learning-based methods plays an increasing role in climate change mitigation and adaptation as well as climate science (Rolnick et al., 2019). Applications include methane source detection (Sheng et al., 2020; Dileep et al., 2020), forest carbon quantiﬁcation (Lüt- jens et al., 2019), deforestation monitoring (Finer et al., 2018; Dao et al.), ﬂood detection (Mateo-Garcia et al., 2021), extreme weather prediction (McGovern et al., 2017), wildﬁre detection (Jain et al., 2020), and crop monitor- ing (Kerner et al., 2020; Dado et al., 2020). Across many 1Element AI / Service Now2Stanford University3University of Maryland4Radiant Earth Foundation5MIT6ETH Zurich. Correspondence to: Alexandre Lacoste <alexan- dre.lacoste@servicenow.com>. Proceedings of the 38thInternational Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).of these applications, pre-trained models (e.g., a ResNet trained on ImageNet) are used to increase generalisation performance. Improvement of the pre-trained models is shown to reduce the need for large labelled datasets in some contexts (Chen et al., 2020) and can improve model gen- eralisation outside of the training distribution (Hendrycks et al., 2019). Recent studies exploring the scaling of such pre-trained models found that increasing the size of an unsu- pervised (or weakly supervised) dataset as well as properly scaling the model led to an even greater increase in perfor- mances under various metrics (Kaplan et al., 2020; Radford et al., 2021). While the training of such large-scale models is usually re- served for industrial research labs with very large computer clusters, the publication of the pre-trained models opens op- portunities to the rest of the community. These pre-trained models were recently coined as foundation models (Bom- masani et al., 2021) as they might serve as foundations for sub-ﬁelds of machine learning. Speciﬁcally, the publica- tion of large pre-trained models like BERT (Devlin et al., 2018), and GPT-3 (Brown et al., 2020) led to a paradigm shift in the ﬁeld of natural language processing (NLP). This inspired a similar shift in the ﬁeld of computer vision with the release of models like CLIP (Radford et al., 2021) and DINO (Caron et al., 2021). While CLIP performs well on various types of vision tasks, it is still under-performing on Earth monitoring tasks (Radford et al., 2021). This is not surprising as it is trained mainly on RGB images taken from a ground perspective, rather than multispectral bands taken from an overhead perspective prevalent in remote sensing data. This suggests that there is still untapped potential for foundation models to beneﬁt the ﬁeld Earth monitoring as it has done for NLP and computer vision. Foundation models also come with downsides. Speciﬁcally, large language models are known to amplify and perpetuate biases (Bender et al., 2021) and have high CO 2e emissions associated with their training (Strubell et al., 2019; Pat- terson et al., 2021). Recently, an interdisciplinary group of researchers published a collective work discussing the risks and opportunities of foundation models (BommasaniToward Foundation Models for Earth Monitoring et al., 2021). This study highlighted that the relevant stake- holders are often not well represented during the design of foundation models. In addition, the increased accessibility of foundation models can lead to the development of un- expected applications with potential positive and negative impacts. To mitigate potential negative impacts, we suggest an open evaluation procedure early in the process. To this end, we propose a benchmark dataset and evaluation process to facilitate the development of foundation models in Earth monitoring. We will aggregate a collection of downstream tasks\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nThe development of foundation models has been transformational to the field of natural language processing. While similar models have been trained on large corpora of images, they are not well-suited for remote sensing data. To stimulate the development of foundation models for Earth monitoring, we propose a new benchmark consisting of a variety of downstream tasks related to climate change. We believe this can lead to substantial improvements in many existing applications and facilitate the development of new applications. This proposal is also a call for collaboration to develop a better evaluation process to mitigate potential downsides of foundation models for Earth monitoring.\n\n1. Introduction\n\nEarth monitoring with machine learning-based methods plays an increasing role in climate change mitigation and adaptation as well as climate science (Rolnick et al., 2019). Applications include methane source detection (Sheng et al., 2020; Dileep et al., 2020), forest carbon quantification (Lütjens et al., 2019), deforestation monitoring (Finer et al., 2018; Dao et al.), flood detection (Mateo-Garcia et al., 2021), extreme weather prediction (McGovern et al., 2017), wildfire detection (Jain et al., 2020), and crop monitoring (Kerner et al., 2020; Dado et al., 2020). Across many of these applications, pre-trained models (e.g., a ResNet trained on ImageNet) are used to increase generalization performance. Improvement of the pre-trained models is shown to reduce the need for large labeled datasets in some contexts (Chen et al., 2020) and can improve model generalization outside of the training distribution (Hendrycks et al., 2019). Recent studies exploring the scaling of such pre-trained models found that increasing the size of an unsupervised (or weakly supervised) dataset as well as properly scaling the model led to an even greater increase in performance under various metrics (Kaplan et al., 2020; Radford et al., 2021).\n\nWhile the training of such large-scale models is usually reserved for industrial research labs with very large computer clusters, the publication of the pre-trained models opens opportunities to the rest of the community. These pre-trained models were recently coined as foundation models (Bommasani et al., 2021) as they might serve as foundations for subfields of machine learning. Specifically, the publication of large pre-trained models like BERT (Devlin et al., 2018), and GPT-3 (Brown et al., 2020) led to a paradigm shift in the field of natural language processing (NLP). This inspired a similar shift in the field of computer vision with the release of models like CLIP (Radford et al., 2021) and DINO (Caron et al., 2021). While CLIP performs well on various types of vision tasks, it is still underperforming on Earth monitoring tasks (Radford et al., 2021). This is not surprising as it is trained mainly on RGB images taken from a ground perspective, rather than multispectral bands taken from an overhead perspective prevalent in remote sensing data. This suggests that there is still untapped potential for foundation models to benefit the field of Earth monitoring as it has done for NLP and computer vision.\n\nFoundation models also come with downsides. Specifically, large language models are known to amplify and perpetuate biases (Bender et al., 2021) and have high CO2 emissions associated with their training (Strubell et al., 2019; Patterson et al., 2021). Recently, an interdisciplinary group of researchers published a collective work discussing the risks and opportunities of foundation models (Bommasani et al., 2021). This study highlighted that the relevant stakeholders are often not well-represented during the design of foundation models. In addition, the increased accessibility of foundation models can lead to the development of unexpected applications with potential positive and negative impacts. To mitigate potential negative impacts, we suggest an open evaluation procedure early in the process. To this end, we propose a benchmark dataset and evaluation process to facilitate the development of foundation models in Earth monitoring. We will aggregate a collection of downstream tasks."
}