{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" in the form of tensors that are both higher dimensional and of larger size than the RGB imagery one encounters in many computer vision experiments. This raises scaling issues and brings up the need to leverage available compute resources efficiently. Motivated by results on masked autoencoders, we show that it is possible to use sampling of subtensors as the sole augmentation strategy for contrastive learning with a sampling ratio of ∼1%. This is to be compared to typical masking ratios of 75% or90% for image and video data respectively. In an ablation study, we explore extreme sampling ratios and find comparable skill for ratios as low as ∼0.0625%. Pursuing efficiencies, we are finally investigating whether it is possible to generate robust embeddings on dimension values which were not present at training time. We answer this question to the positive by using learnable position encoders which have continuous dependence on dimension values. 1 Introduction Following numerous breakthroughs in deep learning, there has been a growing trend to adapt techniques used in computer vision [ 1,2] to other domains, including in weather and climate applications [ 3,4,5,6]. The type of data used, however, is complex due to its high-dimensional nature, especially compared to standard RGB images. For instance, the ERA5 reanalysis [ 7], spans the globe with extensive number parameters and atmospheric levels, resulting in GB-scale tensors at each time step. This research delves into resource-efficient self-supervised representation learning using ERA5 data, employing invariance-based methods like SimCLR [ 8,9] and MoCo [ 10]. In particular, we use sampling as the sole data augmentation technique and impose resource constraints of 24-hour train time on a single A100 GPU with 40GB memory. Across multiple downstream ∗Work done while an intern at IBM Research. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.4D samplingLevel Longitude x Latitude Parameter2D samplingLevel Longitude x Latitude Parameter3D samplingLevel Longitude x Latitude ParameterFigure 1: ( left) Traditional second-order sampling generates a sequence of subtensors by tiling along the two horizontal dimensions. ( center andright ) We sample subtensors that extend along all three spatial or even all four dimensions; which we refer to as third- and fourth-order sampling respectively. regression, classification, and clustering tasks, the learned embeddings generated from our strategy are more robust than previous methods. We also introduce a position encoder that allows our model to generalize to unseen dimension values. In total, our work shows that it is possible to learn skillful representations in an extremely efficient way, which in turn reduces the climate impact of training models for tasks such as weather forecasting [ 11,12], extreme event detection [ 13], and climate model parameterization [14]. 2 Our Proposal 2.1 Higher-Order Sampling While our method generalizes to any weather or climate dataset, we will for concreteness consider ERA5 and consider a single timestamp of a given set of ERA5 parameters on a given set of verticle pressure levels as a sample Xt. The typical approach for invariance-based (contrastive) learning with a vision transformer (ViT) would be to apply some randomized transformations to Xtand then tokenize the sample by cutting it into patches along the horizontal dimensions. This means that the entire data cube is held in GPU memory. Our approach instead patches along either all three spatial dimensions or along all four dimensions. See Figure 1. Then, our only randomized transformation is to sample from this set of patches. I.e. we perform contrastive learning by sampling two sets of tokens from these patches without any further augmentation. Note that we only hold the sampled tokens in GPU memory. Decreasing the sampling ratios reduces GPU memory consumption and makes the training task more difficult. To avoid confusion, let us point out that we run inference without sampling on the complete sample Xt. 2.2 Generalizable Positional Encoder The second part of our method concerns position encodings. After tokenization, it is best practice to add an additional signal to each token that informs the model about the position of said token in relation to the complete sample. Such encodings can be hardcoded or learned. Fully observedLevel Longitude x Latitude ParameterPartially observedLevel Longitude x Latitude Parameter Figure 2: ( left) Position encodings tend to assume the same dimension values to be present during training and inference. ( right ) By using a position encoder fθ(l), we are able to train on a subset of values (blue) yet run inference on the entire set (red and blue) by capturing the non-linear dependencies on coordinates beyond interpolation. 2We note that for weather and climate data, many dimensions such as latitude, longitude, the vertical level and time are continuous. Thus, we can either define or learn a position encoding as a function f that depends on these continuous dimension values. Concretely, we will train a model with a learned position encoding f(level ). This allows us to train a model on\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nIn the form of tensors that are both higher dimensional and larger in size than the RGB imagery commonly encountered in computer vision experiments, scaling issues arise, necessitating the efficient use of available compute resources. Motivated by results on masked autoencoders, we demonstrate that sampling of subtensors can be used as the sole augmentation strategy for contrastive learning with a sampling ratio of approximately 1%. This is in contrast to typical masking ratios of 75% or 90% for image and video data, respectively. In an ablation study, we explore extreme sampling ratios and find comparable performance for ratios as low as approximately 0.0625%. To further pursue efficiency, we investigate whether it is possible to generate robust embeddings on dimension values not present at training time. We answer this question affirmatively by using learnable position encoders that have a continuous dependence on dimension values.\n\nIntroduction\n\nFollowing numerous breakthroughs in deep learning, there has been a growing trend to adapt techniques used in computer vision to other domains, including weather and climate applications. The type of data used, however, is complex due to its high-dimensional nature, especially compared to standard RGB images. For instance, the ERA5 reanalysis spans the globe with an extensive number of parameters and atmospheric levels, resulting in GB-scale tensors at each time step. This research delves into resource-efficient self-supervised representation learning using ERA5 data, employing invariance-based methods like SimCLR and MoCo. In particular, we use sampling as the sole data augmentation technique and impose resource constraints of 24-hour train time on a single A100 GPU with 40GB memory.\n\nAcross multiple downstream regression, classification, and clustering tasks, the learned embeddings generated from our strategy are more robust than previous methods. We also introduce a position encoder that allows our model to generalize to unseen dimension values. In total, our work shows that it is possible to learn skillful representations in an extremely efficient way, which in turn reduces the climate impact of training models for tasks such as weather forecasting, extreme event detection, and climate model parameterization.\n\nOur Proposal\n\n2.1 Higher-Order Sampling\n\nWhile our method generalizes to any weather or climate dataset, we will consider ERA5 and a single timestamp of a given set of ERA5 parameters on a given set of vertical pressure levels as a sample Xt. The typical approach for invariance-based (contrastive) learning with a vision transformer (ViT) would be to apply some randomized transformations to Xt and then tokenize the sample by cutting it into patches along the horizontal dimensions. This means that the entire data cube is held in GPU memory. Our approach instead patches along either all three spatial dimensions or all four dimensions. We then perform contrastive learning by sampling two sets of tokens from these patches without any further augmentation. Note that we only hold the sampled tokens in GPU memory. Decreasing the sampling ratios reduces GPU memory consumption and makes the training task more difficult.\n\n2.2 Generalizable Positional Encoder\n\nThe second part of our method concerns position encodings. After tokenization, it is best practice to add an additional signal to each token that informs the model about the position of the token in relation to the complete sample. Such encodings can be hardcoded or learned. We note that for weather and climate data, many dimensions such as latitude, longitude, the vertical level, and time are continuous. Thus, we can either define or learn a position encoding as a function f that depends on these continuous dimension values. Concretely, we will train a model with a learned position encoding f(level). This allows us to train a model on a subset of values yet run inference on the entire set by capturing the non-linear dependencies on coordinates beyond interpolation."
}