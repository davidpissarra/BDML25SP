{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" for generating satellite imagery conditional on gridded population distributions. We make additions to the existing ALAE [30] architecture, creating a spatially conditional version: SCALAE. This method allows us to explicitly disentangle population from the model’s latent space and thus input custom population forecasts into the generated imagery. We postulate that such imagery could then be directly used for land cover and land use change estimation using existing frameworks, as well as for realistic visualisation of expected local change. We evaluate the model by comparing pixel and semantic reconstructions, as well as calculate the standard FID metric. The results suggest the model captures population distributions accurately and delivers a controllable method to generate realistic satellite imagery. 1 Introduction Human beings are not actionless pawns in the face of climate change, they adapt to both direct and indirect pressures brought about by an increasingly unstable climate. People can either choose to leave problematic areas, migrating both locally and internationally to adapt to their circumstances, or they can stay and change their lifeways. In either case, and on top of expected demographic change, human adaptation to climate change thus reshufﬂes the settlement landscape [9]. As local populations ebb and ﬂow, land use and land cover change in response. Due to this high mobility of populations, particularly given recent work on climate induced migration [32], one of the major challenges for planning for climate change scenarios is thinking about where people will be, and how this will change the landscape. State-of-the-art work on gridded population forecasts shows us the value of a greater geographic resolution and border oblivious approach [32, 20]. However, such forecasts still require analytic processing to evaluate their consequences for local landscapes. In this paper, we explore the potential for generating satellite imagery conditional on population change as an in-between step for analysis, and as a means of directly visualizing forecasts in a realistic way. To do so, we employ the latest generative models from the ﬁeld of unsupervised machine learning, namely generative adversarial networks [13]. GANs are a state-of-the-art technique for high resolution image generation, as they can generate images from random noise, also called the \u0003contact author: langer.tomas@yahoo.com Tackling Climate Change with Machine Learning workshop at NeurIPS 2020.latent space. We refer the reader to a review article [39] for additional details. Generative models have been successfully applied to various high resolution datasets, mainly faces (e.g. CelebAHQ [23], FFHQ [21]), objects (e.g. Imagenet [8]), and scenes (e.g. LSUN [40], Cityscapes [7]). However, generative models in the high resolution earth observation domain are relatively under-explored [33]. On top of image generation, one might wish to edit a real image via a trained generative model. For this, a mapping from image space to latent space is required. This is a natural feature of autoencoders [25] or ﬂow based generative models [24]. However, one can learn the mapping for a trained GAN model as well, which is called projection [1, 22]. In order to facilitate fast and accurate mapping, we decided to train a hybrid model based on Adversarial Latent AutoEncoder (ALAE) [30], which combines a GAN and an AE by training a generator and an encoder end-to-end. Furthermore, to explicitly control the generated population, we add a conditional input to the generator (more information in appendix 5.1), where the input label is a pixel-level population map. Our main contributions are: \u000fAdding spatial conditioning module to the ALAE architecture \u000fTraining the model on satellite + population data \u000fVisualizing population in generated images \u000fEvaluating the generative model’s performance in terms of quality and the population change effect 2 Methods 2.1 Data Two data sets are utilized over a study site of continental Central America including Mexico. We focus on Central America here as it has been identiﬁed as a region already experiencing high levels of internal and international migration due to climate change, and has been the focus of prior modelling efforts [20]. However, this approach could of course be applied to any geographic region. Image data is derived from surface reﬂectance data from ESA’s Sentinel-2 mission [26]. Sentinel-2 is a constellation of two satellites, which collect images at a 5 day revisit. The second data set is the Global Human Settlement population data (GHS-POP) for the years 2000 and2015 from the European Commission’s Joint Research Centre [34, 11]. Both datasets are publicly available, and the details of how we sample the data are available in appendix 5.3. 2.2 Model architecture Figure 1: SCALAE model training where white circles represent original ALAE inputs, yellow circles are our additional inputs, green boxes are original ALAE modules, blue\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nFor generating satellite imagery conditional on gridded population distributions, we make additions to the existing ALAE architecture, creating a spatially conditional version: SCALAE. This method allows us to explicitly disentangle population from the model's latent space and input custom population forecasts into the generated imagery. We postulate that such imagery could be directly used for land cover and land use change estimation using existing frameworks, as well as for realistic visualization of expected local change. We evaluate the model by comparing pixel and semantic reconstructions, as well as calculating the standard FID metric. The results suggest the model captures population distributions accurately and delivers a controllable method to generate realistic satellite imagery.\n\nIntroduction\n\nHuman beings are not passive in the face of climate change; they adapt to both direct and indirect pressures brought about by an increasingly unstable climate. People can either choose to leave problematic areas, migrating both locally and internationally to adapt to their circumstances, or they can stay and change their lifeways. In either case, and on top of expected demographic change, human adaptation to climate change reshuffles the settlement landscape. As local populations ebb and flow, land use and land cover change in response. Due to this high mobility of populations, particularly given recent work on climate-induced migration, one of the major challenges for planning for climate change scenarios is thinking about where people will be, and how this will change the landscape. State-of-the-art work on gridded population forecasts shows the value of a greater geographic resolution and border-oblivious approach. However, such forecasts still require analytic processing to evaluate their consequences for local landscapes.\n\nIn this paper, we explore the potential for generating satellite imagery conditional on population change as an in-between step for analysis, and as a means of directly visualizing forecasts in a realistic way. To do so, we employ the latest generative models from the field of unsupervised machine learning, namely generative adversarial networks. GANs are a state-of-the-art technique for high-resolution image generation, as they can generate images from random noise, also called the latent space. We refer the reader to a review article for additional details. Generative models have been successfully applied to various high-resolution datasets, mainly faces, objects, and scenes. However, generative models in the high-resolution earth observation domain are relatively under-explored.\n\nOn top of image generation, one might wish to edit a real image via a trained generative model. For this, a mapping from image space to latent space is required. This is a natural feature of autoencoders or flow-based generative models. However, one can learn the mapping for a trained GAN model as well, which is called projection. In order to facilitate fast and accurate mapping, we decided to train a hybrid model based on Adversarial Latent AutoEncoder (ALAE), which combines a GAN and an AE by training a generator and an encoder end-to-end. Furthermore, to explicitly control the generated population, we add a conditional input to the generator, where the input label is a pixel-level population map.\n\nOur main contributions are:\n\n* Adding a spatial conditioning module to the ALAE architecture\n* Training the model on satellite and population data\n* Visualizing population in generated images\n* Evaluating the generative model's performance in terms of quality and the population change effect\n\nMethods\n\n2.1 Data\n\nWe utilize two datasets over a study site of continental Central America, including Mexico. We focus on Central America here as it has been identified as a region already experiencing high levels of internal and international migration due to climate change, and has been the focus of prior modeling efforts. However, this approach could be applied to any geographic region. Image data is derived from surface reflectance data from ESA's Sentinel-2 mission. Sentinel-2 is a constellation of two satellites, which collect images at a 5-day revisit. The second dataset is the Global Human Settlement population data (GHS-POP) for the years 2000 and 2015 from the European Commission's Joint Research Centre. Both datasets are publicly available, and the details of how we sample the data are available in the appendix.\n\n2.2 Model architecture\n\nFigure 1: SCALAE model training where white circles represent original ALAE inputs, yellow circles are our additional inputs, green boxes are original ALAE modules, and blue boxes are our additional modules."
}