{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\", making them useful in real-world applications. Most prior works focus on online learning requiring simulators or models of environment dynamics. However, transferring thermal simulators between environments is inefficient and challenging. We build on recent works that employ offline training on static datasets from unknown poli- cies. Pure offline RL is constrained by the replay buffer’s distribution, we propose using offline-to-online RL to enhance pre-trained offline models through online adaptation to distribution shifts. We show that direct online fine-tuning deteriorates performance on offline policies. To address this, we propose automatically tuning the actor’s regularization during training to optimize the exploration-exploitation tradeoff. Specifically, we leverage simple moving averages of mean Q-values sam- pled throughout training. Simulation experiments in building HV AC environments demonstrate our method outperforms state-of-the-art approaches under various conditions, improving performance by 32.9%and enhancing pre-trained models’ capabilities online. 1 Introduction Real-world building HV AC systems are primarily controlled via Rule-Based Control (RBC) policies that consist of a nuanced set of if-else rules crafted by domain experts. These rules do not generalize or scale to different operating environments and buildings. Control policies trained with Reinforcement Learning, on the other hand, adapt to changes in the environment. To do so, building control problems could be formulated as Markov Decision Processes (MDP) and optimized with RL methods [ 31]. The RL agent observes the states via sensors in the buildings (i.e., thermostats, hygrometers) and takes actions upon the control setpoints in a building management system via actuators. In this formulation, the objective of the agent is to increase energy efficiency while maintaining occupants’ thermal comfort [41]. ∗Work unrelated to Amazon. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.3 METHODOLOGY Much of the RL methods in prior works are trained and evaluated in the online paradigm, where an accurate simulator is required to train the RL agent. However, setting and calibrating these simulators requires expertise and is time-consuming. Also, in modern commercial buildings, the building management systems already store a significant amount of sensing and control data. These datasets capture not only the thermal dynamics of the systems but also the control policies that embody the best-known knowledge of the control defined by domain experts. Prior works have shown that offline data-driven methods can learn a more robust policy compared to the behavioral RBC policies that generated the data [ 19]. Nonetheless, without further online interactions, the improvement is limited by the diversity of the state-action distribution in the historical data. Recently, researchers have explored the capability of offline-to-online (O2O) RL [ 22,42,39,37,18], where an offline RL policy is first learned on historical data and continues to be trained with online interactions in the deployed environment. These models are designed to adapt to the distribution drift between the static historical data and the deployment environments, continuing to improve in performance with judicious exploration. To our knowledge, O2O algorithms have not been studied in the building control domain. We show that off-the-shelf algorithms designed for other domains such as robotics degrade in performance compared to purely offline RL policies. We present a novel algorithm that overcomes these shortcomings and continues improve the deployed RL policy after deployment. We exploit the insight that building control systems do not experience rapid change in dynamics, and introduce a moving average term in the RL policy loss function to stabilize online updates. Simulation experiments with the well-established Sinergym environment show that our algorithms achieve state-of-the-art performance. Real environment Pre-train offline model Agent Dataset EnvironmentsInteractSave new transitionsFine-tune𝑠,𝑎,𝑠′,𝑟\t~𝒟𝜋!(𝑠\"##)𝑄!(𝑠\"##,𝑎\"##)(𝑠\"$,𝜋!𝑠\"$)(𝑠\"$,𝜋!𝑠\"$,𝑠\"$%,𝑟\"$)𝜋!→𝜋#&𝑄!→𝑄#& Figure 1: Flow chart of offline-to-online RL: (1.) The offline model learns from the existing dataset. (2.) After pre-training, the agent interacts with the environment online. (3.) The generated transitions are saved in the replay buffer(s) for further learning. (4.) The offline-to-online fine-tuning improves the agent’s performance continuously. We will open-source our code with a permissive license. 2 Problem Statement The objective of the agent is to maintain a comfortable thermal environment with minimal energy use. The state consists of indoor/outdoor temperatures, time/day, occupant count, thermal comfort, and related sensor data. The action is to adjust the temperature setpoint of the thermostat. The reward is a linear combination of occupants’ thermal comfort and energy consumption. The environment is a single-floor building divided into 5 thermal\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nMost prior works on reinforcement learning (RL) focus on online learning that requires simulators or models of environment dynamics. However, transferring thermal simulators between environments is inefficient and challenging. We build on recent works that employ offline training on static datasets from unknown policies. Pure offline RL is constrained by the replay buffer's distribution, so we propose using offline-to-online RL to enhance pre-trained offline models through online adaptation to distribution shifts. We show that direct online fine-tuning deteriorates performance on offline policies. To address this, we propose automatically tuning the actor's regularization during training to optimize the exploration-exploitation tradeoff. Specifically, we leverage simple moving averages of mean Q-values sampled throughout training.\n\nSimulation experiments in building HVAC environments demonstrate our method outperforms state-of-the-art approaches under various conditions, improving performance by 32.9% and enhancing pre-trained models' capabilities online.\n\nIntroduction:\n\nReal-world building HVAC systems are primarily controlled via rule-based control (RBC) policies crafted by domain experts. These rules do not generalize or scale to different operating environments and buildings. Control policies trained with reinforcement learning adapt to changes in the environment. To do so, building control problems can be formulated as Markov decision processes (MDPs) and optimized with RL methods.\n\nThe RL agent observes the states via sensors in the buildings (e.g., thermostats, hygrometers) and takes actions upon the control setpoints in a building management system via actuators. The objective of the agent is to increase energy efficiency while maintaining occupants' thermal comfort.\n\nMethodology:\n\nMuch of the RL methods in prior works are trained and evaluated in the online paradigm, where an accurate simulator is required to train the RL agent. However, setting and calibrating these simulators requires expertise and is time-consuming. In modern commercial buildings, the building management systems already store a significant amount of sensing and control data. These datasets capture not only the thermal dynamics of the systems but also the control policies that embody the best-known knowledge of the control defined by domain experts.\n\nPrior works have shown that offline data-driven methods can learn a more robust policy compared to the behavioral RBC policies that generated the data. Nonetheless, without further online interactions, the improvement is limited by the diversity of the state-action distribution in the historical data. Recently, researchers have explored the capability of offline-to-online (O2O) RL, where an offline RL policy is first learned on historical data and continues to be trained with online interactions in the deployed environment.\n\nThese models are designed to adapt to the distribution drift between the static historical data and the deployment environments, continuing to improve in performance with judicious exploration. To our knowledge, O2O algorithms have not been studied in the building control domain. We show that off-the-shelf algorithms designed for other domains (e.g., robotics) degrade in performance compared to purely offline RL policies.\n\nWe present a novel algorithm that overcomes these shortcomings and continues to improve the deployed RL policy after deployment. We exploit the insight that building control systems do not experience rapid change in dynamics, and introduce a moving average term in the RL policy loss function to stabilize online updates. Simulation experiments with the well-established Sinergym environment show that our algorithm achieves state-of-the-art performance.\n\nProblem Statement:\n\nThe objective of the agent is to maintain a comfortable thermal environment with minimal energy use. The state consists of indoor/outdoor temperatures, time/day, occupant count, thermal comfort, and related sensor data. The action is to adjust the temperature setpoint of the thermostat. The reward is a linear combination of occupants' thermal comfort and energy consumption. The environment is a single-floor building divided into 5 thermal zones."
}