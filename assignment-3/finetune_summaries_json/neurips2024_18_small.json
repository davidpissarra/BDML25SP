{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"-calibrated uncertainty estimates can be difficult to obtain in high-capacity prediction models such as deep neural networks. Moreover, in high-dimensional settings, there may be many valid uncertainty estimates with varied performance profiles— i.e., not all uncertainty is equally valuable for downstream decision-making. To address this challenge, this paper develops an end-to-end framework for conditional robust optimization, with robustness and calibration guarantees provided by conformal prediction. We repre- sent arbitrary convex uncertainty sets with sublevel sets of partially input-convex neural networks, which are learned as part of our framework. We demonstrate the value of our approach for robust decision-making on a battery storage arbitrage application. 1 Introduction Renewable wind and solar electricity generation have seen tremendous growth in the past decade [5], but due to their intermittency, a corresponding growth in grid-scale energy storage is needed to truly achieve grid decarbonization. To achieve profitability and encourage investment in energy storage infrastructure, storage operators use forecasts of electricity prices to schedule battery charg- ing/discharging to maximize profit, and they rely on uncertainty estimates to minimize financial and operational risk. A common method used by battery storage operators to make decisions under uncertainty is robust optimization [ 21,14]. The usual approach known as “estimate then optimize”, separates the decision-making problem into two distinct stages. In the “estimate” stage, a predictive model is trained to forecast electricity prices, yielding an uncertainty set around future prices. Then, in the “optimize” stage, the forecast uncertainty set is used as a parameter in a robust optimization problem to output the battery charge/discharge schedule. Notably, any cost or loss associated with this downstream decision is usually not provided as feedback to the predictive model. While a recent line of work [ 7,16,20,8] has made steps toward bridging the gap between uncertainty quantification and robust optimization-driven decision-making, existing approaches are suboptimal for several reasons: (1) The predictive model is not trained with feedback from the downstream objective, which limits the model’s performance on the decision-making task loss; (2) For the robust optimization to be tractable, the forecast uncertainty sets have restricted parametric forms. Common parametric forms include box and ellipsoidal uncertainty sets, limiting the expressivity of ∗denotes equal contribution Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.ML model nonconformity score functioncalibrated (1-)-confidence setconformal calibration steprobust decisionrobust optimization Input gradient  task lossFigure 1: Our proposed framework for end-to-end conformal calibration for optimization under uncertainty updates the machine learning model using gradients from the task loss. uncertainty estimates; and (3) Because neural network models are often poor at estimating their own uncertainty, the forecasts may not be well-calibrated. Recent approaches such as isotonic regression [ 13] and conformal prediction [ 17] have made progress in providing calibrated uncertainty estimates from deep learning models, but such methods are typically applied post-hoc to pre-trained models and are therefore difficult to incorporate into an end-to-end training procedure. In this work, we make three specific contributions addressing each issue identified above. First, we develop a methodology for training prediction models end-to-end with downstream decision- making objectives and conformally calibrated uncertainty estimates for the conditional robust optimization problem. Following our initial method proposal [ 22], we include differentiable con- formal calibration layers in our model during training. This closes the feedback loop, ensuring that the uncertainty’s impact on the downstream objective is accounted for during training, since not all model errors nor uncertainty estimates will result in the same downstream cost. Second, we use partially input-convex neural networks (PICNNs) [ 3] to tractably approximate arbitrary convex uncertainty sets for the conditional robust optimization problem. Due to the universal convex function approximation property of ICNNs [ 6], this approach enables training far more general representations of uncertainty than prior works have considered, which in turn yields improvements on downstream decision-making performance. Finally, we propose an exact and computationally efficient method for differentiating through the conformal prediction procedure during training. In contrast to prior work [ 18], our method gives exact gradients, without relying on approximate ranking and sorting methods. We empirically evaluate our approach on an energy storage arbitrage task and demonstrate conclu- sively that the combination of end-to-end training with the flexibility of the PICNN-based uncertainty sets achieves substantial improvement over baseline methods. Our code is available on GitHub.2 2 Problem Statement and Background Our problem setting is defined formally as follows: suppose that data (x, y)∈Rm×Rnis sampled i.i.d. from an unknown joint distribution P. Upon observing the input x(but not the label y), an agent makes a decision z∈Rp. After the decision is made, the true label yis revealed, and the agent incurs a task loss f(x, y, z ), for some known task loss function f:Rm×R\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nCalibrated uncertainty estimates can be challenging to obtain in high-capacity prediction models, such as deep neural networks. In high-dimensional settings, there may be many valid uncertainty estimates with varied performance profiles, not all of which are equally valuable for downstream decision-making. To address this challenge, this paper develops an end-to-end framework for conditional robust optimization, providing robustness and calibration guarantees through conformal prediction. We represent arbitrary convex uncertainty sets as sublevel sets of partially input-convex neural networks, which are learned as part of our framework. We demonstrate the value of our approach for robust decision-making in a battery storage arbitrage application.\n\nRenewable wind and solar electricity generation have experienced significant growth in the past decade, but their intermittency necessitates a corresponding growth in grid-scale energy storage to achieve grid decarbonization. To achieve profitability and encourage investment in energy storage infrastructure, storage operators use forecasts of electricity prices to schedule battery charging/discharging and maximize profit, relying on uncertainty estimates to minimize financial and operational risk. A common method used by battery storage operators is robust optimization, which separates the decision-making problem into two stages: estimation and optimization. In the estimation stage, a predictive model is trained to forecast electricity prices, yielding an uncertainty set around future prices. Then, in the optimization stage, the forecast uncertainty set is used as a parameter in a robust optimization problem to output the battery charge/discharge schedule. However, any cost or loss associated with this downstream decision is not typically provided as feedback to the predictive model.\n\nRecent work has bridged the gap between uncertainty quantification and robust optimization-driven decision-making, but existing approaches are suboptimal for several reasons: (1) The predictive model is not trained with feedback from the downstream objective, limiting its performance on the decision-making task loss; (2) For the robust optimization to be tractable, the forecast uncertainty sets have restricted parametric forms, such as box and ellipsoidal uncertainty sets, which limit the expressivity of the model; and (3) Neural network models are often poor at estimating their own uncertainty, resulting in poorly calibrated forecasts.\n\nTo address these issues, we make three specific contributions. First, we develop a methodology for training prediction models end-to-end with downstream decision-making objectives and conformally calibrated uncertainty estimates for the conditional robust optimization problem. We include differentiable conformal calibration layers in our model during training, closing the feedback loop and ensuring that the uncertainty's impact on the downstream objective is accounted for during training. Second, we use partially input-convex neural networks (PICNNs) to tractably approximate arbitrary convex uncertainty sets for the conditional robust optimization problem. This approach enables training far more general representations of uncertainty than prior works have considered, yielding improvements on downstream decision-making performance. Finally, we propose an exact and computationally efficient method for differentiating through the conformal prediction procedure during training, providing exact gradients without relying on approximate ranking and sorting methods.\n\nWe empirically evaluate our approach on an energy storage arbitrage task and demonstrate that the combination of end-to-end training with the flexibility of the PICNN-based uncertainty sets achieves substantial improvement over baseline methods. Our code is available on GitHub.\n\nOur problem setting is defined formally as follows: suppose that data (x, y) is sampled i.i.d. from an unknown joint distribution P. Upon observing the input x (but not the label y), an agent makes a decision z. After the decision is made, the true label y is revealed, and the agent incurs a task loss f(x, y, z) for some known task loss function f."
}