{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" delve into Machine Learning approaches to predict plasma state outcomes. Our contributions are twofold: (1) We present a novel application of Continuous Convolutional Neural Networks for disruption prediction and (2) We examine the advantages and disadvantages of continuous models over discrete models for disruption prediction by comparing our model with the previous, discrete state of the art, and show that continuous models offer significantly better performance (Area Under the Receiver Operating Characteristic Curve = 0.974 v.s. 0.799) with fewer parameters. 1 Introduction Effectively combating climate change requires significant decarbonization of the grid. Nuclear fusion has long been considered a “holy grail” for producing on-demand energy without significant land demands or waste, and many have quantified beneficial carbon impacts [3, 12]. Fusion was considered far away until recently, when renewed public-private enterprise in fusion has ushered in a wave of investment, interest, and supporting industries. One major approach to generating fusion in laboratory plasmas is magnetic confinement, which leverages the tendency of plasmas to remain confined perpendicular to magnetic fields. Currently, the highest performing magnetic fusion concept is the tokamak, a toroidal vacuum chamber with external magnetic coils that help generate a large plasma current. One of the largest challenges for conducting tokamak research and future commercialization are plasma disruptions, or the loss of plasma stability that may deposit large amounts of temperature and current on the tokamak’s walls [8]. Quenches can inflict severe damage to the tokamak, requiring costly repairs and delaying future experiments. For more background, please see the Appendix. Disruptions are difficult to model using physical or “first-principles” simulations due to the numerous causes of instability and unobserved factors. In response, several Machine Learning strategies have been previously suggested to estimate disruptivity, i.e. the probability of a disruption outcome, allowing timely activation of mitigation systems [9]. For instance, DIII-D, the U.S.’s largest tokamak, employs a random forest (RF) for plasma state monitoring [10]. Current state of the art models employ classical neural techniques, such as “Hybrid Deep Learner” (HDL), a convolutional Long-Short Term Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.Figure 1: Disruption prediction framework. Tokamak picture taken from [2]. Memory network [14]. These models are still behind what is necessary for commercially viable fusion, where recall of at least 95% is required [8]. Moreover, they might not be tailored to a plasma dataset’s unique structure. The RF and HDL treat the problem as a discrete time series, where the observation frequency is fundamental to the underlying system (i.e. like characters or words, which are fundamental units of measure in a sentence.) A tokamak’s features’ sampling rates are by no means fundamental, and vary depending on diagnostic instrument. The model that we present here, the Continuous CNN [6], addresses these shortcomings. Instead of learning discrete filters directly, this model learns real, continuous functions that are that sampled. In our case, this is a mapping φ:R→Rthat maps time to filter weight. Here we use the Multiplicative Anisotropic Gabor Network (MAGNet) [11], that uses a combination of 1D convolutions and Gabor functions, (a product of a normal PDF and sin). Learning a MAGNet instead of a discrete filter constrains the kernel to learn smooth features, even though it is later reduced to a discrete series of weights when passing through the sample’s resolution. We endeavor to create a model that surpasses the HDL in the Area Under the Receiver Operating Characteristic Curve (AUC) metric, and also investigate whether a continuous approach is superior. 2 Model and Data 2.1 Dataset composition Our dataset is composed of 4418 plasma shots from the Alcator C-Mod tokamak from MIT (C-Mod) [5]. Of these, 20% culminate in a disruption. The data is normalized to a sample rate of 200Hz (every 5ms). Shots with duration less than 125ms are excluded. Each shot is truncated 40ms prior to its end, as this is the minimum time needed for the activation of disruption mitigation systems. We did not train the model across multiple reactors, as detailed in [14], due to irregularities noticed in the data other than C-Mod, and because C-Mod is most similar to a new machines of interest [1]. Our main objective is to understand how this model performs on a single tokamak, and determine how predictable disruptions actually are when using continuous filters. We set up the training and test task as a sequence to label prediction. Each shot is a training example with a binary disruption label. Non-disruptive shots are augmented by randomly clipping them at shorter lengths. For an explanation of the features we use, please see table 2 in the Appendix. 2.2 CCNN Architecture We instantiated the\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nTo predict plasma state outcomes using Machine Learning approaches, we make two key contributions. First, we apply a novel Continuous Convolutional Neural Network (CNN) for disruption prediction. Second, we compare the advantages and disadvantages of continuous models over discrete models for disruption prediction by comparing our model with the previous state of the art. Our results show that continuous models offer significantly better performance (Area Under the Receiver Operating Characteristic Curve = 0.974 vs. 0.799) with fewer parameters.\n\nTo combat climate change, significant decarbonization of the grid is required. Nuclear fusion has long been considered a promising solution for producing on-demand energy without significant land demands or waste. Recently, renewed public-private investment in fusion has led to a surge in interest and supporting industries. One approach to generating fusion in laboratory plasmas is magnetic confinement, which leverages the tendency of plasmas to remain confined perpendicular to magnetic fields. The tokamak, a toroidal vacuum chamber with external magnetic coils, is the highest-performing magnetic fusion concept. However, plasma disruptions, or the loss of plasma stability, pose a significant challenge for tokamak research and commercialization. Disruptions can inflict severe damage to the tokamak, requiring costly repairs and delaying future experiments.\n\nDisruptions are difficult to model using physical or \"first-principles\" simulations due to the numerous causes of instability and unobserved factors. In response, several Machine Learning strategies have been suggested to estimate disruptivity, allowing timely activation of mitigation systems. For instance, the DIII-D tokamak employs a random forest for plasma state monitoring. Current state-of-the-art models employ classical neural techniques, such as the Hybrid Deep Learner (HDL), a convolutional Long-Short Term Memory network. These models are still behind what is necessary for commercially viable fusion, where recall of at least 95% is required. Moreover, they may not be tailored to a plasma dataset's unique structure.\n\nThe model we present here, the Continuous CNN, addresses these shortcomings. Instead of learning discrete filters directly, this model learns real, continuous functions that are sampled. We use the Multiplicative Anisotropic Gabor Network (MAGNet), which combines 1D convolutions and Gabor functions. Learning a MAGNet constrains the kernel to learn smooth features, even though it is later reduced to a discrete series of weights when passing through the sample's resolution. We aim to create a model that surpasses the HDL in the Area Under the Receiver Operating Characteristic Curve (AUC) metric and investigate whether a continuous approach is superior.\n\nOur dataset consists of 4418 plasma shots from the Alcator C-Mod tokamak from MIT. Of these, 20% culminate in a disruption. The data is normalized to a sample rate of 200Hz (every 5ms). Shots with duration less than 125ms are excluded. Each shot is truncated 40ms prior to its end, as this is the minimum time needed for the activation of disruption mitigation systems. We did not train the model across multiple reactors due to irregularities in the data and because C-Mod is most similar to new machines of interest. Our main objective is to understand how this model performs on a single tokamak and determine how predictable disruptions actually are when using continuous filters.\n\nWe set up the training and test task as a sequence-to-label prediction. Each shot is a training example with a binary disruption label. Non-disruptive shots are augmented by randomly clipping them at shorter lengths. For an explanation of the features we use, please see Table 2 in the Appendix.\n\nOur CCNN architecture is instantiated with a specific configuration, which we will describe in the following sections."
}