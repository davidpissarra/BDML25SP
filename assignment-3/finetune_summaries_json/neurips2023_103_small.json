{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" primarily implemented in Fortran, a language that poses a high barrier of entry for early career scientists and lacks a GPU runtime, which has become essential for contin- ued advancement as GPU power increases and CPU scaling slows. Fortran also lacks differentiability — the capacity to differentiate through numerical code — which enables hybrid models that integrate machine learning methods. Convert- ing an ESM from Fortran to Python/JAX could resolve these issues. This work presents a semi-automated method for translating individual model components from Fortran to Python/JAX using a large language model (GPT-4). By translating the photosynthesis model from the Community Earth System Model (CESM), we demonstrate that the Python/JAX version results in up to 100x faster runtimes using GPU parallelization, and enables parameter estimation via automatic differentiation. The Python code is also easy to read and run and could be used by instructors in the classroom. This work illustrates a path towards the ultimate goal of making climate models fast, inclusive, and differentiable. 1 Introduction Over the last decades, climate models have evolved from very coarse ocean-atmosphere models to complex Earth system models including all Earth components — the ocean, atmosphere, cryosphere and land and detailed biogeochemical cycle — [ 12] and are now being used to address a vast array of questions from projecting natural sinks [ 14] to the impact of land use change on climate [ 13]. Over decades of model development, Earth system models have become huge computing programs written in Fortran or C, for legacy and compatibility reasons. Yet most early career scientists do not know those languages, which hinders model development. Further, because of their complexity, running these models is resource-intensive [ 35;31]. Most (perhaps all) models rely on parallelized CPU hardware, even though highly-parallelized code can now be run more efficiently on Graphics Processing Units (GPUs) [ 1] or Tensor Processing Units Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.(TPUs) [ 9]. Earth system modeling would dramatically benefit from hardware acceleration, so that models could be run at higher resolutions and resolve important fine-scale processes like ocean eddies or deep convection. Using high-level languages also makes model code robust to future hardware evolution; this type of (r)evolution is already starting in other fields of physics [7; 3]. Finally, Earth System Models are starting to use machine learning (ML) to represent subgrid processes [32;15;40;4]. Because of their native Python infrastructure, those machine learning components are not straightforward to implement in Fortran- or C/C++-based Earth System Models and require a bridge (e.g., Fortran to Python), without the flexibility to do online learning (updating between ML and the physical code while the physical model runs) [ 30] [8]. The missing piece is automatic differentiation, which allows us to take the partial derivatives to any parameters in the code. Python has many libraries supporting differentiation; one of these is JAX, which enables automatic differentiation and hardware acceleration of Python code while maintaining a near-Python visual look [6]. In this work, we intend to break this Earth System Model language and hardware deadlock by modernizing Earth System Model codes to run on modern hardware (e.g., GPUs) and enabling automatic differentiation [ 25] using Python and JAX to enhance both efficiency and precision [ 6], while also making model development more inclusive. To realize this goal, our approach leverages recent developments in large-language models (LLMs) to rapidly translate code from one language to another [ 19] [38]. Specifically, we split the model into units using static analysis, develop a topological sort order based on dependencies, and translate each unit using OpenAI’s GPT-4 [34]. Our contribution is twofold: we demonstrate the Python/JAX version’s utility through an example model (leaf-level photosynthesis in the Community Land Model (CLM) of CESM), and we offer open source tools to address translation challenges. As an added benefit, we also demonstrate that the Python translation can be computationally more efficient than the original model by leveraging GPUs with the JAX library. 2 Translation Workflow Large language models alone cannot translate a whole codebase for two reasons. First, there are more tokens (i.e., words) in a typical codebase than a model like GPT-4 can accept in its token limit. Second, the model alone often writes incorrect code. To resolve these issues, we’ve implemented a divide-and-conquer approach that splits a Fortran codebase into ordered units using static analysis, generates Python/JAX translations using the GPT-4 API, and uses unit test outputs to iterate on the generated code. Figure 1 shows the architecture of our translation process (see details in appendix). Isolate Fortran source code and dependencies Generate For- tran unit testsGenerate Python unit testsUse unit test output to update unit tests and code Generate Python code\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nClimate models have evolved significantly over the last few decades, transitioning from simple ocean-atmosphere models to complex Earth system models that incorporate all Earth components, including the ocean, atmosphere, cryosphere, and land, as well as detailed biogeochemical cycles. These models are now used to address a wide range of questions, from projecting natural sinks to the impact of land use change on climate.\n\nDespite their complexity, Earth system models are often written in legacy languages like Fortran or C, which poses a significant barrier to entry for early career scientists. Moreover, running these models is resource-intensive and relies heavily on parallelized CPU hardware. However, highly-parallelized code can now be run more efficiently on Graphics Processing Units (GPUs) or Tensor Processing Units (TPUs).\n\nEarth system modeling would greatly benefit from hardware acceleration, enabling models to be run at higher resolutions and resolve important fine-scale processes like ocean eddies or deep convection. Using high-level languages also makes model code more robust to future hardware evolution, as seen in other fields of physics.\n\nEarth System Models are starting to incorporate machine learning (ML) to represent subgrid processes, but this requires a bridge between the physical code and ML components. The missing piece is automatic differentiation, which allows us to take the partial derivatives of any parameters in the code. Python has many libraries supporting differentiation, including JAX, which enables automatic differentiation and hardware acceleration of Python code while maintaining a near-Python visual look.\n\nTo address this issue, we propose a semi-automated method for translating individual model components from Fortran to Python/JAX using a large language model (GPT-4). Our approach involves splitting the model into units using static analysis, developing a topological sort order based on dependencies, and translating each unit using OpenAI's GPT-4.\n\nWe demonstrate the utility of this approach by translating the photosynthesis model from the Community Earth System Model (CESM) and show that the Python/JAX version results in up to 100x faster runtimes using GPU parallelization, and enables parameter estimation via automatic differentiation. The Python code is also easy to read and run, making it suitable for use in the classroom.\n\nOur contribution is twofold: we demonstrate the Python/JAX version's utility through an example model, and we offer open-source tools to address translation challenges. As an added benefit, we also demonstrate that the Python translation can be computationally more efficient than the original model by leveraging GPUs with the JAX library.\n\nTo overcome the limitations of large language models, we implemented a divide-and-conquer approach that splits a Fortran codebase into ordered units using static analysis, generates Python/JAX translations using the GPT-4 API, and uses unit test outputs to iterate on the generated code. This approach allows us to translate a whole codebase while addressing the issues of token limits and incorrect code generation."
}