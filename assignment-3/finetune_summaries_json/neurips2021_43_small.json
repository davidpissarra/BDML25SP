{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" sub-optimal policies may lead to trafﬁc jams and as a result increased levels of air pollution and wasted time. Many adaptive trafﬁc signal controllers have been proposed in the literature, but research on their relative performance differences is limited. On the other hand, to the best of our knowledge there has been no work that directly targets CO2 emission reduction, even though pollution is currently a critical issue. In this paper, we propose a reward shaping scheme for various RL algorithms that not only produces lowers CO2 emissions, but also produces respectable outcomes in terms of other metrics such as travel time. We compare multiple RL algorithms — sarsa, and A2C — as well as diverse scenarios with a mix of different road users emitting varied amounts of pollution. 1 Introduction People spend a considerable and unnecessary amount of time and money on roadways, sometimes due to trafﬁc lights not being responsive to the trafﬁc. According to Forbes [ 1], trafﬁc congestion costs US $124 billion per year. Likewise, [ 2] states that trafﬁc congestion costs up to 1% of the European Union’s GDP. Air pollution is responsible for about three million deaths worldwide each year, according to [ 3]. One-third of all pollution-based mortalities in North America are attributed to land trafﬁc emissions [ 4]. In 2017, residents of Los Angeles, New York, and San Francisco spent an average of three to four days per year stuck in trafﬁc, wasting ten billion dollars in fuel and individual time waste, according to [ 5]. Needless to say, optimizing trafﬁc ﬂow is a critical issue, and an important subproblem is trafﬁc light optimization at intersections. A number of approaches have been proposed for constructing trafﬁc light control policies. For example, a ﬁxed-time, cycle-based trafﬁc signal controller that chooses the next phase by displaying the phases in an ordered sequence known as a cycle, with each phase having a ﬁxed, potentially unique duration. Since trafﬁc needs to follow predictable patterns over long periods of time (i.e., times of the day, days of the week), simple ﬁxed-time approaches are common in transportation networks because they are predictable, stable, and effective. Researchers have long attempted to build Tackling Climate Change with Machine Learning: workshop at NeurIPS 2021.Environment:intersection Interpreter Agent:A2C, Sarsa Action:Green phase Reward:Weighted scheme Objective State(a) Structure: Ecolight components (b) Environment: two way single intersection Figure 1: Intersection has through, left, and right option in each lane with different road user: yellow for car, blue for truck, green for bus, red for light truck. Interpreter render the computation. new trafﬁc signal controllers that can adjust to changing trafﬁc conditions, despite the ﬁxed-time controller’s widespread use. Actuated trafﬁc signal controllers create dynamic phase durations using sensors and Boolean logic [ 6]. To react to changing intersections, adaptive trafﬁc signal controllers can use acyclic phase sequences and dynamic phase durations. They strive for improved performance at the cost of complexity, high expenses, and dependability and have been proposed using a variety of methodologies, including analytic mathematical solutions, heuristics, and machine learning [ 7,8,9]. There are numerous learning based approaches in the literature such as tabular Q learning [ 10], DQN [ 11], DDPG [ 12]. Traditional reinforcement learning is difﬁcult to implement due to two major issues: (1) how to deﬁne the state space to adequately describe the environment, (2) how to deﬁne the action space to capture decisions for changing the trafﬁc lights, (3) how to choose a reward function to effectively targets the cost functions [13]. In much of past work, reward has been deﬁned as an ad-hoc weighted linear combination of numerous trafﬁc measures. However, there is no certainty that the reward will reduce the journey duration. Furthermore, in order to take into account more aspects of the trafﬁc condition, recent RL techniques incorporate more sophisticated states (e.g., image). However, none of the prior research has analyzed whether such a complex state representation is required. This added complexity may result in a less efﬁcient learning process without a considerable improvement in performance. To the best of our knowledge, this paper is the ﬁrst attempt of trafﬁc control optimization that directly targets CO2 reduction in a complex intersection including different types of vehicles. To this end, we propose a reward shaping scheme that weighs different classes of road users such as cars, trucks and buses differently. This additional hyper-parameter allows us to adjust to different scenarios and real world objectives. 2 Method This section will describe the design process and reward shaping scheme we propose for complex intersections\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nSuboptimal traffic policies may lead to traffic jams and increased levels of air pollution and wasted time. Many adaptive traffic signal controllers have been proposed in the literature, but research on their relative performance differences is limited. To the best of our knowledge, there has been no work that directly targets CO2 emission reduction, even though pollution is a critical issue. In this paper, we propose a reward shaping scheme for various reinforcement learning (RL) algorithms that not only produces lower CO2 emissions but also produces respectable outcomes in terms of other metrics such as travel time. We compare multiple RL algorithms, including Sarsa and A2C, as well as diverse scenarios with a mix of different road users emitting varied amounts of pollution.\n\nIntroduction\n\nPeople spend a considerable and unnecessary amount of time and money on roadways, sometimes due to traffic lights not being responsive to traffic. According to Forbes, traffic congestion costs the US $124 billion per year. Likewise, traffic congestion costs up to 1% of the European Union's GDP. Air pollution is responsible for about three million deaths worldwide each year. One-third of all pollution-based mortalities in North America are attributed to land traffic emissions. In 2017, residents of Los Angeles, New York, and San Francisco spent an average of three to four days per year stuck in traffic, wasting ten billion dollars in fuel and individual time waste. Needless to say, optimizing traffic flow is a critical issue, and an important subproblem is traffic light optimization at intersections. A number of approaches have been proposed for constructing traffic light control policies. For example, a fixed-time, cycle-based traffic signal controller that chooses the next phase by displaying the phases in an ordered sequence known as a cycle, with each phase having a fixed, potentially unique duration. Since traffic needs to follow predictable patterns over long periods of time (i.e., times of the day, days of the week), simple fixed-time approaches are common in transportation networks because they are predictable, stable, and effective.\n\nResearchers have long attempted to build new traffic signal controllers that can adjust to changing traffic conditions, despite the fixed-time controller's widespread use. Actuated traffic signal controllers create dynamic phase durations using sensors and Boolean logic. To react to changing intersections, adaptive traffic signal controllers can use acyclic phase sequences and dynamic phase durations. They strive for improved performance at the cost of complexity, high expenses, and dependability and have been proposed using a variety of methodologies, including analytic mathematical solutions, heuristics, and machine learning.\n\nThere are numerous learning-based approaches in the literature, such as tabular Q-learning, DQN, and DDPG. Traditional reinforcement learning is difficult to implement due to two major issues: (1) how to define the state space to adequately describe the environment, (2) how to define the action space to capture decisions for changing the traffic lights, (3) how to choose a reward function to effectively target the cost functions. In much of past work, reward has been defined as an ad-hoc weighted linear combination of numerous traffic measures. However, there is no certainty that the reward will reduce the journey duration. Furthermore, to take into account more aspects of the traffic condition, recent RL techniques incorporate more sophisticated states (e.g., images). However, none of the prior research has analyzed whether such a complex state representation is required. This added complexity may result in a less efficient learning process without a considerable improvement in performance.\n\nTo the best of our knowledge, this paper is the first attempt at traffic control optimization that directly targets CO2 reduction in a complex intersection including different types of vehicles. To this end, we propose a reward shaping scheme that weighs different classes of road users, such as cars, trucks, and buses, differently. This additional hyper-parameter allows us to adjust to different scenarios and real-world objectives.\n\nMethod\n\nThis section will describe the design process and reward shaping scheme we propose for complex intersections."
}