{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" temperature change globally and for central England from an ensemble of climate models, in a scalable manner and with robust uncertainty propagation. 1 Introduction Climate models simulate the evolving state of the atmosphere, ocean, and land, given certain inputs and according to fundamental physics and chemistry. Individual climate projections, called realisations, from multiple models are routinely combined to estimate the most likely climate projection for given pathways of future emissions. It is a widely used principle that the average of a set of models (climate or statistical) typically produces a better estimate than a single model [19]. It is also possible to approximate the uncertainty of the combined projection from the spread of model realisations. However, which method one should use to average these models to extract the latent climate change signal common between all models, is an open and important question. The most widely used approach is to take a multi-model mean, deriving an estimate of uncertainty from the standard deviation across the models [e.g., 7]. However, this approach does not robustly propagate uncertainties from models and simulations to the ﬁnal estimate. In order to make important policy relevant decisions we require well characterised uncertainty estimates. Consequently, we develop a Bayesian framework based on a hierarchy of Gaussian processes (GPs). Gaussian processes A GP is a distribution over functions, deﬁned by a mean function \u00162and kernelk[18]. Considering functions f:Rd!R, we can write p(f(\u0001)) =GP(0;k(\u0001;\u0001)): (1) For dataD=fX;yg=fxn;yngN n=1comprised of inputs xn2Rdand corresponding outputs yn2R, we condition on the function evaluations f= [f(xn)]N n=1through a factorising likelihood function that we will assume to be Gaussian throughout this work p(yjf) =NY i=nN(ynjfn;\u001b2 n); (2) \u0003The ﬁrst two authors contributed equally to this work. 2We follow convention and assume this to be zero in this work. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.where\u001b2 n2R>0is an observational noise parameter. For conjugate models of this form, evaluating the predictive posterior distribution at a set of test points X?has the analytical form p(f(X?)jf;y) =GP(K?f(K\u000b+\u001b2 nIN)\u00001y;K??\u0000K?f(K\u000b+\u001b2 nIN)\u00001Kf?); (3) where K\u000bis the matrix formed by evaluating kpairwise on X, and K??pairwise on X?. GPs are widely used within climate modelling [13] as they are capable of producing well characterised uncertainty estimates and, through the choice of kernel function, offer a principled way for experts to posit their prior beliefs into the model. Constructing GP models for climate data with non-trivial supports is an active area of research with recent works considering data deﬁned on a sphere [6], the vertices of a graph [17], and vector ﬁelds [10]. However, modelling a set of vectors, such as time-series, in a scalable manner is still an open problem. In this work we aim to answer the question “ how can we probabilistically infer the latent function from a set of noisy vector-valued realisations ”? To answer this, we compose a set of GPs into a two-layered hierarchical model. In the model’s upper layer we learn a GP posterior distribution that represents the latent function. This distribution is then propagated down into the lower layer where we learn a single GP posterior distribution for each observed realisation. To ensure scalable computation, we construct a variational approximation to each GP in the hierarchy by building upon the sparse GP literature. Finally, we use our sparse hierarchical Gaussian process (SHGP) model to infer latent surface temperature changes from an ensemble of climate models. 2 Methodology: From realisations to distributions We want to model a set of realisations Y=fyrgR r=1where yris a vector in RN. Allyrshare a latent function ^yand are deﬁned across the same index set X=fxngN n=1. In our examples, these dataD=fX;yrgR r=1are a collection of surface temperature projections across a common temporal range, simulated by multiple climate models, where we wish to extract the latent climate signal. To model these data we use a Bayesian hierarchical model of GPs g(X)\u0018GP (0;kg(X;X0)); (4) f(r)(X)\u0018GP (\u0016gjy;k(r)(X;X0)); (5) whereg(X)is the group-level prior distribution that underpins all yr.f(r)(X)is the prior dis- tribution for an individual vector and has a mean given by the posterior mean of gthat we de- note\u0016gjy=Ep(gj\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nTemperature change globally and for central England from an ensemble of climate models, in a scalable manner and with robust uncertainty propagation.\n\n1. Introduction\n\nClimate models simulate the evolving state of the atmosphere, ocean, and land, given certain inputs and according to fundamental physics and chemistry. Individual climate projections, called realizations, from multiple models are routinely combined to estimate the most likely climate projection for given pathways of future emissions. It is a widely used principle that the average of a set of models (climate or statistical) typically produces a better estimate than a single model. However, which method one should use to average these models to extract the latent climate change signal common between all models is an open and important question.\n\nThe most widely used approach is to take a multi-model mean, deriving an estimate of uncertainty from the standard deviation across the models. However, this approach does not robustly propagate uncertainties from models and simulations to the final estimate. To make important policy-relevant decisions, we require well-characterized uncertainty estimates. Consequently, we develop a Bayesian framework based on a hierarchy of Gaussian processes (GPs).\n\nGaussian Processes\n\nA GP is a distribution over functions, defined by a mean function and a kernel. Considering functions f: Rd → R, we can write p(f(⋅)) = GP(0, k(⋅, ⋅)). For data D = {x_n, y_n}^N n=1, comprised of inputs x_n ∈ Rd and corresponding outputs y_n ∈ R, we condition on the function evaluations f = [f(x_n)]^N n=1 through a factorizing likelihood function that we assume to be Gaussian throughout this work.\n\np(y|f) = ∏^N n=1 N(y_n|f_n, σ^2_n)\n\nThe first two authors contributed equally to this work. We follow convention and assume this to be zero in this work.\n\nTackling Climate Change with Machine Learning: workshop at NeurIPS 2022.\n\nGPs are widely used within climate modeling as they are capable of producing well-characterized uncertainty estimates and, through the choice of kernel function, offer a principled way for experts to posit their prior beliefs into the model. Constructing GP models for climate data with non-trivial supports is an active area of research with recent works considering data defined on a sphere, the vertices of a graph, and vector fields. However, modeling a set of vectors, such as time-series, in a scalable manner is still an open problem.\n\nIn this work, we aim to answer the question: \"How can we probabilistically infer the latent function from a set of noisy vector-valued realizations?\" To answer this, we compose a set of GPs into a two-layered hierarchical model. In the model's upper layer, we learn a GP posterior distribution that represents the latent function. This distribution is then propagated down into the lower layer, where we learn a single GP posterior distribution for each observed realization.\n\nTo ensure scalable computation, we construct a variational approximation to each GP in the hierarchy by building upon the sparse GP literature. Finally, we use our sparse hierarchical Gaussian process (SHGP) model to infer latent surface temperature changes from an ensemble of climate models.\n\n2. Methodology: From Realizations to Distributions\n\nWe want to model a set of realizations Y = {y_r}^R r=1, where y_r is a vector in R^N. All y_r share a latent function ^y and are defined across the same index set X = {x_n}^N n=1. In our examples, these data D = {X, y_r}^R r=1 are a collection of surface temperature projections across a common temporal range, simulated by multiple climate models, where we wish to extract the latent climate signal.\n\nTo model these data, we use a Bayesian hierarchical model of GPs:\n\ng(X) ∼ GP(0, k(X, X_0))\nf(r)(X) ∼ GP(μ_g|y, k(r)(X, X_0))\n\nwhere g(X) is the group-level prior distribution that underpins all y_r. f(r)(X) is the prior distribution for an individual vector and has a mean given by the posterior mean of g, denoted μ_g|y = E_p(g|y)."
}