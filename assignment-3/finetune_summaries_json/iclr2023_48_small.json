{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" or unstable. As solar and wind energy production varies in time and space depending on the weather and the energy usage patterns differ for different households, how to store and exchange energy is an important research issue. In this work, we explore the use of deep reinforcement learning (DRL) for adaptive control of energy storage in local batteries and energy sharing through DC grids. We extend the Autonomous Power Interchange System (APIS) emulator from SonyCSL to combine it with reinforcement learning algorithms in each house. We implemented deep Q-network (DQN) and prioritized DQN to dynamically set the parameters of the real-time energy exchange protocol of APIS and tested it using the actual data collected from the DCOES in the faculty houses of Okinawa Institute of Science and Technology (OIST). The simulation results showed that RL agents outperformed the hand-tuned control strategy. Sharing average energy production, storage, and usage within the local community further improved efficiency. The implementation of DRL methods for adaptive energy storage and exchange can help reducing carbon emission and positively impact the climate. 1 I NTRODUCTION A local DC-based microgrid system was set up at the OIST faculty housing area in Okinawa, Japan (Tokoro, 2014; Werth et al., 2015). It focuses on redefining electricity grid systems in the form of interconnected nano-grid subsystems. It is designed to harness a mixture of renewable energy sources, including photovoltaic (PV) panels, rechargeable batteries, and local DC grid. It is also designed to be deployed in various communities and is especially helpful in rural, isolated communities. In contrast to conventional centralized energy systems, the distributed design of DCOES allows the energy source close to consumers with short electric mileage and energy storage in batteries to fill the temporal gap between supply and demand (Sakagami et al., 2015). Each house is equipped with PV panels and an energy storage system (ESS), including lithium-ion batteries, through which it is connected to the others via DC networks (Tokoro, 2015). Figure 1 is a sketch of energy exchange among several houses. Houses are connected with the energy exchange DC Grid Bus Line (blue dashed line) and a Communication Line (orange dashed line) to determine the deals of request/accept energy. With its ability to request and receive energy from the local neighbor nodes, this system has been proven in a real environment to be efficient in the use of renewable energy, flexible in size, as well as to increase dependability when used with utility electricity (Sakagami et al., 2016). So far, various approaches for optimizing the energy exchange have been applied to the DCOES. (Werth et al., 2016) devised a peer-to-peer control for DC microgrids. They found that PV and battery with DC exchanges outperformed the PV-only (classic home system) cases and the standalone (without DC exchanges) cases and performed close to the theoretical limit of centralized control. In addition, (Sakagami et al., 2016) also compared the standalone cases and DCOES case. They calculated the DCOES self-sufficiency rate (DSSR) to evaluate the performances in each case and found the optimal configurations of PV and installation cost. They found that the cost recovery 1Published as a workshop paper at \"Tackling Climate Change with Machine Learning\", ICLR 2023 Figure 1: ESS system and DCOES connections among different houses. period of the DCOES can further be cut down if more PVs and batteries are installed, while the current recovery period is 15 years Sakagami et al. (2016). More recently, (Kawamoto & Rajendiran, 2021) showed that using machine learning techniques could further optimize the energy exchange by minimizing the wasting of the surplus of solar energy. They found simple RNN (Elman-net) outperforms linear regression and LSTM methods in predicting the future hourly consumption from the past days to maintain the charging space for the battery. The battery could reserve more charging space and reduce the time of wasting surplus energy after the battery gets fully charged. The current control policy of DCOES is limited in using a fixed rule-based policy for energy sharing, which is manually adjusted. It is challenging to fine-tune for different households due to varying usage and storage levels. This results in a waste of energy. To further improve the effectiveness of the energy grid and achieve better energy management, it is desired to utilize an AI-based adaptive control system for energy sharing. Energy management is a complex task that requires balancing the demand and supply of energy to ensure optimal utilization of resources and minimizing costs. Reinforcement learning is a promising approach for energy management to make decisions in the face of uncertainty through interaction with the environment. Many previous works also tried applying RL to these problems. Hau et al. (2020) used a value-based reinforcement learning and DQN-based energy management algorithm for energy trading. They applied\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nThe stability of energy production from solar and wind sources varies over time and space due to weather conditions and differing energy usage patterns among households. Therefore, developing effective methods for storing and exchanging energy is a crucial research area. This work explores the application of deep reinforcement learning (DRL) for adaptive control of energy storage in local batteries and energy sharing through DC grids. We extended the Autonomous Power Interchange System (APIS) emulator from SonyCSL to combine it with reinforcement learning algorithms in each house. We implemented deep Q-network (DQN) and prioritized DQN to dynamically set the parameters of the real-time energy exchange protocol of APIS and tested it using actual data collected from the DCOES in the faculty houses of Okinawa Institute of Science and Technology (OIST). The simulation results showed that reinforcement learning agents outperformed the hand-tuned control strategy. Sharing average energy production, storage, and usage within the local community further improved efficiency. The implementation of DRL methods for adaptive energy storage and exchange can help reduce carbon emissions and positively impact the climate.\n\nA local DC-based microgrid system was set up at the OIST faculty housing area in Okinawa, Japan. It focuses on redefining electricity grid systems as interconnected nano-grid subsystems, harnessing a mixture of renewable energy sources, including photovoltaic (PV) panels, rechargeable batteries, and local DC grids. This system is designed to be deployed in various communities, particularly in rural and isolated areas. In contrast to conventional centralized energy systems, the distributed design of DCOES allows energy sources to be close to consumers, with short electric distances and energy storage in batteries to fill the temporal gap between supply and demand. Each house is equipped with PV panels and an energy storage system (ESS), including lithium-ion batteries, connected to others via DC networks.\n\nFigure 1 illustrates energy exchange among several houses, connected by an energy exchange DC Grid Bus Line and a Communication Line to determine energy request/accept deals. With its ability to request and receive energy from local neighbor nodes, this system has been proven to be efficient in using renewable energy, flexible in size, and increase dependability when used with utility electricity.\n\nVarious approaches have been applied to optimize energy exchange in DCOES. A peer-to-peer control for DC microgrids was devised, showing that PV and battery with DC exchanges outperformed PV-only and standalone cases. The self-sufficiency rate (DSSR) was calculated to evaluate performances in each case, and the optimal configurations of PV and installation cost were found. The cost recovery period of DCOES can be further reduced if more PVs and batteries are installed.\n\nRecently, machine learning techniques were used to optimize energy exchange by minimizing the wasting of surplus solar energy. Simple RNN (Elman-net) outperformed linear regression and LSTM methods in predicting future hourly consumption to maintain charging space for the battery. The battery could reserve more charging space and reduce the time of wasting surplus energy after being fully charged.\n\nThe current control policy of DCOES is limited by a fixed rule-based policy for energy sharing, which is manually adjusted. This results in energy waste due to varying usage and storage levels. To improve the effectiveness of the energy grid and achieve better energy management, an AI-based adaptive control system for energy sharing is desired. Energy management is a complex task requiring balancing demand and supply to ensure optimal resource utilization and minimize costs. Reinforcement learning is a promising approach for energy management, making decisions in the face of uncertainty through interaction with the environment. Previous works have applied RL to these problems, including value-based reinforcement learning and DQN-based energy management algorithms for energy trading."
}