{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" processes. Over the years, with significantly improved computational performance, a series of Deep Learning (DL) parameterization schemes have been developed and incorporated into GCMs. However, these schemes use classic architectures whereas the new attention mechanism is not widely investigated. We proposed a “memory-aware” transformer-based model on ClimSim, the largest-ever dataset for climate param- eterization. Our results show that the attention mechanism successfully captures the complex non-linear dependencies of sub-grid scale variables and reduces the prediction error. 1 Introduction Understanding and modeling complex physical processes in the earth system is of great importance for making prompt and precise decisions regarding climate change issues. The Global Climate Model (GCM) is a tool to simulate the changes in the Earth’s climate due to both natural and anthropogenic activities. Since the end of the last century, GCMs have been developed rapidly with joint efforts from Earth system scientists and computer scientists. With the help of supercomputers, there was a large improvement in terms of the model complexity and prediction accuracy in the current generation of GCMs compared to their early counterparts. However, the current generation of GCMs still has a coarse spatial resolution of 100-200 km and a large spread in simulations due to the presence of sub-grid scale physical processes. For example, studies have shown that one of the major sources of uncertainty in the current generation of GCMs is the simulation of small-scale clouds [ 1,2]. These processes cannot be explicitly resolved by the model and thus must be parameterized. Over the years, a series of parameterization schemes were developed using the traditional data assimilation method, integrating observational data with a numerical model to estimate the state of the Earth’s system. However, in recent years, there has been a rising trend in developing parameterization schemes using data-driven approaches like Deep Learning (DL) [ 3,4]. One critical challenge of this method is the acquisition of sub-grid information since it requires high-resolution model runs. Although some studies took an alternative approach by testing these schemes on a highly idealized model such as the Lorenz 96 system, which consists of two sets of differential equations representing Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.the change in sub-grid and grid-scale variables [ 5,6,7], it is preferable to test parameterization schemes directly on real-geography GCMs. The newly published ClimSim dataset addressed this challenge, providing the largest and most physically comprehensive testbed for DL parameterization schemes on climate simulations [8]. In this work, we proposed a transformer-based “memory-aware” parameterization scheme on the ClimSim dataset. Transformer is a new generation of DL model first introduced in the field of Natural Language Processing [ 9] and applied to machine translation [ 10] and text generation [ 11]. The attention mechanism in transformers performs well at handling long-range dependencies in sequential data and thus has been used for time series prediction recently [ 12,13,14]. Current DL parameterization schemes on weather and climate primarily use classic architectures such as Convolutional Neural Networks [ 15,16,17], Multi-Layer Perceptron [ 4,18], and Generative Adversarial Networks [ 6,19,20], whereas the attention mechanism is not widely investigated. The attention mechanism allows transformer models to focus on specific parts of the input data, assigning importance scores to previous data points. In this way, a transformer model can generate a dynamic memory bank while predicting the next item. For parametrization schemes, it translates to the model capturing inter-state and intra-state temporal dependencies while approximating sub-grid scale processes. Our results suggest that the attention mechanism effectively captures the complex non-linear dependencies of sub-grid processes and achieves lower prediction error compared to classic architectures. 2 Data and Methods 2.1 The ClimSim Dataset ClimSim is a dataset for developing machine-learning emulators on climate parameterization [ 8]. The input and output variables are generated by the Energy Exascale Earth System Model (E3SM) with a Multiscale Modeling Framework (MMF), representing the grid-scale and sub-grid scale physical processes in the Earth’s system, respectively. Details of these variables are provided in Table A1. We use the low-resolution, real-geography dataset from the ClimSim suite, which contains 10 years of data at 384 unstructured spatial grids with a temporal resolution of 20 minutes. We further subsample the dataset to have an effective temporal resolution of 140 minutes while keeping the spatial structure intact. The training set covers 7 years (0001-02 to 0008-01) and the validation set includes 1 year (0008-02 to 0009-01). The test set has the same temporal coverage as the validation set but with a temporal resolution of 120 minutes to approximate a different climate model run. We use this subsampled version for two reasons: there is a memory limitation to input all the data without subsampling; the variables used are consistent\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy data removed:\n\nUnderstanding and modeling complex physical processes in the Earth's system is crucial for making informed decisions about climate change. The Global Climate Model (GCM) is a tool used to simulate climate changes caused by natural and human activities. Since the end of the last century, GCMs have been developed rapidly, with significant improvements in model complexity and prediction accuracy. However, current GCMs still have a coarse spatial resolution of 100-200 km and a large spread in simulations due to sub-grid scale physical processes. For example, studies have shown that small-scale clouds are a major source of uncertainty in current GCMs. These processes cannot be explicitly resolved by the model and must be parameterized.\n\nOver the years, various parameterization schemes have been developed using traditional data assimilation methods. However, in recent years, there has been a growing trend towards developing parameterization schemes using data-driven approaches like Deep Learning (DL). One challenge of this method is acquiring sub-grid information, which requires high-resolution model runs. To address this challenge, researchers have tested these schemes on idealized models, such as the Lorenz 96 system. However, it is preferable to test parameterization schemes directly on real-geography GCMs.\n\nThe ClimSim dataset has addressed this challenge by providing a large and comprehensive testbed for DL parameterization schemes on climate simulations. In this work, we proposed a transformer-based \"memory-aware\" parameterization scheme on the ClimSim dataset. The transformer model is a new generation of DL model that performs well in handling long-range dependencies in sequential data. The attention mechanism in transformers allows the model to focus on specific parts of the input data, assigning importance scores to previous data points. This enables the model to generate a dynamic memory bank while predicting the next item.\n\nOur results show that the attention mechanism effectively captures the complex non-linear dependencies of sub-grid processes and achieves lower prediction error compared to classic architectures. The attention mechanism translates to the model capturing inter-state and intra-state temporal dependencies while approximating sub-grid scale processes.\n\n2 Data and Methods\n\n2.1 The ClimSim Dataset\n\nThe ClimSim dataset is a collection of data for developing machine-learning emulators on climate parameterization. The input and output variables are generated by the Energy Exascale Earth System Model (E3SM) with a Multiscale Modeling Framework (MMF), representing grid-scale and sub-grid scale physical processes in the Earth's system. The dataset contains 10 years of data at 384 unstructured spatial grids with a temporal resolution of 20 minutes. We subsampled the dataset to have an effective temporal resolution of 140 minutes while keeping the spatial structure intact. The training set covers 7 years, the validation set includes 1 year, and the test set has the same temporal coverage as the validation set but with a temporal resolution of 120 minutes. We used this subsampled version due to memory limitations and to ensure consistent variables."
}