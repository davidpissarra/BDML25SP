{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" or eliminate them. Attaining such a task requires the highest degree of the flow of knowledge from science into policy. The multidisciplinary, location-specific, and vastness of published science makes it challenging to keep track of novel work in this area, as well as making the traditional knowledge synthesis methods inefficient in infusing science into policy. To this end, we consider developing multiple domain-specific language models (LMs) with different variations from Climate- and Health-related information, which can serve as a foundational step toward capturing available knowledge to enable solving different tasks, such as detecting similarities between climate- and health-related concepts, fact-checking, relation extraction, evidence of health effects to policy text generation, and more. To our knowledge, this is the first work that proposes developing multiple domain-specific language models for the considered domains. We will make the developed models, resources, and codebase available for the researchers.  1 Introduction The World Health Organization (WHO) recognizes climate change as the most significant health threat to humans in the 21st century, which comes in many forms and is expected to cause approximately 250,000 additional deaths annually from 2030 to 2050 [1]. With timely and effective adaptation, many of these health risks can be reduced or avoided, but this requires comprehensive studies and policies that are multi-sectoral, multi-system, and collaborative at different scales [2]. Reaching that goal needs a thorough understanding and synthesis of the ever-growing research to inform policies [3]. While systematic reviews are the only commonly accepted tool for evidence synthesis [4], exponential growth in literature makes manual methods unattainable and biased toward more restrictive inclusion criteria [5]. To overcome these limitations, a recent study used different machine learning methods to map climate and health scientific literature [5]. Such comprehensive mappings can provide necessary inputs into global climate and health assessment, especially considering that bibliographic analysis of climate change policy documents shows that only 1.2% of climate change papers have at least one policy citation [6]. While such advancements in climate and health mapping can help distinguish gaps and focus areas, it becomes more challenging to capture the hidden knowledge in the ever-increasing literature. To this end, we propose to develop relevant multi-domain LMs that can serve as a foundation for various downstream NLP tasks such as identifying similarities between health and climate concepts, relationship extraction, summarization, classification, policy text generation, etc. [7]. To the best of our knowledge, this is the first work that proposes developing multiple domain specific LMs with different datasets and fine-tuning variations for the considered domains.       2 2 Related Work Our proposed work is motivated by pre-trained LMs, such as BERT, which have shown considerable improvements in different downstream NLP tasks where they are fine-tuned on other domain-specific corpora [8]â€“[11]. Various LMs are available separately in our considered domains that are similarly fine-tuned on domain-specific corpora. For example, ClimateBERT is a domain-adapted pretraining of DistilROBERTA over 1.6 million paragraphs of climate-related texts from different resources such as news, scientific articles, and reports and fine-tuned over three downstream tasks with considerable improvements compared to the base model over climate-related NLP tasks [8], [12]. BioBERT [9] is another domain-specific LM pre-trained on biomedical corpora over BERT. Its largest corpora contain over 21B words from Wiki, Books, and scientific articles from PubMed and PMC. diseaseBERT is a domain-specific LM that can distinguish different aspects of diseases in a document [13]. It created the model through disease knowledge infusion training of BERT, ALBERT, and four pre-trained biomedical LMs (BlueBERT, ClinicalBERT, SciBERT, BioBERT). It showed improvements in three biomedical NLP tasks over their base models in all six models.  3 Methodology 3.1 Datasets Our pre-training dataset contains scientific literature and policy documents in pdf format. We will also use a smaller and separate set of documents for downstream fine-tuning tasks. The pre-training corpora consist of the 16,078 scientific papers that are listed by Berrang-Ford et al. 2021 in their evidence synthesis study [5], and a total of 311 related policy documents that are listed in three studies [6], [14], [15]. We are also considering other Global or national policy documents such as IPCC AR6 Chapter 7 [2] and the US National Climate Assessment Chapter 14 on Health [16]. We will also consider searching in the Overton1 database for more climate and health policy documents at national or regional levels. After collecting all policy documents, we will check and remove possible duplications. For downstream fine-tuning of the models, we will collect 100 scientific articles on climate- and health-related topics published after April 2020, parse them into paragraphs and label each paragraph accordingly. This will consider documents that were not part of our pre-training corpora. We are also considering using the\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nAttaining a task that requires the highest degree of knowledge flow from science into policy is challenging due to the multidisciplinary, location-specific, and vast nature of published science. Traditional knowledge synthesis methods are inefficient in infusing science into policy. To address this, we propose developing multiple domain-specific language models (LMs) with different variations for Climate- and Health-related information. These models can serve as a foundational step toward capturing available knowledge to enable solving different tasks, such as detecting similarities between climate- and health-related concepts, fact-checking, relation extraction, evidence of health effects to policy text generation, and more.\n\nTo our knowledge, this is the first work that proposes developing multiple domain-specific language models for the considered domains. We will make the developed models, resources, and codebase available for researchers.\n\nThe World Health Organization (WHO) recognizes climate change as the most significant health threat to humans in the 21st century, with approximately 250,000 additional deaths expected annually from 2030 to 2050. Timely and effective adaptation can reduce or avoid many of these health risks, but this requires comprehensive studies and policies that are multi-sectoral, multi-system, and collaborative at different scales. A thorough understanding and synthesis of the ever-growing research are necessary to inform policies.\n\nSystematic reviews are the commonly accepted tool for evidence synthesis, but exponential growth in literature makes manual methods unattainable and biased toward more restrictive inclusion criteria. A recent study used machine learning methods to map climate and health scientific literature, providing necessary inputs into global climate and health assessment. However, bibliographic analysis of climate change policy documents shows that only 1.2% of climate change papers have at least one policy citation.\n\nTo overcome these limitations, we propose developing relevant multi-domain LMs that can serve as a foundation for various downstream NLP tasks such as identifying similarities between health and climate concepts, relationship extraction, summarization, classification, policy text generation, etc. To the best of our knowledge, this is the first work that proposes developing multiple domain-specific LMs with different datasets and fine-tuning variations for the considered domains.\n\nOur proposed work is motivated by pre-trained LMs, such as BERT, which have shown considerable improvements in different downstream NLP tasks where they are fine-tuned on other domain-specific corpora. Various LMs are available separately in our considered domains that are similarly fine-tuned on domain-specific corpora. For example, ClimateBERT is a domain-adapted pretraining of DistilROBERTA over 1.6 million paragraphs of climate-related texts from different resources. BioBERT is another domain-specific LM pre-trained on biomedical corpora over BERT. diseaseBERT is a domain-specific LM that can distinguish different aspects of diseases in a document.\n\nWe will use a dataset containing scientific literature and policy documents in pdf format for pre-training. The pre-training corpora consist of 16,078 scientific papers listed by Berrang-Ford et al. 2021 and 311 related policy documents listed in three studies. We will also consider other Global or national policy documents and search the Overton1 database for more climate and health policy documents at national or regional levels. After collecting all policy documents, we will check and remove possible duplications.\n\nFor downstream fine-tuning of the models, we will collect 100 scientific articles on climate- and health-related topics published after April 2020, parse them into paragraphs, and label each paragraph accordingly. This will consider documents that were not part of our pre-training corpora."
}