{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"Universität zu Berlin Berlin, Germany vihsjana@hu-berlin.de Abstract Understandable and persuasive recommendations support the electricity consumers’ behavioral change to tackle the energy efficiency problem. This paper proposes an explainable multi-agent recommendation system for load shifting for the household appliances. First, we provide agents with enhanced predictive capacity by including weather data, applying state-of-the-art models and tuning the hyperparameters. Sec- ond, we suggest an Explainability Agent providing transparent recommendations. Third, we discuss the potential impact of the suggested approach. 1 Introduction Europe faces a double urgency to increase energy efficiency: on the one hand, caused by the war in Ukraine, on the other hand, due to the continuous rise in electricity consumption [1]. Tackling the energy efficiency problem through consumers’ behavioral change is an obvious, however challenging solution. People often need a guidance, and sometimes a soft nudge to put the intentions into actions [2], for instance, to change the timing of appliances usage. Recommender systems can suggest energy- efficient actions to facilitate such behavioral change. To increase the trust in the recommendation system, and, thus, the acceptance rate of recommendations, users need to understand why and how the model makes its predictions [3], [4]. Thus, the recommendation system should be explainable. The existing research on explainability in recommender systems for energy-efficient smart homes is very scarce [5]. [6] provide a thorough literature review on explainability in recommender systems for other application domains. However, most existing approaches are not applicable to the smart home area because of the missing data structures. [7] design an explainable context-aware recommendation system for a smart home ecosystem. They show that displaying the advantages and the reasoning behind recommendations lead to a 19% increase in acceptance rate. To our best knowledge, the issue of explainability in multi-agent recommendation systems for energy-efficient smart homes has not been studied yet. ∗Corresponding author. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.Our contributions are twofold. First, we suggest an explainable multi-agent recommendation system for energy efficiency in private households. In particular, we extend a novel multi-agent approach of [8] by designing an Explainability Agent that provides explainable recommendations for optimal appliance scheduling in a textual and visual manner. Second, we enhance the predictive capacity of other agents by including weather data and applying state-of-the-art models. We also provide an overview of predictive and explainability performance. We provide a comprehensive tutorial in Jupyter Notebook in GitHub1for all the steps described in this paper and beyond. 2 Explainable multi-agent recommendation system [8] introduce a utility-based context-aware multi-agent recommendation system that provides load shifting recommendations for household devices for the next 24 hours. Their system includes six agents: Price Agent (prepares external hourly electricity prices), Preparation Agent (prepares data for the other agents), Availability Agent (predicts the hourly user availability for the next 24 hours), Usage Agent (calculates the devices’ usage probabilities for the prediction day), Load Agent (extracts the typical devices’ loads), and Recommendation Agent (collects the inputs from the other agents and provides recommendations). The multi-agent architecture of [8] is flexible and can be easily integrated into existing smart home systems. However, the cost of the simplicity of the approach (i.e., they use Logistic Regression for the availability and usage predictions) is a relatively low prediction accuracy. We address the limitations in [8] by enhancing the performance of the Availability and the Usage Agents. In particular, we apply the K-Nearest-Neighbours (KNN), extreme gradient boosting (XG- Boost), adaptive boosting (AdaBoost), and Random Forest to predict the availability and usage probabilities. Furthermore, we use Logistic Regression (Logit) and Explainable Boosting Machines (EBM, see [9]) as inherently explainable models. We propose including the Explainability Agent in the system (see Figure 1). The explainability models are divided into local and global, depending on their capability to explain a particular instance or the entire model. Since we want to help the user understand a single recommendation, we focus on local approaches. In particular, we apply post-model approaches LIME (local, interpretable, model-agnostic explanation; [10]) and SHAP (Shapley additive explanations; [11]) as model-agnostic tools that can explain the predictions of the chosen classifiers. Figure 1: Architecture of the explainable multi-agent recommendation system. To create an explanation for a recommendation, the Explainability Agent extracts feature importance from the explainability models. We design the explanation to include two parts: (i) Usage explanation - which features lead to the specific device usage prediction for the day? and (ii) Availability explanation - which features drive the user availability prediction for the hour? We do not include an explanation for the Load Agent since we do not consider the extracted typical load profile of every shiftable device as informative to the users. As a\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nUniversity of Berlin, Berlin, Germany\nvihsjana@hu-berlin.de\n\nAbstract\nUnderstandable and persuasive recommendations can support electricity consumers' behavioral change to tackle the energy efficiency problem. This paper proposes an explainable multi-agent recommendation system for load shifting for household appliances. First, we enhance the predictive capacity of agents by including weather data and applying state-of-the-art models, and tuning hyperparameters. Second, we suggest an Explainability Agent providing transparent recommendations. Third, we discuss the potential impact of the suggested approach.\n\n1 Introduction\nEurope faces a double urgency to increase energy efficiency: on the one hand, caused by the war in Ukraine, on the other hand, due to the continuous rise in electricity consumption. Tackling the energy efficiency problem through consumers' behavioral change is an obvious, yet challenging solution. People often need guidance, and sometimes a soft nudge to put their intentions into action, for instance, to change the timing of appliances usage. Recommender systems can suggest energy-efficient actions to facilitate such behavioral change. To increase the trust in the recommendation system, and thus, the acceptance rate of recommendations, users need to understand why and how the model makes its predictions. Thus, the recommendation system should be explainable.\n\nThe existing research on explainability in recommender systems for energy-efficient smart homes is very scarce. A literature review on explainability in recommender systems for other application domains is provided in [2] and [3]. However, most existing approaches are not applicable to the smart home area due to the missing data structures. [4] designed an explainable context-aware recommendation system for a smart home ecosystem, showing that displaying the advantages and the reasoning behind recommendations leads to a 19% increase in acceptance rate. To our best knowledge, the issue of explainability in multi-agent recommendation systems for energy-efficient smart homes has not been studied yet.\n\nOur contributions are twofold. First, we suggest an explainable multi-agent recommendation system for energy efficiency in private households. In particular, we extend a novel multi-agent approach by designing an Explainability Agent that provides explainable recommendations for optimal appliance scheduling in a textual and visual manner. Second, we enhance the predictive capacity of other agents by including weather data and applying state-of-the-art models. We also provide an overview of predictive and explainability performance.\n\n2 Explainable multi-agent recommendation system\n[1] introduced a utility-based context-aware multi-agent recommendation system that provides load shifting recommendations for household devices for the next 24 hours. Their system includes six agents: Price Agent (prepares external hourly electricity prices), Preparation Agent (prepares data for the other agents), Availability Agent (predicts the hourly user availability for the next 24 hours), Usage Agent (calculates the devices' usage probabilities for the prediction day), Load Agent (extracts the typical devices' loads), and Recommendation Agent (collects the inputs from the other agents and provides recommendations).\n\nThe multi-agent architecture of [1] is flexible and can be easily integrated into existing smart home systems. However, the cost of the simplicity of the approach (i.e., they use Logistic Regression for the availability and usage predictions) is a relatively low prediction accuracy. We address the limitations in [1] by enhancing the performance of the Availability and the Usage Agents. In particular, we apply the K-Nearest-Neighbors (KNN), extreme gradient boosting (XGBoost), adaptive boosting (AdaBoost), and Random Forest to predict the availability and usage probabilities. Furthermore, we use Logistic Regression (Logit) and Explainable Boosting Machines (EBM, see [5]) as inherently explainable models.\n\nWe propose including the Explainability Agent in the system (see Figure 1). The explainability models are divided into local and global, depending on their capability to explain a particular instance or the entire model. Since we want to help the user understand a single recommendation, we focus on local approaches. In particular, we apply post-model approaches LIME (local, interpretable, model-agnostic explanation; [6]) and SHAP (Shapley additive explanations; [7]) as model-agnostic tools that can explain the predictions of the chosen classifiers.\n\nFigure 1: Architecture of the explainable multi-agent recommendation system.\n\nTo create an explanation for a recommendation, the Explainability Agent extracts feature importance from the explainability models. We design the explanation to include two parts: (i) Usage explanation - which features lead to the specific device usage prediction for the day? and (ii) Availability explanation - which features drive the user availability prediction for the hour? We do not include an explanation for the Load Agent since we do not consider the extracted typical load profile of every shiftable device as informative to the users.\n\nReferences:\n[1] \n[2] \n[3] \n[4] \n[5] \n[6] \n"
}