{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"pretable Machine Learning (IML) is expected to remove significant barriers for the application of Machine Learning (ML) algorithms in power systems. This work first seeks to showcase the benefits of SHapley Additive exPlanations (SHAP) for understanding the outcomes of ML models, which are increasingly being used to optimise power systems with increasing share of Renewable Energy (RE), to support worldwide calls for decarbonisation and climate change. To do so, we demonstrate that the Power Transfer Distribution Factors (PTDF)—a power system physics-based linear sensitivity index—can be derived from the SHAP values. To do so, we take the derivatives of SHAP values from a ML model trained to learn line-flows from generator power-injections, using a DC power-flow case in a benchmark test network. In demonstrating that SHAP values can be related back to the physics that underpin the power system, we build confidence in the explanations SHAP can offer. 1 Introduction Tackling climate change and mitigating its effects on the ecosystem and mankind are one of the key challenges of today’s world. In response to worldwide calls on decarbonisation, the Paris Agreement on climate change (2021) has set a target to minimize global warming to 1.5◦C[1]. In order to meet this ambitious target, the emission of greenhouse gases (GHGs) like CO2need to be drastically cut down. It is estimated that around 25% GHG emissions come from electricity and heating [ 2]). Therefore, in order to cut down these emissions, there is a global trend to integrate more renewable energy sources (like wind, solar, etc.) for electric power generation. To integrate them effectively into legacy power systems (both in a cost-effective and technically feasible manner), power system operators need to resort to more efficient techniques for solving day-to-day problems like optimal dispatch of generators and longer term problems like transmission system expansion planning. For example, a 2012 report from the Federal Energy Regulatory Commission (FERC) estimated that the ∗Corresponding author – tabia.ahmad@manchester.ac.uk Tackling Climate Change with Machine Learning: workshop at ICLR 2024.inefficiencies induced by approximate-solution techniques may cost billions of dollars and release unnecessary emissions [3]. In recent years, many Machine Learning (ML) applications to predict the behaviour of power systems have been developed—some of which are summarised in [ 4]. While these black-box ML algorithms have shown good accuracy and computational savings, their applicability to mission- critical infrastructure such as power systems is limited due to absence of trustworthy explanations. The premise of interpretable ML—an emerging area of research—is to provide detailed explanations of ML model predictions in order to enhance confidence in the model predictions. Post-hoc ML model interpretability methods are presented concisely in [ 5]. Notably, Local Interpretable Model- agnostic Explanations (LIME) is a local technique capable of providing feature effects for individual points that can be extrapolated to form global explanations[ 6]. However, a key limitation with LIME is that defining the neighbourhood around the instance for explanation is complicated and can lead to errors. Permutation Feature Importance (PFI) is a global technique[ 7], whereby PFI can provide feature importance, but not feature effects. The authors in [ 8] use gradient-based neural network interpretability methods like Integrated Gradients, Expected Gradients, and DeepLIFT for forecasting of aggregated renewable generation. SHapley Additive exPlanations (SHAP) has gained some initial traction in power systems, as [ 9] reviews. For example, in [ 10], SHAP is used to provide explanations of ML models for gaining insights for complex underlying mechanisms affecting the transient stability boundary of power systems (a phenomenon driven by complex non-linear dynamics). In [ 11] SHAP is used for the frequency stability problem. In [ 12], authors use SHAP values to determine the relationship between power system small-signal stability and topological graph metrics relating to connection of RE. The focus of the limited number of applications in literature, however, usually centre around applying SHAP, rather than analysing the method itself for proving one-to-one equivalence with the physics of the system, which is indeed non-trivial. The SHAP [ 13] framework proposes using a simple linear explanation model as an interpretable approximation of the complex original model. In doing so, a class of additive feature attribution methods can be defined which unifies six existing IML methods (including LIME and DeepLIFT) demonstrating that SHAP is the only method that possesses the aforementioned properties. Such methods construct a simple additive explanation model,g—which is a linear function of binary variables—to represent the complex original model,f. In the SHAP framework, the explanation model is expressed as a “conditional expectation function of the original model” [ 13]. Simplified inputs x′are used to map to the original input, xthrough mapping function\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nPretable Machine Learning (IML) is expected to remove significant barriers for the application of Machine Learning (ML) algorithms in power systems. This work aims to showcase the benefits of SHapley Additive exPlanations (SHAP) for understanding the outcomes of ML models, which are increasingly being used to optimize power systems with an increasing share of Renewable Energy (RE), to support worldwide calls for decarbonisation and climate change mitigation. To do so, we demonstrate that the Power Transfer Distribution Factors (PTDF) - a power system physics-based linear sensitivity index - can be derived from the SHAP values. We achieve this by taking the derivatives of SHAP values from a ML model trained to learn line-flows from generator power-injections, using a DC power-flow case in a benchmark test network. By demonstrating that SHAP values can be related back to the physics that underpin the power system, we build confidence in the explanations SHAP can offer.\n\nIntroduction\n\nTackling climate change and mitigating its effects on the ecosystem and humanity are one of the key challenges of today's world. In response to worldwide calls for decarbonisation, the Paris Agreement on climate change (2021) has set a target to minimize global warming to 1.5°C. To meet this ambitious target, the emission of greenhouse gases (GHGs) like CO2 needs to be drastically cut down. It is estimated that around 25% of GHG emissions come from electricity and heating. Therefore, to cut down these emissions, there is a global trend to integrate more renewable energy sources (like wind and solar) for electric power generation. To integrate them effectively into legacy power systems (both in a cost-effective and technically feasible manner), power system operators need to resort to more efficient techniques for solving day-to-day problems like optimal dispatch of generators and longer-term problems like transmission system expansion planning.\n\nFor example, a 2012 report from the Federal Energy Regulatory Commission (FERC) estimated that the inefficiencies induced by approximate-solution techniques may cost billions of dollars and release unnecessary emissions. In recent years, many Machine Learning (ML) applications to predict the behavior of power systems have been developed. While these black-box ML algorithms have shown good accuracy and computational savings, their applicability to mission-critical infrastructure such as power systems is limited due to the absence of trustworthy explanations. The premise of interpretable ML - an emerging area of research - is to provide detailed explanations of ML model predictions to enhance confidence in the model predictions. Post-hoc ML model interpretability methods are presented in [5]. Notably, Local Interpretable Model-agnostic Explanations (LIME) is a local technique capable of providing feature effects for individual points that can be extrapolated to form global explanations. However, a key limitation with LIME is that defining the neighborhood around the instance for explanation is complicated and can lead to errors. Permutation Feature Importance (PFI) is a global technique that can provide feature importance but not feature effects.\n\nSHapley Additive exPlanations (SHAP) has gained some initial traction in power systems, as reviewed in [9]. For example, in [10], SHAP is used to provide explanations of ML models for gaining insights into complex underlying mechanisms affecting the transient stability boundary of power systems (a phenomenon driven by complex non-linear dynamics). In [11], SHAP is used for the frequency stability problem. In [12], authors use SHAP values to determine the relationship between power system small-signal stability and topological graph metrics relating to the connection of RE. The focus of the limited number of applications in the literature, however, usually centers around applying SHAP, rather than analyzing the method itself for proving one-to-one equivalence with the physics of the system, which is indeed non-trivial.\n\nThe SHAP framework proposes using a simple linear explanation model as an interpretable approximation of the complex original model. In doing so, a class of additive feature attribution methods can be defined, which unifies six existing IML methods (including LIME and DeepLIFT), demonstrating that SHAP is the only method that possesses the aforementioned properties. Such methods construct a simple additive explanation model, g, which is a linear function of binary variables, to represent the complex original model, f. In the SHAP framework, the explanation model is expressed as a \"conditional expectation function of the original model.\" Simplified inputs x' are used to map to the original input, x, through a mapping function."
}