{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" for reliably planning for and responding to extreme weather events in a warming world. This motivates the need for well-calibrated ensembles in probabilistic weather forecasting. We present initial results for the calibration of large-scale deep neural weather models for data-driven probabilistic weather forecasting. By explicitly accounting for uncertainties about the forecast’s initial condition and model parameters, we gen- erate ensemble forecasts that show promising results on standard diagnostics for probabilistic forecasts. Specifically, we are approaching the Integrated Forecasting System (IFS), the gold standard on probabilistic weather forecasting, on: (i) the spread-error agreement; and (ii) the Continuous Ranked Probability Score (CRPS). Our approach scales to state-of-the-art data-driven weather models, enabling cheap post-hoc calibration of pretrained models with tens of millions of parameters and paving the way towards the next generation of well-calibrated data-driven weather models. 1 Introduction Data-driven neural weather models [Sønderby et al., 2020] [Pathak et al., 2022] [Hu et al., 2022] promise to revolutionize our capacity to predict extreme weather events, mitigate their disastrous impacts, manage energy systems, and democratize access to high-quality weather forecasts. In contrast to traditional, hand-crafted numerical weather prediction (NWP) systems like the IFS by the European Centre for Medium-Range Weather Forecasts (ECMWF), these deep learning-based systems learn to forecast future weather conditions given an initial condition from vast amounts of historical observational or reanalysis data. This enables orders-of-magnitude speedups over state-of- the-art NWP systems on modern hardware accelerators, while potentially matching or surpassing NWP forecasting skill [Schultz et al., 2021]. Earth’s weather is a highly complex, multi-scale, nonlinear, chaotic system. Weather forecasts are made based on incomplete knowledge of initial conditions and uncertainties in the model’s characterization of the physics of the Earth system. Hence, it is crucial to quantify uncertainties in our predictions to enable informed decision making. Therefore, a requirement for most applications Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.of weather prediction systems are probabilistic forecasts. These probabilistic forecasts often take the form of ensemble forecasts [Houtekamer and Derome, 1995] [Atger, 1999], predicting ndifferent trajectories using perturbed initial conditions or perturbed parameters in models to statistically quantify forecast uncertainties. Deep neural networks, however, just like deterministic NWP models, are known to be overconfident in their predictions, unless careful countermeasures are taken [Nguyen et al., 2015] [Guo et al., 2017]. In this paper we show that model-agnostic perturbation strategies based on spatially correlated scale- aware initial condition noise and Bayesian model uncertainty leads to well-calibrated ensembles of trajectories and consistent calibration gains over the baseline, measured by metrics routinely used to quantify reliability of probabilistic weather forecasts: (i) spread-error agreement; and (ii) the CRPS. Figure 1: The ensembling strategy presented in this paper: (i) Perturbing initial conditions with spatially correlated noise to produce ninitial states; and (ii) Sampling ndifferent FourCastNet models from a posterior distribution over models. Each model auto-regressively predicts a unique trajectory. 2 Methods 2.1 FourCastNet and ERA5 Reanalysis We base this work on the state-of-the-art FourCastNet [Pathak et al., 2022], a data-driven high- resolution global weather model. FourCastNet is an auto-regressive transformer-based neural network based on the Adaptive Fourier Neural Operator [Guibas et al., 2021]. Given an initial condition, FourCastNet predicts the atmospheric state 6 hours into the future. The output of the network then gets fed back into the network to iteratively produce subsequent forecasts, time-step by time-step, up to two weeks. FourCastNet is highly scalable [Kurth et al., 2022] and its skill has been shown to be approaching ECMWF’s IFS, including on extreme weather events such as hurricanes, tropical cyclones, and extratropical cyclones. Here, we take the ground-truth for training and evaluation to be a 26-variable subset of the ERA5 global reanalysis dataset [Hersbach et al., 2020] provided by ECMWF. The input and output of the neural network are therefore 26 meteorological variables at a global spatial resolution of 0.25 degrees, or 25 km (represented by a tensor of shape [26, 720, 1440]). 2.2 Initial condition uncertainty To create ensemble forecasts we perturb the initial condition (IC), i.e. the weather conditions from which we start the forecast. We thus run ntrajectories from perturbed versions of our initial state. We empirically notice that perturbing the initial condition by uncorrelated Gaussian noise leads to under-dispersive\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy and dirty parts removed:\n\nFor reliable planning and response to extreme weather events in a warming world, well-calibrated ensembles in probabilistic weather forecasting are essential. We present initial results for calibrating large-scale deep neural weather models for data-driven probabilistic weather forecasting. By accounting for uncertainties in the forecast's initial condition and model parameters, we generate ensemble forecasts that show promising results on standard diagnostics for probabilistic forecasts. Specifically, we approach the Integrated Forecasting System (IFS), the gold standard for probabilistic weather forecasting, on two metrics: (i) spread-error agreement and (ii) the Continuous Ranked Probability Score (CRPS). Our approach scales to state-of-the-art data-driven weather models, enabling cheap post-hoc calibration of pre-trained models with tens of millions of parameters and paving the way for the next generation of well-calibrated data-driven weather models.\n\nData-driven neural weather models [Sønderby et al., 2020; Pathak et al., 2022; Hu et al., 2022] have the potential to revolutionize our ability to predict extreme weather events, mitigate their impacts, manage energy systems, and provide high-quality weather forecasts to everyone. Unlike traditional numerical weather prediction (NWP) systems like the IFS by the European Centre for Medium-Range Weather Forecasts (ECMWF), these deep learning-based systems learn to forecast future weather conditions given an initial condition from vast amounts of historical observational or reanalysis data. This enables significant speedups over state-of-the-art NWP systems on modern hardware accelerators, while potentially matching or surpassing NWP forecasting skill [Schultz et al., 2021].\n\nEarth's weather is a complex, multi-scale, nonlinear, and chaotic system. Weather forecasts are based on incomplete knowledge of initial conditions and uncertainties in the model's characterization of the Earth system. Therefore, it is crucial to quantify uncertainties in our predictions to enable informed decision-making. Probabilistic forecasts, which often take the form of ensemble forecasts [Houtekamer and Derome, 1995; Atger, 1999], are a requirement for most weather prediction applications. These ensemble forecasts predict different trajectories using perturbed initial conditions or parameters to statistically quantify forecast uncertainties. However, deep neural networks, like deterministic NWP models, are known to be overconfident in their predictions unless careful countermeasures are taken [Nguyen et al., 2015; Guo et al., 2017].\n\nIn this paper, we show that model-agnostic perturbation strategies based on spatially correlated scale-aware initial condition noise and Bayesian model uncertainty lead to well-calibrated ensembles of trajectories and consistent calibration gains over the baseline, measured by metrics used to quantify the reliability of probabilistic weather forecasts: (i) spread-error agreement and (ii) the CRPS.\n\nWe base our work on the state-of-the-art FourCastNet [Pathak et al., 2022], a data-driven high-resolution global weather model. FourCastNet is an auto-regressive transformer-based neural network based on the Adaptive Fourier Neural Operator [Guibas et al., 2021]. Given an initial condition, FourCastNet predicts the atmospheric state 6 hours into the future. The output of the network is then fed back into the network to iteratively produce subsequent forecasts, time-step by time-step, up to two weeks. FourCastNet is highly scalable [Kurth et al., 2022] and has been shown to approach ECMWF's IFS, including on extreme weather events such as hurricanes, tropical cyclones, and extratropical cyclones.\n\nWe use a 26-variable subset of the ERA5 global reanalysis dataset [Hersbach et al., 2020] provided by ECMWF as the ground-truth for training and evaluation. The input and output of the neural network are 26 meteorological variables at a global spatial resolution of 0.25 degrees, or 25 km (represented by a tensor of shape [26, 720, 1440]).\n\nTo create ensemble forecasts, we perturb the initial condition (IC), i.e., the weather conditions from which we start the forecast. We run multiple trajectories from perturbed versions of our initial state. We find that perturbing the initial condition by uncorrelated Gaussian noise leads to under-dispersive forecasts."
}