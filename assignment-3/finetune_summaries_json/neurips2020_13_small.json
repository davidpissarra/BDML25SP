{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" learning, to accurately predict temperature ﬁelds. Furthermore, we are able to use low ﬁdelity models that are cheap to train and evaluate, to selectively avoid expensive high ﬁdelity function evaluations, as well as uncover seasonal variations in predictive power. This allows for an adaptive training strategy for computationally efﬁcient geophysical emulation. 1 Introduction While numerical weather forecasting dates back over a century [ 10], climate change is expected to signiﬁcantly alter the predictability of the atmosphere, increasing the error of weather forecasts [ 18]. Simultaneously, climate mitigation requires more accurate forecasts of weather events for power grid optimization [ 1], and extreme event prediction [ 20], from ﬂooding [ 9], to heatwaves [ 6]. Even the largest climate simulations have O(10km)separation between grid points, meaning dynamics of smaller scales can not be explicitly resolved [ 14,17,15], and often ad-hoc closure conditions are postulated to account for the unresolved dynamics. Machine learning has the potential to ﬁnd data-driven closure conditions [ 16] and parameterize sub-grid scale modeling [ 3,7]. Convolutional neural networks can forecast weather [ 4], deep learning can predict extreme weather events [ 5], and neural network architectures can be optimized automatically to enhance the quality of forecasts [ 11,2]. However, these models can become expensive to train and deploy, especially when ensemble predic- tions are required [ 8], and often their predictions lack interpretability. Here, we use a combination of proper orthogonal decomposition (POD), and long-short term memory (LSTM) reccurent neural networks to create forecasts for two real-world data sets. In addition to the LSTM network, which is our high-ﬁdelity (HF) model, we introduce simpler or low-ﬁdelity (LF) models, which do not have the same predictive power, but are faster to deploy. By studying the prediction differences of the LF models, we are able to selectively avoid HF function evaluations, as well as uncovering seasonal variations in prediction accuracy. With the proliferation of data driven methods for climate forecasting, the approach introduced here, could be used to reduce computational resources, which is of particular relevance for ensemble forecasting [ 8], as well as to identify uncertainties in neural network predictions. 2 Proper Orthogonal Decomposition To ﬁrst reduce the dimensionality of the problem, we project spatially resolved ﬁelds, such as temperature, onto a set of principal modes which capture the salient features and then track the evolution of these modes in time. We use the technique of Proper Orthogonal Decomposition (POD) Tackling Climate Change with Machine Learning workshop at NeurIPS 20200 90 180 270 360 Long itudeWeek 1Week 10Week 20 90 -90-4545 0Latitude0 8 24 16Temper ature (˚C) Time (w eeks)10 20 30 40 50400 200 0 -400-200Mode 3 Mode 2 Mode 1(a) (b) (c) 6 mode POD r econstruc tion Measured temperature /f_ieldModal coeﬃcientFigure 1: Proper orthogonal decomposition of sea surface temperature. (a) Historical climate data measuring ocean surface temperature weekly from 1981 to 2000. (b) Evolution of the ﬁrst 3 modal coefﬁcients, together with their respective basis elements. (c) Reconstructed temperature ﬁeld from ﬁrst the 6 modal coefﬁcients against measured temperature ﬁeld. in order to ﬁnd the dominant modes [ 21,12,13]. In short, suppose we are given spatial snapshots of the data at various times, \u00121; : : : ;\u0012T. We take a truncated set of orthonormal basis vectors v1; : : : ;vM, that approximates the spatial snapshots optimally with respect to the L2norm, i.e. minimizingPT ijj\u0012i\u0000\u001eijj2over\u001ei2Spanfv1; : : : ;vMg. Deﬁning the snapshot matrix, S= [\u00121j\u0001\u0001\u0001j\u0012T], the optimal basis is given by the Meigenvectors of SST, with largest eigenvalues [ 21], which is readily found numerically. See appendix A, for further details. 3 NOAA sea surface temperature Our ﬁrst data set is the NOAA Optimum Interpolation SST V2 data set, containing the sea surface temperature weekly on a 1 degree grid across the period 1981-20181. The ﬁrst 20 years were used as training data, the rest was reserved for testing. From this, we build data driven forecasts, predicting the next 5 weeks of temperature evolution from\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nLearning to accurately predict temperature fields is crucial. We can use low-fidelity models that are cheap to train and evaluate to selectively avoid expensive high-fidelity function evaluations and uncover seasonal variations in predictive power. This allows for an adaptive training strategy for computationally efficient geophysical emulation.\n\nIntroduction\n\nNumerical weather forecasting dates back over a century, but climate change is expected to significantly alter the predictability of the atmosphere, increasing the error of weather forecasts. Climate mitigation requires more accurate forecasts of weather events for power grid optimization and extreme event prediction, including flooding and heatwaves. Even the largest climate simulations have a grid point separation of O(10km), meaning dynamics of smaller scales cannot be explicitly resolved, and ad-hoc closure conditions are often postulated to account for the unresolved dynamics. Machine learning has the potential to find data-driven closure conditions and parameterize sub-grid scale modeling.\n\nConvolutional neural networks can forecast weather, deep learning can predict extreme weather events, and neural network architectures can be optimized automatically to enhance the quality of forecasts. However, these models can become expensive to train and deploy, especially when ensemble predictions are required, and often their predictions lack interpretability. We use a combination of proper orthogonal decomposition (POD) and long-short term memory (LSTM) recurrent neural networks to create forecasts for two real-world data sets. In addition to the LSTM network, which is our high-fidelity (HF) model, we introduce simpler or low-fidelity (LF) models that do not have the same predictive power but are faster to deploy.\n\nBy studying the prediction differences of the LF models, we can selectively avoid HF function evaluations and uncover seasonal variations in prediction accuracy. With the proliferation of data-driven methods for climate forecasting, the approach introduced here could be used to reduce computational resources, which is of particular relevance for ensemble forecasting, as well as to identify uncertainties in neural network predictions.\n\n2 Proper Orthogonal Decomposition\n\nTo reduce the dimensionality of the problem, we project spatially resolved fields, such as temperature, onto a set of principal modes that capture the salient features and then track the evolution of these modes in time. We use the technique of Proper Orthogonal Decomposition (POD) to find the dominant modes.\n\nSuppose we are given spatial snapshots of the data at various times. We take a truncated set of orthonormal basis vectors that approximates the spatial snapshots optimally with respect to the L2 norm. The optimal basis is given by the eigenvectors of the snapshot matrix, with largest eigenvalues. This is readily found numerically.\n\n3 NOAA Sea Surface Temperature\n\nOur first data set is the NOAA Optimum Interpolation SST V2 data set, containing the sea surface temperature weekly on a 1-degree grid across the period 1981-2018. The first 20 years were used as training data, and the rest was reserved for testing. From this, we build data-driven forecasts, predicting the next 5 weeks of temperature evolution."
}