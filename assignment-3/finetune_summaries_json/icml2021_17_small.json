{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"s from the ERA5 reanalysis. I demonstrate that the ensemble transformer can calibrate the ensemble spread and extract addi- tional information from the ensemble. As it is a member-by-member approach, the ensem- ble transformer directly outputs multivariate and spatially-coherent ensemble members. Therefore, self-attention and the transformer technique can be a missing piece for a non-parametric post- processing of ensemble data with neural net- works. 1. Introduction In Earth system modelling, an ensemble of simulations (Leith, 1974) is a Monte-Carlo approach to estimate uncer- tainties in weather predictions (Bauer et al., 2015; Molteni et al., 1996; Toth & Kalnay, 1993) or to assess forced re- sponse and internal variability in the Earth system (Deser et al., 2020; Kay et al., 2015; Maher et al., 2019). Every ensemble members is physically-consistent in their multi- variate structure. The ensemble can thus naturally represent non-linear evolutions and non-Gaussian distributed states as they appear in nature. Nevertheless, weather and cli- 1Meteorological Institute, University of Hamburg, Hamburg, Germany2International Max Planck Research School on Earth System Modelling, Max Planck Institute for Meteorology, Ham- burg, Germany. Correspondence to: Tobias Sebastian Finn <tobias.sebastian.ﬁnn@uni-hamburg.de >. Tackling Climate Change with Machine Learning workshop at ICML 2021.mate ensembles have to be post-processed (Hemri et al., 2014; Steininger et al., 2020) by model output statistics to correct model biases, calibrate the ensemble, and predict variables that are not modelled by the Earth system model. Often, post-processing targets summarized ensemble statis- tics (Schulz & Lerch, 2021), predicting either the param- eters (Gneiting et al., 2005; Raftery et al., 2005; Rasp & Lerch, 2018) or the cumulative distribution function (Baran & Lerch, 2018; Bremnes, 2020; Scheuerer et al., 2020; Tail- lardat et al., 2016) of the target distribution. As a conse- quence, the member-wise multivariate and spatial-coherent representation of the ensemble forecast is lost. By contrast, I propose a member-by-member post-processing approach (Schaeybroeck & Vannitsem, 2015) with neural networks and a self-attentive ensemble transformer that keeps the spatial correlation structure within the ensemble intact. To calibrate the ensemble, ensemble members have to be informed about the evolution of other ensemble members. The necessary term to represent the ensemble interactions is missing in neural networks that are applied on each ensem- ble member independently. As a consequence, this direct neural network approach leads to a loss of information and to problems with tuning of the ensemble spread. Ensemble Kalman ﬁlters (Bishop et al., 2001; Burgers et al., 1998; Evensen, 1994) include the dynamics between ensem- ble members by using the predicted ensemble covariances in a linear update step to assimilate given observations into ensemble predictions. In their core approach, ensemble Kalman ﬁlters are similar to (self-)attention modules (Lu- ong et al., 2015; Vaswani et al., 2017; Wang et al., 2018) for neural networks, despite having another terminology: the value in attention modules or the state in ensemble Kalman ﬁlters is modiﬁed based on weights estimated with keys (the sensitivity in ensemble Kalman ﬁlters) and queries (obser- vations). In self-attention modules, the keys and queries are projections of the same input that is also used to project the values. The module literally informs itself about the searched information. I bridge the ideas of the ensemble Kalman ﬁlters and self- attention. I introduce the self-attentive ensemble transformer for processing of ensemble data as neural network archi- tecture by stacking multiple self-attention modules. EachEnsemble Transformer in Neural Networks for Earth System Models (a) Schematic overview of the ensemble transformer architecture. The separated side of the ﬁgure is a zoom-in to a single module. (b) Two selected attention maps (layer number, head number) from the Transformer (5) experiment for 2019-09-01 12:00 UTC. Red (blue) colours indicate a high (low) inﬂuence on the attention. Figure 1. Schematic overview of the self-attentive ensemble transformer architecture and two exemplary attention maps. module adds to the static value for each ensemble member a dynamic self\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nThe ensemble transformer can calibrate the ensemble spread and extract additional information from the ensemble. As it is a member-by-member approach, the ensemble transformer directly outputs multivariate and spatially-coherent ensemble members. Therefore, self-attention and the transformer technique can be a missing piece for non-parametric post-processing of ensemble data with neural networks.\n\nIn Earth system modeling, an ensemble of simulations is a Monte-Carlo approach to estimate uncertainties in weather predictions or to assess forced response and internal variability in the Earth system. Every ensemble member is physically consistent in its multivariate structure. The ensemble can naturally represent non-linear evolutions and non-Gaussian distributed states as they appear in nature. However, weather and climate ensembles need to be post-processed to correct model biases, calibrate the ensemble, and predict variables not modeled by the Earth system model.\n\nPost-processing often targets summarized ensemble statistics, predicting either parameters or the cumulative distribution function of the target distribution. As a result, the member-wise multivariate and spatial-coherent representation of the ensemble forecast is lost. I propose a member-by-member post-processing approach with neural networks and a self-attentive ensemble transformer that keeps the spatial correlation structure within the ensemble intact.\n\nTo calibrate the ensemble, ensemble members need to be informed about the evolution of other ensemble members. The necessary term to represent ensemble interactions is missing in neural networks that are applied to each ensemble member independently. This direct neural network approach leads to a loss of information and problems with tuning the ensemble spread. Ensemble Kalman filters include the dynamics between ensemble members by using predicted ensemble covariances in a linear update step to assimilate given observations into ensemble predictions.\n\nEnsemble Kalman filters are similar to self-attention modules for neural networks, despite having different terminology. In self-attention modules, the value is modified based on weights estimated with keys and queries. I bridge the ideas of ensemble Kalman filters and self-attention by introducing the self-attentive ensemble transformer for processing ensemble data as a neural network architecture by stacking multiple self-attention modules.\n\nEach module adds a dynamic self-attention component to the static value for each ensemble member. The self-attentive ensemble transformer architecture is shown in Figure 1. The figure includes a schematic overview of the architecture and two exemplary attention maps.\n\nFigure 1: Schematic overview of the self-attentive ensemble transformer architecture and two exemplary attention maps.\n\nNote: I removed the following noisy characters and text:\n\n- \"s\" from the ERA5 reanalysis (likely a typo or a reference that was not relevant to the main text)\n- Footnotes and corresponding references\n- Unnecessary words and phrases (e.g., \"I demonstrate that\", \"As it is a member-by-member approach\")\n- Unnecessary punctuation and formatting\n- Unnecessary words and phrases (e.g., \"Tackling Climate Change with Machine Learning workshop at ICML 2021\")\n- Unnecessary characters (e.g., \"1Meteorological Institute, University of Hamburg, Hamburg, Germany2International Max Planck Research School on Earth System Modelling, Max Planck Institute for Meteorology, Ham- burg, Germany. Correspondence to: Tobias Sebastian Finn <tobias.sebastian.ﬁnn@uni-hamburg.de >\")\n- Unnecessary text (e.g., \"a missing piece for a non-parametric post- processing of ensemble data with neural net- works\")"
}