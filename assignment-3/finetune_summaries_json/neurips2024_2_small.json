{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\".ghorbanpour, avisek.naug, desik.rengarajan, ashwin.ramesh-babu, paolo.faraboschi, soumyendu.sarkar} @hpe.com {reja, vgopalas, betti} @lle.rochester.edu Abstract We introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), which integrates Meta-Learning and expert preferences to enhance BO. MBO-HF employs Transformer Neural Processes (TNPs) to create a meta-learned surrogate model and a human-informed acquisition function (AF) to suggest and explain pro- posed candidate experiments. MBO-HF outperforms current methods in optimizing various scientific experiments and benchmarks in simulation, including the energy yield of the inertial confinement fusion (ICF), practical molecular optimization (PMO), and critical temperature maximization for superconducting materials. 1 Introduction Due to the high costs and limited experimentation opportunities of some scientific applications, efficient optimization techniques incorporating expert knowledge are crucial. Bayesian Optimization (BO) has shown potential in optimizing expensive black-box functions (Feurer et al., 2015; Snoek et al., 2012; Wang et al., 2023; Shmakov et al., 2023; Gundecha et al., 2024; Ghorbanpour et al., 2024) but faces challenges in real-world applications due to trustworthiness concerns (Bouthillier and Varoquaux, 2020). Recent advancements in human-AI collaborative approaches have improved BO by incorporating expert input (Colella et al., 2020; A V et al., 2022; Gupta et al., 2023; Hvarfner et al., 2022; Adachi et al., 2024), though these methods typically focus on single-task scenarios, failing to leverage knowledge from previous tasks. Meta-Learning within the BO framework (Meta-BO) (V olpp et al., 2020) addresses sample efficiency by enabling models to adapt quickly to new tasks using knowledge from related tasks, enhancing sample efficiency. However, trustworthiness and explainability, critical for the adoption of black-box optimization by scientists, are under-explored in Meta-BO. Moreover, previous Meta-BO studies have not investigated the impact of integrating expert knowledge through methods like preference learning on optimization performance. To address this, we introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), a framework that integrates expert knowledge into Meta-BO using Transformer Neural Processes (TNPs) to build an adaptable surrogate model. MBO-HF provides an explainable system that helps experts evaluate optimization recommendations, improving both ∗Corresponding author. †These authors contributed equally. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2024.Expert  Meta  Surrogate Makes binary choices from  pairs of points to initialize  preference model  Suggest areas of high  likelihood to contain the  optima  Trains On Source  Tasks Evaluation Output Scientific  Experiments -$  ob  C:  0 ·-.....,  MBO-HF adapts  optimization I Provides Explanations I C.  0 --\"C  P\"\"\"'I Q)  C\\I... to new feedback  I Proposes Candidates I  I [o.s, 0.21 I  0 0 Meta Surrogate + Expert Preferences Candidate  ----• __ M_e_t_a_s_u_r_r_o.. ga_t_e_c_a_n_d_i_d_a~t_e~~~~~~~~--... •  MBO-HF....  0~ .. Q)  Ll)...  ca.......... __ u,  Q) u,  0  0.c:  (..)  I [o.a, o.s1 I Expert Figure 1: MBO-HF combines meta-learning and experts’ knowledge for a highly efficient optimization process, with the expert-in-the-loop. the effectiveness and the trustworthiness of the optimization process, as shown in Figure 1. We summarize our main contributions as follows: •We introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), an explainable framework that enhances Meta-BO by incorporating human expertise. We demonstrate that MBO-HF outperforms SOTA in both human-aided BO and standard Meta-BO. •We present a novel expert-informed acquisition function (AF) that integrates preference learning with expert hypotheses, and evaluate the impact of human expertise on optimization. •We implement an explainable framework for Meta-BO, utilizing Shapley values (Shapley, 1952) to provide the human-in-the-loop with insights into the optimization decisions. 2 MBO-HF Unlike single-task approaches, MBO-HF trains a meta-surrogate using Transformer Neural Processes (TNPs) on prior tasks and incorporates expert preferences to optimize the target function effectively. 2.1 Preference Learning To fit the\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here is the rephrased text with the noisy characters and data removed:\n\nGhorbanpour, Avisek Naug, Desik Rengarajan, Ashwin Ramesh-Babu, Paolo Faraboschi, Soumyendu Sarkar}@hpe.com, Reja, V. Gopalas, Betti}@lle.rochester.edu\n\nAbstract:\nWe introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), which integrates Meta-Learning and expert preferences to enhance Bayesian Optimization (BO). MBO-HF employs Transformer Neural Processes (TNPs) to create a meta-learned surrogate model and a human-informed acquisition function (AF) to suggest and explain proposed candidate experiments. MBO-HF outperforms current methods in optimizing various scientific experiments and benchmarks in simulation, including the energy yield of inertial confinement fusion (ICF), practical molecular optimization (PMO), and critical temperature maximization for superconducting materials.\n\n1 Introduction\nDue to the high costs and limited experimentation opportunities of some scientific applications, efficient optimization techniques incorporating expert knowledge are crucial. Bayesian Optimization (BO) has shown potential in optimizing expensive black-box functions (Feurer et al., 2015; Snoek et al., 2012; Wang et al., 2023; Shmakov et al., 2023; Gundecha et al., 2024; Ghorbanpour et al., 2024) but faces challenges in real-world applications due to trustworthiness concerns (Bouthillier and Varoquaux, 2020). Recent advancements in human-AI collaborative approaches have improved BO by incorporating expert input (Colella et al., 2020; Gupta et al., 2023; Hvarfner et al., 2022; Adachi et al., 2024), though these methods typically focus on single-task scenarios, failing to leverage knowledge from previous tasks. Meta-Learning within the BO framework (Meta-BO) (V olpp et al., 2020) addresses sample efficiency by enabling models to adapt quickly to new tasks using knowledge from related tasks, enhancing sample efficiency. However, trustworthiness and explainability, critical for the adoption of black-box optimization by scientists, are under-explored in Meta-BO. Moreover, previous Meta-BO studies have not investigated the impact of integrating expert knowledge through methods like preference learning on optimization performance.\n\nTo address this, we introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), a framework that integrates expert knowledge into Meta-BO using Transformer Neural Processes (TNPs) to build an adaptable surrogate model. MBO-HF provides an explainable system that helps experts evaluate optimization recommendations, improving both effectiveness and trustworthiness of the optimization process.\n\nWe summarize our main contributions as follows:\n\n• We introduce Meta Bayesian Optimization with Human Feedback (MBO-HF), an explainable framework that enhances Meta-BO by incorporating human expertise. We demonstrate that MBO-HF outperforms state-of-the-art in both human-aided BO and standard Meta-BO.\n• We present a novel expert-informed acquisition function (AF) that integrates preference learning with expert hypotheses, and evaluate the impact of human expertise on optimization.\n• We implement an explainable framework for Meta-BO, utilizing Shapley values (Shapley, 1952) to provide the human-in-the-loop with insights into the optimization decisions.\n\n2 MBO-HF\nUnlike single-task approaches, MBO-HF trains a meta-surrogate using Transformer Neural Processes (TNPs) on prior tasks and incorporates expert preferences to optimize the target function effectively.\n\n2.1 Preference Learning\nTo fit the expert's preferences, we use a preference learning approach that makes binary choices from pairs of points to initialize the preference model. This approach suggests areas of high likelihood to contain the optima and trains on source tasks. The expert-in-the-loop evaluates the optimization recommendations, and MBO-HF adapts to new feedback, providing explanations and proposing candidates."
}