{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" Bouabid Dpt. of Statistics University of Oxford shahine.bouabid@stats.ox.ac.uk James Fulton School of Geosciences University of Edinburgh james.fulton@ed.ac.ukDánnell Quesada-Chacón Institute of Hydrology and Meteorology Dresden University of Technology dannell.quesada@tu-dresden.de Aris Marcolongo Mathematical Institute University of Bern Climate and Environmental Physics University of Bern aris.marcolongo@math.unibe.chSoﬁja Stefanovi ´c Dpt. of Atmospheric, Oceanic and Planetary Physics University of Oxford sofija.stefanovic@physics.ox.ac.uk Yuhan Rao Institute for Climate Studies North Carolina State University yrao5@ncsu.eduPeter Manshausen Dpt. of Atmospheric, Oceanic and Planetary Physics University of Oxford peter.manshausen@physics.ox.ac.uk Duncan Watson-Parris Dpt. of Atmospheric, Oceanic and Planetary Physics University of Oxford duncan.watson-parris@physics.ox.ac.uk Abstract The recent explosion in applications of machine learning to satellite imagery often rely on visible images and therefore suffer from a lack of data during the night. The gap can be ﬁlled by employing available infra-red observations to generate visible images. This work presents how deep learning can be applied successfully to create those images by using U-Net based architectures. The Tackling Climate Change with Machine Learning workshop at NeurIPS 2020.proposed methods show promising results, achieving a structural similarity index (SSIM) up to 86% on an independent test set and providing visually convincing output images, generated from infra-red observations. The code is available at: https://github.com/paulaharder/hackathon-ci-2020 1 Introduction The availability of huge amounts of open, high-quality satellite imagery from e.g. Sentinel-2 and GOES-16 has enabled many machine learning (ML) applications as diverse as the detection of Penguins [Fretwell and Trathan, 2020], solar-panels [Hou et al., 2019] and ship-tracks [Watson-Parris et al., 2019]. Many of these applications rely on visible imagery because of the prevalence of pre-existing models and easier processing and validation. This visible imagery relies on the detection of reﬂected sunlight during the day, whereas instruments such as the Advanced Baseline Imager (ABI) on-board GOES-16 are also able to measure infra-red emission throughout the day and night. These different spectra contain different, but often complementary information, and in principle ML models could be trained to use either or both depending on their availability. In practice, having access to homogenised imagery allows a single ML model to be trained to, for example, detect and track clouds 24 hours a day. This capability would transform our ability to detect the subtle, but important, perturbations humans are exerting on the climate system [Stevens and Feingold, 2009]. Here we present the result of a three-day hackathon which challenged contestants to generate visible (RGB) images using only the infra-red imagery available at night. We describe the challenge and introduce the publicly available training datasets in Section 2, present the three winning models and their notable features in Section 3, and discuss avenues of future work in Section 4. We are not aware of any work on generating visible satellite imagery from infra-red observation. In [Berg et al., 2018] thermal infra-red observations are used to generate visible spectrum images for trafﬁc scene datasets, also by employing convolutional neural networks. 2 Data Preparation Data is acquired by the ABI aboard the Geostationary Operational Environment Satellite (GOES)-16 [Schmit et al., 2008]. This is a modern Earth Observation (EO) platform placed in a geostationary orbit, allowing it to provide visible and IR imagery every ten minutes. Channels 8-16 of infra-red (IR) channels from the GOES-16 ABI instrument are used to create the inputs of our algorithm, while channels 1-6, which detect reﬂected solar radiation, are used to create the visible target outputs. Additional information about the channels used by ABI, and what physical properties they measure, can be found here1. For ease of processing we transform these raw channel radiances into RGB composite images using SatPy software [Raspaud et al., 2020] for both the IR channels (model input) and visible (target outut). The considered region is -85 to -70 degrees longitude and -15 to -30 degrees latitude, and to reduce the size of the data we downsample the images to a size of 127\u0002127pixels. For the competition, 2 years of data are provided as training dataset from January 1st2018 to December 15th2019, the two last weeks of December are thrown out to avoid data leakage between train and test datasets.\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nBouabid, Department of Statistics, University of Oxford, shahine.bouabid@stats.ox.ac.uk\nJames Fulton, School of Geosciences, University of Edinburgh, james.fulton@ed.ac.uk\nDánnell Quesada-Chacón, Institute of Hydrology and Meteorology, Dresden University of Technology, dannell.quesada@tu-dresden.de\nAris Marcolongo, Mathematical Institute, University of Bern, Climate and Environmental Physics, University of Bern, aris.marcolongo@math.unibe.ch\nSoﬁja Stefanović, Department of Atmospheric, Oceanic and Planetary Physics, University of Oxford, sofija.stefanovic@physics.ox.ac.uk\nYuhan Rao, Institute for Climate Studies, North Carolina State University, yrao5@ncsu.edu\nPeter Manshausen, Department of Atmospheric, Oceanic and Planetary Physics, University of Oxford, peter.manshausen@physics.ox.ac.uk\nDuncan Watson-Parris, Department of Atmospheric, Oceanic and Planetary Physics, University of Oxford, duncan.watson-parris@physics.ox.ac.uk\n\nAbstract\nThe recent surge in machine learning applications using satellite imagery often relies on visible images, which are limited by the lack of data during nighttime. This gap can be filled by utilizing available infrared observations to generate visible images. This work demonstrates the successful application of deep learning to create such images using U-Net-based architectures. The methods proposed at the Tackling Climate Change with Machine Learning workshop at NeurIPS 2020 show promising results, achieving a structural similarity index (SSIM) of up to 86% on an independent test set and producing visually convincing output images from infrared observations. The code is available at: https://github.com/paulaharder/hackathon-ci-2020\n\nIntroduction\nThe availability of large amounts of open, high-quality satellite imagery from sources like Sentinel-2 and GOES-16 has enabled various machine learning applications, such as detecting penguins [Fretwell and Trathan, 2020], solar panels [Hou et al., 2019], and ship tracks [Watson-Parris et al., 2019]. Many of these applications rely on visible imagery due to the prevalence of pre-existing models and easier processing and validation. However, visible imagery relies on reflected sunlight during the day, whereas instruments like the Advanced Baseline Imager (ABI) on GOES-16 can measure infrared emission throughout the day and night. These different spectra contain different, but often complementary, information, and in principle, machine learning models could be trained to use either or both depending on their availability. In practice, having access to homogenized imagery allows a single model to be trained to detect and track clouds 24 hours a day. This capability would significantly enhance our ability to detect the subtle, but important, perturbations humans are exerting on the climate system [Stevens and Feingold, 2009].\n\nHere, we present the results of a three-day hackathon that challenged contestants to generate visible (RGB) images using only infrared imagery available at night. We describe the challenge and introduce the publicly available training datasets in Section 2, present the three winning models and their notable features in Section 3, and discuss avenues of future work in Section 4. To the best of our knowledge, there is no existing work on generating visible satellite imagery from infrared observations. However, [Berg et al., 2018] used thermal infrared observations to generate visible spectrum images for traffic scene datasets, employing convolutional neural networks.\n\n2 Data Preparation\nData is acquired by the ABI aboard the Geostationary Operational Environmental Satellite (GOES)-16 [Schmit et al., 2008]. This is a modern Earth Observation platform placed in a geostationary orbit, allowing it to provide visible and IR imagery every ten minutes. Channels 8-16 of infrared (IR) channels from the GOES-16 ABI instrument are used to create the inputs of our algorithm, while channels 1-6, which detect reflected solar radiation, are used to create the visible target outputs. Additional information about the channels used by ABI and the physical properties they measure can be found here. For ease of processing, we transform these raw channel radiances into RGB composite images using SatPy software [Raspaud et al., 2020] for both the IR channels (model input) and visible (target output). The considered region is -85 to -70 degrees longitude and -15 to -30 degrees latitude, and to reduce the size of the data, we downsample the images to a size of 127x127 pixels. For the competition, 2 years of data are provided as a training dataset from January 1"
}