{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" probabilistic interpretation of the predictions, while also capturing the stochasticity inherent in the relationships among fine and coarse spatial scales. We showcase its successful performance on an ERA5 water content dataset for different upsampling factors. Additionally, we show that the method allows us to assess the predictive uncertainty in terms of standard deviation from the fitted conditional distribution mean. 1 Introduction In climate modeling, simulations are typically run at coarse spatial resolution due to computational constraints. However, it is often of interest to obtain accurate predictions about the earth’s climate not only on global but also on local scales, for example to guide local adaptation to precipitation or temperature trends. To fill this gap, statistical downscaling methods have increasingly been used to derive high-resolution information from low-resolution input. Early works have used Convolutional Neural Networks (CNNs) for climate variable downscaling [2, 10, 7, 11, 4, 9, 3]. However, these purely deterministic methods fail to capture the ill-determined nature of the problem – for the same low-resolution image, there exist many possible fine-scale realizations. Capturing such stochasticity is important in order to improve the accuracy of local scale predictions. Therefore, Generative Adversarial Networks (GANs) have become a widely used method in super-resolution and climate variable downscaling [ Chen_undated-ck, 14, 15, 5, 12, 6, 15, 1]. However, such methods lack latent-space encoders and are known to suffer from mode collapse. It is hard to assess whether they are overfitting or generalizing. In climate variable downscaling, we require estimating a density as close to the true high-resolution pixel distribution as possible, as high-frequency details are of main importance. In this field, recently exact likelihood methods such as diffusion models have been applied [13] for climate variable downscaling by leveraging theory from optimal transport. First, the data is debiased and then a diffusion model is used for upsampling. In [5], the authors use normalizing flows for aligning the latent variables to a reference representation after performing statistical downscaling using a GAN architecture. Current state-of-the-art work [17] uses Fourier neural operators to learn a mapping between high and low-resolution climate data for arbitrary resolution downscaling. In this work, we introduce the use of Conditional Normalizing Flows [16] (CNFs) for stochastic climate variable downscaling. They are particularly desirable, since we can tractably compute likelihood values, their sampling procedure is efficient, and we are able to assess predictive uncertainty due to its probabilistic interpretation. Unlike other generative models where predictive uncertainty is Preprint. Under review.often computed over an ensemble of different runs of weight initializations, or using techniques such as dropout, we are able to directly evaluate the predictive uncertainty of the CNF by computing the standard deviation from the fitted distribution mean. Contributions Our main contributions can be summarized as follows: •We show for the first time how to apply conditional normalizing flows to the task of climate variable downscaling. •We verify that CNF makes it possible to evaluate predictive uncertainty, by computing uncertainty maps from the standard deviation of sampled outputs. 2 Background Normalizing flows represent a function as the composition of simpler invertible functions f(z) = fK◦fK−1◦ ··· ◦ f1(z)which yield the transformed random variables zK←...←z1←z0as intermediates after applying the transformations f1through fk. The functions fk:Rd7−→Rdare defined such that f(z0) =ywithy∈Rd. All transformations fkare invertible and differentiable, making it possible to computing the Jacobian determinant. Then, by applying the Change of Variables Formula we can model the density: py(y) = pz(f(y))\f\f\fdet∂f(y) ∂y\f\f\f (1) where yis our input data at training time mapping to latent variable z. This allows us to formulate a model for the marginal likelihood py(y)that can be computed tractably and optimized on the negative log-likelihood. We propose to learn conditional likelihoods using conditional normalizing flows [16] for the task of super-resolution on climate data. Take as input the low-resolution image x∈ X and as target the high-resolution image y∈ Y. We learn a distribution pY|X(y|x)using a conditional prior pZ|X(z|x)and a mapping fϕ:Y × X → Z, which is bijective in YandZ. The likelihood of this model is then defined as: pY|X(y|x) =pZ|X(z|x)\f\f\f\f∂z ∂y\f\f\f\f=pZ|X(fϕ(y,x)|x)\f\f\f\f∂fϕ(y,x) ∂y\f\f\f\f. (2) Notice that the difference between Equations\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's a rephrased version of the text, removing noisy characters and formatting issues while preserving the content:\n\nA Probabilistic Approach to Climate Variable Downscaling\n\nClimate modeling simulations are typically run at coarse spatial resolution due to computational constraints. However, accurate predictions about the Earth's climate on both global and local scales are often required to guide local adaptation to precipitation or temperature trends. To address this gap, statistical downscaling methods have been increasingly used to derive high-resolution information from low-resolution input. Early works have employed Convolutional Neural Networks (CNNs) for climate variable downscaling, but these deterministic methods fail to capture the inherent stochasticity of the problem. For the same low-resolution image, there exist many possible fine-scale realizations, making it essential to capture such stochasticity to improve the accuracy of local scale predictions.\n\nGenerative Adversarial Networks (GANs) have been widely used in super-resolution and climate variable downscaling, but they lack latent-space encoders and are prone to mode collapse. Assessing whether they are overfitting or generalizing is challenging. In climate variable downscaling, estimating a density close to the true high-resolution pixel distribution is crucial, as high-frequency details are of primary importance. Recent exact likelihood methods, such as diffusion models, have been applied to climate variable downscaling by leveraging theory from optimal transport. These methods involve debiasing the data and then using a diffusion model for upsampling.\n\nIn this work, we introduce the use of Conditional Normalizing Flows (CNFs) for stochastic climate variable downscaling. CNFs are particularly desirable due to their ability to tractably compute likelihood values, efficient sampling procedure, and assessment of predictive uncertainty through their probabilistic interpretation. Unlike other generative models, predictive uncertainty can be directly evaluated by computing the standard deviation from the fitted distribution mean.\n\nContributions:\n\n* We demonstrate the application of conditional normalizing flows to climate variable downscaling for the first time.\n* We show that CNFs enable the evaluation of predictive uncertainty by computing uncertainty maps from the standard deviation of sampled outputs.\n\nBackground\n\nNormalizing flows represent a function as the composition of simpler invertible functions, which yield transformed random variables as intermediates. The functions are defined such that they are invertible and differentiable, making it possible to compute the Jacobian determinant. By applying the Change of Variables Formula, we can model the density of the input data. This allows us to formulate a model for the marginal likelihood that can be computed tractably and optimized on the negative log-likelihood.\n\nWe propose to learn conditional likelihoods using conditional normalizing flows for the task of super-resolution on climate data. Given a low-resolution image x and a high-resolution image y, we learn a distribution pY|X(y|x) using a conditional prior pZ|X(z|x) and a bijective mapping fϕ: Y × X → Z. The likelihood of this model is defined as:\n\npY|X(y|x) = pZ|X(z|x) ∂z ∂y\n\nThis equation shows the difference between the likelihood of the model and the marginal likelihood of the input data."
}