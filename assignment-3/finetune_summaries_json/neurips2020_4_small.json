{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" deep reinforcement learning (DRL) approach to address this task. We build a recurrent neural network, trained on simulated data, to predict the indoor temperatures. This model is then used to train two DRL agents, with or without expert guidance, for the optimal control of the supply water temperature. Our tests in a multi-apartment setting show that both agents can ensure a higher thermal comfort and at the same time a smaller energy cost, compared to an optimized baseline strategy. 1 Introduction Space heating and cooling in buildings is well-known for representing a signiﬁcant part of global CO 2emissions. For instance, district heating alone in China consumes more energy than the entire United Kingdom [1]. As of today, operating such industrial heating networks is prone to energy losses due to complex nonlinear building thermal behaviours. Hence signiﬁcant efﬁciency gains are possible resulting in a clear line of work to ﬁght climate change: develop and implement advanced control strategies for an optimal operation. This is all the more relevant in China where 87% of heat production is either from coal or oil [2]. In a typical Chinese district heating system, heat is produced in a central location and conveyed towards substations and onwards for distribution to customers via a heat exchanger and a network of insulated pipes. The distribution network is organized in a feed-and-return line and contains two parts, the primary and secondary networks as shown in Figure 1. The secondary network contains temperature sensors Ts, for the ﬂuid exiting the substation towards the feed line, and Trfor the ﬂuid entering the heat exchanger back from the return line. As is the norm in China and unlike some contributions on optimal district heating such as [3 –5], we assume that the network operator buys heat at a constant price and from a unique third-party producer. We focus thus on the control of the secondary indoor temperatures: heat must be delivered to each apartment on the feed line to ensure that every indoor temperature is within an admissible range. In particular, any temperature above the upper bound results in both a waste of energy and an economic loss for the utility - most often, heating fees in China are a ﬂat cost per square meter, regardless of the actual energy consumption. However, it is not possible to individually control the thermal behaviour of every apartment. Instead, an operator should control the indoor temperatures with two commands located inside the substation: (i) the supply temperature Ts, by acting on a control valve on the primary side, and (ii) the ﬂow rate of the ﬂuid, thanks to a pump at the inlet bound of the secondary side of the heat exchanger. The state-of-the-art industrial control strategies rely on a relationship, called the water curve, which is tuned by an expert, between Tsand the outdoor temperature To. This paper investigates how to improve this strategy to ensure both higher thermal comfort and smaller energy and operation cost. 34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.\u0001\u0001\u0001 \u0001\u0001\u0001\u0001\u0001\u0001 \u0001\u0001\u0001towards heat generation plant\u000f \u000fTs Tr Heat Exchanger_mPrimary Secondary \u000f\u000f\u000f \u000f\u000f\u000f \u000f\u000f\u000f \u000f\u000f\u000f Figure 1: A district heating network. 2 Approach 2.1 Control strategy We apply a Reinforcement Learning (RL) paradigm [6], where an agent learns a control strategy (policy ) by interacting with the environment - here, the set of rooms heated by the network. The problem is modelled as a Markov Decision Process: the agent receives an observation of the state of the environment, chooses an action and receives as a result a reward from the environment. The best control strategy maximizes the expected cumulative discounted reward over the lifetime of the agent. Learning such a policy requires ﬁrst to derive a model of the environment, to predict the indoor temperatures from the commands and the weather conditions. This model is described in Section 2.2. At timet, the state is a vector stcontaining the outdoor temperature To;t, supply water temperature Ts;t, time of the day and indoor temperatures T(j) in;tfor every room j2f1;:::;Ngin the network. stcontains both present and past nmeasurements of these quantities. At an hourly time step, a history of 24 hours is used to form st. At that same hourly time step, the agent is asked to select an actionat. The ﬂow rate being kept constant, the action is restricted to the supply temperature Ts;t. Two discrete action spaces, with Ts(\u000eC)2f20;21;:::; 50g, are considered. Agent 1 is the standard strategy while Agent 2 is a ﬁnetuning of the baseline control strategy (cf Section 2.3): 1.Agent 1: to enforce the smoothness of the control signal, the action is limited to the incrementsat=Ts;t\u0000Ts;t\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nDeep reinforcement learning (DRL) is used to address the task of controlling indoor temperatures in a district heating system. A recurrent neural network is trained on simulated data to predict indoor temperatures. This model is then used to train two DRL agents, with or without expert guidance, for optimal control of the supply water temperature. Tests in a multi-apartment setting show that both agents can ensure higher thermal comfort and smaller energy costs compared to an optimized baseline strategy.\n\nIntroduction\nSpace heating and cooling in buildings is a significant contributor to global CO2 emissions. For instance, district heating in China consumes more energy than the entire United Kingdom. Operating industrial heating networks is prone to energy losses due to complex nonlinear building thermal behaviors. Significant efficiency gains are possible, resulting in a clear line of work to fight climate change: develop and implement advanced control strategies for optimal operation. This is particularly relevant in China, where 87% of heat production is from coal or oil.\n\nIn a typical Chinese district heating system, heat is produced in a central location and conveyed to substations and onwards for distribution to customers via a heat exchanger and a network of insulated pipes. The distribution network is organized in a feed-and-return line and contains two parts: the primary and secondary networks. The secondary network contains temperature sensors for the fluid exiting the substation towards the feed line and the fluid entering the heat exchanger back from the return line.\n\nUnlike some contributions on optimal district heating, we assume that the network operator buys heat at a constant price from a unique third-party producer. We focus on the control of the secondary indoor temperatures: heat must be delivered to each apartment on the feed line to ensure that every indoor temperature is within an admissible range. Any temperature above the upper bound results in both a waste of energy and an economic loss for the utility.\n\nHowever, it is not possible to individually control the thermal behavior of every apartment. Instead, an operator should control the indoor temperatures with two commands located inside the substation: (i) the supply temperature, by acting on a control valve on the primary side, and (ii) the flow rate of the fluid, thanks to a pump at the inlet bound of the secondary side of the heat exchanger.\n\nThe state-of-the-art industrial control strategies rely on a relationship, called the water curve, which is tuned by an expert, between the supply temperature and the outdoor temperature. This paper investigates how to improve this strategy to ensure both higher thermal comfort and smaller energy and operation costs.\n\nApproach\nWe apply a reinforcement learning paradigm, where an agent learns a control strategy (policy) by interacting with the environment - here, the set of rooms heated by the network. The problem is modeled as a Markov Decision Process: the agent receives an observation of the state of the environment, chooses an action, and receives a reward from the environment. The best control strategy maximizes the expected cumulative discounted reward over the lifetime of the agent.\n\nLearning such a policy requires first to derive a model of the environment, to predict the indoor temperatures from the commands and the weather conditions. This model is described in Section 2.2. At time t, the state is a vector containing the outdoor temperature, supply water temperature, time of the day, and indoor temperatures in each room in the network. The state contains both present and past measurements of these quantities.\n\nAt an hourly time step, a history of 24 hours is used to form the state. At that same hourly time step, the agent is asked to select an action. The flow rate is kept constant, and the action is restricted to the supply temperature. Two discrete action spaces are considered: one for the standard strategy and one for a fine-tuning of the baseline control strategy (cf Section 2.3).\n\nAgent 1 is the standard strategy, which enforces the smoothness of the control signal by limiting the action to the increment of the supply temperature. Agent 2 is a fine-tuning of the baseline control strategy."
}