{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" creasing land usage, greenhouse gas emissions, and water consumption. However, breeding programs are challenging due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change. This paper introduces the use of Reinforcement Learning (RL) to optimize simu- lated crop breeding programs. RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information. To benchmark RL- based breeding algorithms, we introduce a suite of Gym environments. The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data. 1 Introduction Crop breeding programs aim to create new cultivars with desired traits by controlled mating among individuals in a population ( 1). Crop breeding played a vital role in the Green Revolution that took place in the 20th century, which resulted in a threefold increase in plant-based food production within 50 years, while only raising land usage by 30% ( 2). However, current growth trends do not meet the forecasted 2050 demand. Moreover, evidence shows that yields for staple crops are plateauing or degrading mainly due to climate change and soil degradation (3). The decrease in genotyping cost has produced abundant crop genomic data, unveiling novel oppor- tunities to adapt crop genetics for these adverse conditions. Recent advancements have focused on predicting phenotype traits from genomic data for quicker and cheaper crop selection without expen- sive and slow trials ( 4;5;6). However, relying solely on estimated traits can lead to low-diversity pools and compromise long-term breeding program success. Data-driven methods for decision-making can fill this gap and allow breeding programs to retain genetic diversity and adapt to long-term objectives, e.g., climate adaptation. To use these methods to optimize crop breeding, we can frame a breeding program as a sequential decision-making problem where, at each breeding generation, the breeder takes some actions, e.g., which and how many plants to select, and how to cross them. Crossing two plants produces offspring with a mixture of genes from the two parents. The process is repeated for several generations to deploy a cultivar with desired traits. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.From this perspective, we can optimize design choices of breeding programs with Reinforcement Learning (RL) (7), a paradigm to address sequential decision-making problems. Contributions: In this paper, we develop a framework to optimize the design choices of breeding programs, with three main contributions. (1) In Section 2, we model a breeding program as a Markov Decision Process (MDP), allowing us to leverage techniques from the RL literature to adaptively optimize the design choices. (2) For this scope, we present in Section 4 a package containing multiple RL environments that simulate breeding programs with different sets of actions and observations. (3) In Section 5, we show the effectiveness of the approach in selecting plants based on a learned score; this approach outperforms the standard genomic selection based on the estimated traits. Please note that the results presented are based on simulations and may differ in real-world applications. Related work: Machine learning models have been applied in breeding programs in various ways (8), however, often for trait prediction in genomic assisted breeding ( 9;10). To the best of our knowledge, RL has been only used in the context of breeding in ( 11), where the authors optimize the population size at every step (generation) of the breeding program, while other decisions (e.g., genomic prediction function) are considered fixed. 2 Problem definition In reinforcement learning, sequential decision-making problems are formalized as Markov decision processes (MDPs) ( 12), i.e. a tuple (S, ρ,A,P,R, γ), where: Sis the set of all possible states; ρ is the probability distribution of the initial state s0;A:S→Amaps each state sto the possible actions in s;Pdenotes the transition probability P(st+1|st, at)withst, st+1∈S, at∈ A(st); R:S×A→Ris the reward function that defines the desirability of the state-action pair; γ(between 0 and 1) balances the preference for immediate versus long-term rewards. A policy πis a function that maps a state stto a distribution over valid actions in the set A(st). RL algorithms aim to find a policy that maximizes the expected return, i.e., π∗= argmax π∈ΠEP,π,ρ\"TX t=0γtR(st, at)#, (1) where Tis the length of the episode, and Πis the set of possible policies. In the case of breeding programs, at each generation t, the breeder performs arbitrary crosses obtaining a new population. We define the state at time step tas the set of genomes of the population st={g(t)\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nCrop breeding programs face challenges due to long turnover times, high-dimensional decision spaces, long-term objectives, and the need to adapt to rapid climate change. This paper introduces the use of Reinforcement Learning (RL) to optimize simulated crop breeding programs. RL agents are trained to make optimal crop selection and cross-breeding decisions based on genetic information.\n\nTo benchmark RL-based breeding algorithms, we introduce a suite of Gym environments. The study demonstrates the superiority of RL techniques over standard practices in terms of genetic gain when simulated in silico using real-world genomic maize data.\n\nIntroduction\n\nCrop breeding programs aim to create new cultivars with desired traits by controlled mating among individuals in a population. Crop breeding played a vital role in the Green Revolution, which resulted in a threefold increase in plant-based food production within 50 years, while only raising land usage by 30%. However, current growth trends do not meet the forecasted 2050 demand. Yields for staple crops are plateauing or degrading mainly due to climate change and soil degradation.\n\nThe decrease in genotyping cost has produced abundant crop genomic data, unveiling novel opportunities to adapt crop genetics for these adverse conditions. Recent advancements have focused on predicting phenotype traits from genomic data for quicker and cheaper crop selection without expensive and slow trials. However, relying solely on estimated traits can lead to low-diversity pools and compromise long-term breeding program success.\n\nData-driven methods for decision-making can fill this gap and allow breeding programs to retain genetic diversity and adapt to long-term objectives, such as climate adaptation. To use these methods to optimize crop breeding, we can frame a breeding program as a sequential decision-making problem where, at each breeding generation, the breeder takes some actions, such as which and how many plants to select, and how to cross them.\n\nCrossing two plants produces offspring with a mixture of genes from the two parents. The process is repeated for several generations to deploy a cultivar with desired traits. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2023.\n\nFrom this perspective, we can optimize design choices of breeding programs with Reinforcement Learning (RL), a paradigm to address sequential decision-making problems.\n\nContributions\n\nIn this paper, we develop a framework to optimize the design choices of breeding programs, with three main contributions. (1) We model a breeding program as a Markov Decision Process (MDP), allowing us to leverage techniques from the RL literature to adaptively optimize the design choices. (2) We present a package containing multiple RL environments that simulate breeding programs with different sets of actions and observations. (3) We show the effectiveness of the approach in selecting plants based on a learned score, which outperforms the standard genomic selection based on estimated traits.\n\nPlease note that the results presented are based on simulations and may differ in real-world applications.\n\nRelated work\n\nMachine learning models have been applied in breeding programs in various ways, often for trait prediction in genomic-assisted breeding. To the best of our knowledge, RL has been only used in the context of breeding in one previous study, where the authors optimized the population size at every step of the breeding program, while other decisions were considered fixed.\n\nProblem definition\n\nIn reinforcement learning, sequential decision-making problems are formalized as Markov decision processes (MDPs), which consist of a tuple (S, ρ, A, P, R, γ), where:\n\n- S is the set of all possible states\n- ρ is the probability distribution of the initial state s0\n- A: S → A maps each state s to the possible actions in s\n- P denotes the transition probability P(st+1|st, at) with st, st+1 ∈ S, at ∈ A(st)\n- R: S × A → R is the reward function that defines the desirability of the state-action pair\n- γ (between 0 and 1) balances the preference for immediate versus long-term rewards\n\nA policy π is a function that maps a state st to a distribution over valid actions in the set A(st). RL algorithms aim to find a policy that maximizes the expected return, i.e., π∗ = argmax π ∈ Π E[P,π,ρ] ∑T t=0 γt R(st, at), where T is the length of the episode, and Π is the set of possible policies.\n\nIn the case of breeding programs, at each generation t, the breeder performs arbitrary crosses obtaining a new population. We define the state at time step t as the set of genomes of the population st = {g(t)}."
}