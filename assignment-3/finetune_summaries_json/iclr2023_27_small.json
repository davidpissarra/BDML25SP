{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\", we have created an energy estimation pipeline1, which allows practi- tioners to estimate the energy needs of their models in advance, without actually running or training them. We accomplished this, by collecting high-quality en- ergy data and building a first baseline model, capable of predicting the energy consumption of DL models by accumulating their estimated layer-wise energies. 1 I NTRODUCTION Deep CNNs, such as VGG16 or ResNet50 already deliver great performance (Simonyan & Zisser- man, 2014; He et al., 2015). Yet the increasing number of layers in such models comes at the cost of severely increased computational complexity, resulting in the need for power-hungry hardware (Thompson et al., 2020; Jin et al., 2016). An example of a model that behaves extremely poorly in this regard is a big transformer with neural architecture search (Strubell et al., 2019). Clearly, training and running these models is not just a matter of financial cost, but also environmental im- pact. Yet, deep learning models are often only evaluated concerning standard performance metrics like accuracy or inference time, but not energy consumption or ecological footprint. While advance- ments in the research field often take precedence over energy considerations, this trade-off between performance and environmental impact should still be taken into account, despite the difficulty in accurately reporting these metrics due to a lack of appropriate tools (Lacoste et al., 2019; Garc ´ıa- Mart ´ın et al., 2019). In light of these motivations, we developed a pipeline to collect model energy data and to ap- proximate a model’s energy consumption, before training and without the need for a direct energy measurement. Not only would this approach be more efficient and environmentally friendly, but it would also allow deep learning engineers to strike a balance between performance and environmen- tal impact with greater ease. To achieve this goal, we attempt to compute the energy consumption of various deep learning architectures as the sum of their layer-wise estimated energies. Related work. While previous publications in this field are rather scarce, some notable con- tributions helped tremendously with our research. The experiment-impact-tracker is a python package that allows the user to measure their models and report their carbon footprint (Henderson et al., 2020). Another python package and integral part of our research is codecarbon, which also allows for accurate CPU energy measurements and carbon reporting (Mila, 2022). Additionally, Lacoste et al. (2019) created ML CO2 Impact, a web tool to calculate a models CO2 impact based on key parameters. Furthermore, in Cai et al. (2017) the authors also attempt to estimate the energy consumption of CNNs through layer-wise estimates. Unfortunately, their work shows some limitations as the energy consumption was measured from a GPU with a fixed clock speed, which is not realistic for real-world use. Moreover, the authors utilize hardware features in their models, making their tool more hardware-dependent. The authors of Dehghani et al. (2021) further motivate our cause by promoting the usage of concrete real-world metrics, and show that no single ∗Corresponding author 1https://www.cs.cit.tum.de/daml/energy-consumption-dl 1Published as a workshop paper at ”Tackling Climate Change with Machine Learning”, ICLR 2023 Table 1: All possible parameter configurations for the data-collection process for each layer-type; the Activations encompass: ReLU, Sigmoid, Tanh, and Softmax; the Architectures encompass AlexNet (Krizhevsky et al., 2017), and VGG11/13/16. Parameter Conv2d MaxPool2d Linear Activations Architectures batch-size [1, 256] [1, 256] [1, 512] [1, 512] [1, 256] image-size [4, 224] [4, 224] / / 224 kernel-size [1, 11] [1, 11] / / / in-channels/size [1, 512] / [1, 5000] [ 5∗104,5∗106] 3 out-channels/size [1, 512] [1, 512] [1, 5000] / / stride [1, 5] [1, 5] / / / padding [0, 3] [0, 3] / / / cost indicator is sufficient and that incomplete cost reporting can have a direct environmental impact. Our Contribution. (1) We release a high-quality data set on the energy consumption of various models and layer types, along with a novel and modular data-collection process, built to make our research hardware independent (Sec. 2). (2)We created a collection of predictors for different layer types, forming a simple energy-prediction baseline for the energy consumption\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nWe have developed an energy estimation pipeline that enables practitioners to estimate the energy needs of their models in advance without actually running or training them. To achieve this, we collected high-quality energy data and built a baseline model that can predict the energy consumption of deep learning models by summing the estimated energy of each layer.\n\nIntroduction\n\nDeep convolutional neural networks (CNNs), such as VGG16 and ResNet50, deliver excellent performance (Simonyan & Zisserman, 2014; He et al., 2015). However, the increasing number of layers in these models comes at the cost of significantly increased computational complexity, resulting in the need for power-hungry hardware (Thompson et al., 2020; Jin et al., 2016). A notable example of a model that performs poorly in this regard is a large transformer with neural architecture search (Strubell et al., 2019). Training and running these models is not only a matter of financial cost but also environmental impact. Despite this, deep learning models are often evaluated based on standard performance metrics such as accuracy or inference time, but not energy consumption or ecological footprint.\n\nAdvancements in the research field often prioritize performance over energy considerations, but this trade-off between performance and environmental impact should still be taken into account, despite the difficulty in accurately reporting these metrics due to a lack of suitable tools (Lacoste et al., 2019; García-Martín et al., 2019). In light of these motivations, we developed a pipeline to collect model energy data and approximate a model's energy consumption before training and without the need for direct energy measurement. This approach would be more efficient and environmentally friendly, allowing deep learning engineers to balance performance and environmental impact more easily.\n\nTo achieve this goal, we compute the energy consumption of various deep learning architectures as the sum of their layer-wise estimated energies.\n\nRelated Work\n\nPrevious publications in this field are scarce, but some notable contributions have helped our research. The experiment-impact-tracker is a Python package that allows users to measure their models and report their carbon footprint (Henderson et al., 2020). Another Python package, codecarbon, also allows for accurate CPU energy measurements and carbon reporting (Mila, 2022). Lacoste et al. (2019) created ML CO2 Impact, a web tool to calculate a model's CO2 impact based on key parameters. Cai et al. (2017) also attempted to estimate the energy consumption of CNNs through layer-wise estimates, but their work has limitations, as the energy consumption was measured from a GPU with a fixed clock speed, which is not realistic for real-world use. Additionally, their tool is more hardware-dependent.\n\nDehghani et al. (2021) further motivate our cause by promoting the use of concrete real-world metrics and show that no single metric is sufficient, and incomplete cost reporting can have a direct environmental impact.\n\nOur Contribution\n\n(1) We release a high-quality dataset on the energy consumption of various models and layer types, along with a novel and modular data-collection process that makes our research hardware-independent (Section 2).\n\n(2) We created a collection of predictors for different layer types, forming a simple energy-prediction baseline for the energy consumption."
}