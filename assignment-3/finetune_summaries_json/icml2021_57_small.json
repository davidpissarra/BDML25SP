{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" safe. In this paper, we combine RL with set-theoretic control to propose a computationally efﬁcient ap- proach to safe RL. We demonstrate the method on a simpliﬁed power system model and compare it with other RL techniques. 1. Introduction Distributed energy resources (DERs) such as wind, solar, and energy storage systems are emerging as a crucial means to achieve greenhouse gas emissions reductions (Steinberg et al., 2017). However, integrating DERs into existing power infrastructure while maintaining reliability proves to be chal- lenging (Kroposki et al., 2017). For example, as DERs come to dominate the energy supply, the uncertainty in wind and solar generation will narrow the transient stability margins of electric generators (Kundur et al., 1994). As a result, the electric grid will be more prone to failures under large disturbances such as line outages. The ﬂexibility and speed of electronically-controlled DERs can be leveraged for control algorithms that achieve higher efﬁciency in maintaining system stability. However, when DERs are integrated at large scale, it is challenging to ﬁnd or implement the optimal strategy. Further, power systems are a quintessential example of safety-critical infrastructure, 1Department of Electrical and Computer Engineering, Univer- sity of Washington, Seattle, WA, United States. Correspondence to: Daniel Tabas <dtabas@uw.edu >, Baosen Zhang <zhang- bao@uw.edu >. Tackling Climate Change with Machine Learning workshop at ICML 2021.in which the violation of operational constraints can lead to large blackouts with high economic and human cost. Safe reinforcement learning (safe RL) offers the possibility to design efﬁcient controllers for safety-critical applications without the need for detailed power system models (Chen et al., 2021; Glavic, 2019; Cao et al., 2020; Cui & Zhang, 2021). Safe RL can take several perspectives (Garc ´ıa & Fern ´andez, 2015), but in this paper, we are concerned with the ability to guarantee satisfaction of hard state and action constraints during both exploration and execution of a pol- icy. Since guaranteeing constraint satisfaction is impossible without some degree of knowledge about the system (Wachi & Sui, 2020), we assume that the system model is at least ap- proximately known. Even with this model, the task at hand may be too complex for traditional model-based control. Contributions. We propose a safe and computationally ef- ﬁcient reinforcement learning paradigm using results from set-theoretic control. Our method exploits the geometry of polytopic robust controlled-invariant sets (RCIs), obtained either from models (Blanchini & Miani, 2015) or directly from data (Chen et al., 2019). We design a policy neural network with an output layer that uses iterative linear pro- jections to guarantee satisfaction of hard state constraints during exploration and execution. Our method applies to safety-critical control tasks which require computationally efﬁcient decision-making on short timescales. We apply the technique to a transient stability problem in a small power system model. Approaches to safe RL that use weaker notions of safety include restricting actions to those with Lyapunov stability guarantees (Perkins & Barto, 2002; Berkenkamp et al., 2017; Cui & Zhang, 2021) or robustness guarantees (Kretchmar et al., 2001; Donti et al., 2021). However, stability does not always translate to constraint satisfaction. Other approaches use optimization-based “safety ﬁlters” to project an action recommended by a policy network into a set of safe control actions (Cheng et al., 2019; Wabersich & Zeilinger, 2021). However, it may not be possible to solve optimization prob- lems in real time. The work in (Zheng et al., 2020) takes a geometric approach in order to guarantee safe exploration without solving an optimization problem, but the resulting “one-step” safety guarantee can still lead a system into states from which no action is safe. Our work extends this one- step safety guarantee to an inﬁnite-horizon guarantee for aA Set-Theoretic Approach to Safe RL in Power Systems class of systems with bounded uncertainty. 2. Problem Model Consider the discrete-time, discounted, inﬁnite-horizon, constrained robust optimal control problem given by min x(\u0001);u(\u0001)Ed(\u0001)¥ å t=1gtJ(xt;ut) (1a) subject to,8t:xt+1=f(xt;ut;dt) (1b) xt+12X8dt2D (1c) ut2U (1d) where xt2Rn,ut\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nIn this paper, we combine reinforcement learning (RL) with set-theoretic control to propose a computationally efficient approach to safe RL. We demonstrate the method on a simplified power system model and compare it with other RL techniques.\n\nDistributed energy resources (DERs), such as wind, solar, and energy storage systems, are emerging as a crucial means to achieve greenhouse gas emissions reductions. However, integrating DERs into existing power infrastructure while maintaining reliability proves to be challenging. For example, as DERs come to dominate the energy supply, the uncertainty in wind and solar generation will narrow the transient stability margins of electric generators. As a result, the electric grid will be more prone to failures under large disturbances such as line outages. The flexibility and speed of electronically-controlled DERs can be leveraged for control algorithms that achieve higher efficiency in maintaining system stability. However, when DERs are integrated at large scale, it is challenging to find or implement the optimal strategy. Furthermore, power systems are a quintessential example of safety-critical infrastructure, where the violation of operational constraints can lead to large blackouts with high economic and human cost.\n\nSafe reinforcement learning (safe RL) offers the possibility to design efficient controllers for safety-critical applications without the need for detailed power system models. Safe RL can take several perspectives, but in this paper, we are concerned with the ability to guarantee satisfaction of hard state and action constraints during both exploration and execution of a policy. Since guaranteeing constraint satisfaction is impossible without some degree of knowledge about the system, we assume that the system model is at least approximately known. Even with this model, the task at hand may be too complex for traditional model-based control.\n\nContributions. We propose a safe and computationally efficient reinforcement learning paradigm using results from set-theoretic control. Our method exploits the geometry of polytopic robust controlled-invariant sets (RCIs), obtained either from models or directly from data. We design a policy neural network with an output layer that uses iterative linear projections to guarantee satisfaction of hard state constraints during exploration and execution. Our method applies to safety-critical control tasks that require computationally efficient decision-making on short timescales. We apply the technique to a transient stability problem in a small power system model.\n\nApproaches to safe RL that use weaker notions of safety include restricting actions to those with Lyapunov stability guarantees or robustness guarantees. However, stability does not always translate to constraint satisfaction. Other approaches use optimization-based \"safety filters\" to project an action recommended by a policy network into a set of safe control actions. However, it may not be possible to solve optimization problems in real time. The work in (Zheng et al., 2020) takes a geometric approach to guarantee safe exploration without solving an optimization problem, but the resulting \"one-step\" safety guarantee can still lead a system into states from which no action is safe. Our work extends this one-step safety guarantee to an infinite-horizon guarantee for a class of systems with bounded uncertainty.\n\n2. Problem Model\n\nConsider the discrete-time, discounted, infinite-horizon, constrained robust optimal control problem given by:\n\nmin x(t);u(t) ∑t=1gtJ(x(t);u(t))\n\nsubject to:\nx(t+1) = f(x(t);u(t);d(t))\nx(t) ∈ X\nu(t) ∈ U\n\nwhere x(t) ∈ R^n, u(t) ∈ R^m."
}