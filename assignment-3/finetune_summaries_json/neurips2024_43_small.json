{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\"geosense.uni-freiburg.de moiexpositoalonso@berkeley.edu Abstract Climate change is negatively impacting the world’s biodiversity. To build automated systems to monitor these negative biodiversity impacts, large-scale, volunteer- collected datasets like iNaturalist are built from community-identified, natural imagery. However, such volunteer-based data are opportunistic and lack a struc- tured sampling strategy, resulting in geographic, temporal, observation quality, and socioeconomic, biases that stymie uptake of these models for downstream biodi- versity monitoring tasks. Here we introduce DivShift North American West Coast (DivShift-NAWC), a curated dataset of almost 8 million iNaturalist plant images across the western coast of North America, for exploring the effects of these biases on deep learning model performance. We compare model performance across four known biases and observe that they indeed confound model performance. We suggest practical strategies for curating datasets to train deep learning models for monitoring climate change’s impacts on the world’s biodiversity. 1 Introduction The world’s biodiversity is under threat from climate and land-use change [ 96,90]. Biodiversity helps ecosystems combat climate change by improving carbon sequestration [ 137] and novel climate adaptation [ 105]. Therefore, monitoring the world’s biodiversity via automated tools [ 100] is critical to mitigate climate change’s effects on the natural world. Building machine learning tools for this automated monitoring requires large volumes of natural world imagery [ 53]. Participatory science applications—where users can upload and identify photos of species in their natural environments— have surged in popularity [ 37,3,39] (see Appdx. 5.1.1 for participatory science overview). These applications now provide sufficient finely-labeled imagery data to build large-scale biodiversity datasets [ 125,60,122,107,50,55] and train deep learning models for automated biodiversity monitoring tasks such as species recognition [ 125,50,55,60], species distribution modeling [ 122, 60, 107, 53], novel species identification [115], and visual question answering [125]. However, as these observations become easier for the public to make, sampling becomes more unstructured [ 2,98,52] and injects biases into these data [ 63,37,11,39]. Therefore, volunteer- collected biodiversity datasets often do not fully reflect the world’s biodiversity [ 3,27,20,70], presenting challenges for the general uptake of these data for biodiversity monitoring [ 66,12,34]. To help quantify the effects of these biases on model performance, here we introduce a new public *Denotes equal contribution to this work Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.Spatial Bias Observation Quality Bias Temporal Bias S   M   T   W   T   F    SSociopolitical BiasFigure 1: Biodiversity data biases include spatial bias, e.g. more observations in urban compared to wild areas; temporal bias, e.g. more observations on weekends compared to weekdays; observation quality bias, e.g. more diverse observations made by highly engaged observers than infrequent users; andsociopolitical bias, e.g. disparities between observation density due to political as opposed to ecological boundaries. biodiversity imagery dataset DivShift-North American West Coast (DivShift-NAWC), comprised of 8 million observations and 10,000 plant species across the North American west coast. DivShift- NAWC spans wide climatic, ecological, and sociopolitical gradients, enabling the targeted testing of downstream performance effects of spatial, temporal, observation quality, and sociopolitical biases present in volunteer-collected biodiversity datasets. 2 Related Works Biases in volunteer-collected biodiversity datasets. Collection of large-scale volunteer datasets are subject to social and ecological filters [ 21,63], which inject many types of bias into biodiversity datasets built from these collections [ 21,38,64] (see Appdx. 5.1.2 for further definition). In this work, we focus on four kinds of bias common to volunteer-collected biodiversity datasets: spatial, temporal, observation quality, and sociopolitical (Fig. 1). Spatial bias includes observer preferences to sampling easy-to-access greenspaces in urban areas [ 56,83,3,38,39,21] (see Appdx. 5.1.3). Temporal bias [3,39,119,106,30,61] includes a skew towards more observations on weekends when observers are free from work [ 38,29,27] (see Appdx. 5.1.3). Observation quality bias [39,124,85,10] manifests as a small but dedicated group of users that tend to observe more species in more diverse habitats [ 38,104] (see Appdx. 5.1.3). Lastly, sociopolitical bias in who has access to the resources, time, and areas to collect biodiversity observations [ 9\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy text removed:\n\nClimate change is negatively impacting the world's biodiversity. To build automated systems to monitor these negative biodiversity impacts, large-scale, volunteer-collected datasets like iNaturalist are built from community-identified, natural imagery. However, such volunteer-based data are opportunistic and lack a structured sampling strategy, resulting in geographic, temporal, observation quality, and socioeconomic biases that hinder the uptake of these models for downstream biodiversity monitoring tasks.\n\nWe introduce DivShift North American West Coast (DivShift-NAWC), a curated dataset of almost 8 million iNaturalist plant images across the western coast of North America, for exploring the effects of these biases on deep learning model performance. We compare model performance across four known biases and observe that they indeed confound model performance. We suggest practical strategies for curating datasets to train deep learning models for monitoring climate change's impacts on the world's biodiversity.\n\nThe world's biodiversity is under threat from climate and land-use change. Biodiversity helps ecosystems combat climate change by improving carbon sequestration and novel climate adaptation. Therefore, monitoring the world's biodiversity via automated tools is critical to mitigate climate change's effects on the natural world. Building machine learning tools for this automated monitoring requires large volumes of natural world imagery. Participatory science applications, where users can upload and identify photos of species in their natural environments, have surged in popularity. These applications now provide sufficient finely-labeled imagery data to build large-scale biodiversity datasets and train deep learning models for automated biodiversity monitoring tasks such as species recognition, species distribution modeling, novel species identification, and visual question answering.\n\nHowever, as these observations become easier for the public to make, sampling becomes more unstructured and injects biases into these data. Therefore, volunteer-collected biodiversity datasets often do not fully reflect the world's biodiversity, presenting challenges for the general uptake of these data for biodiversity monitoring.\n\nTo help quantify the effects of these biases on model performance, we introduce a new public dataset, DivShift-North American West Coast (DivShift-NAWC), comprised of 8 million observations and 10,000 plant species across the North American west coast. DivShift-NAWC spans wide climatic, ecological, and sociopolitical gradients, enabling the targeted testing of downstream performance effects of spatial, temporal, observation quality, and sociopolitical biases present in volunteer-collected biodiversity datasets.\n\nBiases in volunteer-collected biodiversity datasets are subject to social and ecological filters, which inject many types of bias into biodiversity datasets built from these collections. In this work, we focus on four kinds of bias common to volunteer-collected biodiversity datasets: spatial, temporal, observation quality, and sociopolitical biases.\n\nSpatial bias includes observer preferences to sampling easy-to-access greenspaces in urban areas. Temporal bias includes a skew towards more observations on weekends when observers are free from work. Observation quality bias manifests as a small but dedicated group of users that tend to observe more species in more diverse habitats. Lastly, sociopolitical bias in who has access to the resources, time, and areas to collect biodiversity observations."
}