{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" of a common dataset and evaluation metrics make inter-comparison between the proposed models difﬁcult. In fact, despite the recent research surge in data- driven weather forecasting, there is currently no standard approach for evaluating the proposed models. Here we introduce WeatherBench, a benchmark dataset for data-driven medium-range weather forecasting. We provide data derived from an archive of assimilated earth observations for the last 40 years that has been pro- cessed to facilitate the use in machine learning models. We propose a simple and clear evaluation metric which will enable a direct comparison between different proposed methods. Further, we provide baseline scores from simple linear re- gression techniques, purely physical forecasting models as well as existing deep learning weather forecasting models. All data and code are made publicly avail- able along with tutorials for getting started. We believe WeatherBench will pro- vide a useful and reproducible way of evaluating data-driven weather forecasting models and we hope that it will accelerate research in this direction. 1 I NTRODUCTION Extreme weather events are the most immediate and threatening consequence of climate change to human life, property, agricultural production and natural capital (Bouwer, 2019). As the frequency of extreme weather events increases, it becomes clear that accurate weather forecasts will play a cru- cial role in local and national crisis management planning and climate adaptation decision-making. Currently, these forecasts are generated using physics-based computer simulations, in which the governing equations (or our best approximation thereof) of the atmosphere and ocean are solved on a discrete numerical grid. These models are generally effective, but computationally expensive. They also perform poorly when it comes to the prediction of extreme events. If data-driven models were able to learn a more efﬁcient representation of the underlying dynamical and physical equa- tions, they would enable computationally cheaper forecasts. This could help improve the probability estimation of extreme events through large ensemble (Monte-Carlo) simulations. It is also possible that by using a diverse set of data sources, data driven models can outperform physical models in areas where the latter struggle, for example predicting rainfall over Africa (V ogel et al., 2018). In the last couple of years, several studies have pioneered data-driven forecasting methods (Dueben & Bauer, 2018; Scher, 2018; Scher & Messori, 2019; Weyn et al., 2019). These studies considered different settings of general circulation models at different resolutions to be ground truth, and used these simulations to train different neural network architectures evaluated using different metrics. The differences of the proposed methods already highlight the importance of a common benchmark case to compare prediction skill. In particular, benchmark datasets can have a huge impact because they make different algorithms inter-comparable and foster constructive competition, particularly in a nascent direction of research. 1Published as a conference paper at ICLR 2020 Here, we introduce WeatherBench, the ﬁrst benchmark problem for data-driven weather forecast- ing. Our initial release provides a ready-to-use dataset for download along with speciﬁc metrics to compare different approaches. In this paper, we describe the dataset and evaluation metrics. We show how one can use WeatherBench to perform two types of forecasting experiments: a direct forecast and an iterative forecast. We also provide several baseline models and highlight directions for further research. 2 B ENCHMARK OVERVIEW The main goal of WeatherBench is to evaluate deep learning models for global medium-range (i.e. several days to two weeks) weather forecasting. By specifying a target variable and a target horizon, WeatherBench allows experimentation and evaluation of new model architectures. For this initial release, we use the ERA5 reanalysis dataset (C3S, 2017) for training, evaluating (1979-2016) and testing (2017-2018). Reanalysis datasets provide the best guess of the atmospheric state at any point in time by combining a forecast model with the available observations. We regrid the data from its original resolution, which comprises several Terabytes, to lower resolutions using bilinear interpolation. This is a more realistic use case, since very high resolutions are hard to handle for most deep learning models because of GPU memory constraints and I/O speed. In particular, we chose 5.625\u000e(32\u000264 grid points), 2.8125\u000e(64\u0002128 grid points) and 1.40525\u000e(128\u0002256 grid points) degrees latitude by longitude. Further, for 3D ﬁelds we selected 10 vertical levels, varying from 1 to 1000 hPa. The available variables were chosen based on meteorological considerations. Geopotential, temper- ature, humidity and wind are prognostic state variables in most physical numerical weather predic- tion (NWP) and climate models, and were hence included along with horizontal relative vorticity and potential vorticity. In addition to the 3D ﬁelds, we also include 2D ﬁeld\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nThe lack of a common dataset and evaluation metrics makes inter-comparison between proposed models difficult. Despite the recent surge in data-driven weather forecasting research, there is currently no standard approach for evaluating proposed models. Here, we introduce WeatherBench, a benchmark dataset for data-driven medium-range weather forecasting. We provide data derived from an archive of assimilated earth observations for the last 40 years, processed to facilitate use in machine learning models. We propose a simple and clear evaluation metric to enable direct comparison between different proposed methods. We also provide baseline scores from simple linear regression techniques, physical forecasting models, and existing deep learning weather forecasting models. All data and code are made publicly available along with tutorials for getting started. We believe WeatherBench will provide a useful and reproducible way of evaluating data-driven weather forecasting models and hope it will accelerate research in this direction.\n\nExtreme weather events are the most immediate and threatening consequence of climate change to human life, property, agricultural production, and natural capital. As the frequency of extreme weather events increases, accurate weather forecasts will play a crucial role in local and national crisis management planning and climate adaptation decision-making. Currently, these forecasts are generated using physics-based computer simulations, which are generally effective but computationally expensive and perform poorly when predicting extreme events. Data-driven models, if able to learn a more efficient representation of the underlying dynamical and physical equations, could enable computationally cheaper forecasts and improve the probability estimation of extreme events through large ensemble simulations. Data-driven models may also outperform physical models in areas where the latter struggle, such as predicting rainfall over Africa.\n\nIn recent years, several studies have pioneered data-driven forecasting methods. These studies considered different settings of general circulation models at different resolutions as ground truth and used these simulations to train different neural network architectures evaluated using different metrics. The differences in the proposed methods highlight the importance of a common benchmark case to compare prediction skill. Benchmark datasets can have a significant impact by making different algorithms inter-comparable and fostering constructive competition, particularly in a nascent direction of research.\n\nHere, we introduce WeatherBench, the first benchmark problem for data-driven weather forecasting. Our initial release provides a ready-to-use dataset for download along with specific metrics to compare different approaches. We describe the dataset and evaluation metrics and show how one can use WeatherBench to perform two types of forecasting experiments: direct forecasting and iterative forecasting. We also provide several baseline models and highlight directions for further research.\n\nThe main goal of WeatherBench is to evaluate deep learning models for global medium-range weather forecasting. By specifying a target variable and a target horizon, WeatherBench allows experimentation and evaluation of new model architectures. For this initial release, we use the ERA5 reanalysis dataset for training, evaluation (1979-2016), and testing (2017-2018). Reanalysis datasets provide the best guess of the atmospheric state at any point in time by combining a forecast model with available observations. We regrid the data from its original resolution to lower resolutions using bilinear interpolation. We chose 5.625° (32x64 grid points), 2.8125° (64x128 grid points), and 1.40525° (128x256 grid points) degrees latitude by longitude. For 3D fields, we selected 10 vertical levels, varying from 1 to 1000 hPa. The available variables were chosen based on meteorological considerations, including geopotential, temperature, humidity, wind, horizontal relative vorticity, and potential vorticity. We also include 2D fields."
}