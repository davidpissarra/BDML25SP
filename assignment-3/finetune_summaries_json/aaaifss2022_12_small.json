{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" language processing (NLP). However, while pretraining on general language has been shown to work very well for common language, it has been observed that niche language poses problems. In par- ticular, climate-related texts include speciﬁc language that common LMs can not represent accurately. We argue that this shortcoming of today’s LMs limits the applicability of modern NLP to the broad ﬁeld of text processing of climate- related texts. As a remedy, we propose C LIMATE BERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts, crawled from various sources such as common news, research arti- cles, and climate reporting of companies. We ﬁnd that C LI- MATE BERT leads to a 48% improvement on a masked lan- guage model objective which, in turn, leads to lowering error rates by 3.57% to 35.71% for various climate-related down- stream tasks like text classiﬁcation, sentiment analysis, and fact-checking. 1 Introduction Researchers working on climate change-related topics in- creasingly use natural language processing (NLP) to auto- matically extract relevant information from textual data. Ex- amples include the sentiment or speciﬁcity of language used by companies when discussing climate risks and measuring corporate climate change exposure, which increases trans- parency to help the public know where we stand on climate change (e.g., Callaghan et al. 2021; Bingler et al. 2022b). Many studies in this domain apply traditional NLP meth- ods, such as dictionaries, bag-of-words approaches or sim- ple extensions thereof (e.g., Gr ¨uning 2011; Sautner et al. 2022). However, such analyses face considerable limita- tions, since climate-related wording could vary substan- tially by source (Kim and Kang 2018). Deep learning tech- niques that promise higher accuracy are gradually replacing these approaches (e.g., K ¨olbel et al. 2020; Luccioni, Baylor, and Duchene 2020; Bingler et al. 2022a; Callaghan et al. 2021; Wang, Chillrud, and McKeown 2021; Friederich et al. 2021). Indeed, it has been shown in related domains that Copyright c 2022, Association for the Advancement of Artiﬁcial Intelligence (www.aaai.org). All rights reserved.deep learning in NLP allows for impressive results, outper- forming traditional methods by large margins (Varini et al. 2020). These deep learning-based approaches make use of lan- guage models (LMs), which are trained on large amounts of textual and unlabelled data. This training on unlabelled data is called pretraining and leads to the model learning representations of words and patterns of common language. One of the most prominent language models is called BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al. 2018) with its successors R OBERT A(Liu et al. 2019), Transformer-XL (Dai et al. 2019) and ELECTRA (Clark et al. 2020). These models have been trained on huge amounts of text which was crawled from an unprecedented amount of online resources. After the pretraining phase, most LMs are trained on addi- tional tasks, the downstream task. For the downstream tasks, the LM builds on and beneﬁts from the word representations and language patterns learned in the pretraining phase. The pre-training beneﬁt is especially large on downstream tasks for which the collection of samples is difﬁcult and, thus, the resulting training datasets are small (hundreds or few thou- sands of samples). Furthermore, it has been shown that a model that was pretrained on the downstream task-speciﬁc text exhibits better performance, compared to a model that has been pretrained solely on general text (Araci 2019; Lee et al. 2020). Hence, a straightforward extension to the standard com- bination of pretraining is the so-called domain-adaptive pre- training (Gururangan et al. 2020). This approach has re- cently been studied for various tasks and basically comes in the form of pretraining multiple times — in particular pre- training in the language domain of the downstream task, i.e., pretraining (general domain) +domain-adaptive pretraining (downstream domain) +training (downstream task) : To date, regardless of the increase in using NLP for cli- mate change related research, a model with climate domain- adaptive pretraining has not been publicly available, yet. Research so far rather relied on models pretrained on gen- eral language, and ﬁne-tuned on the downstream task.\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and text removed:\n\nLanguage processing (NLP) has been shown to be effective for common language, but it faces challenges when dealing with niche language, such as climate-related texts. These texts often include specific language that common language models (LMs) cannot accurately represent. This limitation restricts the applicability of modern NLP to the broad field of text processing of climate-related texts. To address this issue, we propose CLIMATE BERT, a transformer-based language model that is further pretrained on over 2 million paragraphs of climate-related texts from various sources, including news articles, research papers, and climate reporting from companies. We find that CLIMATE BERT leads to a 48% improvement on a masked language model objective, resulting in a 3.57% to 35.71% reduction in error rates for various climate-related downstream tasks, such as text classification, sentiment analysis, and fact-checking.\n\nResearchers working on climate change-related topics increasingly use NLP to automatically extract relevant information from textual data. Examples include analyzing the sentiment or specificity of language used by companies when discussing climate risks and measuring corporate climate change exposure, which increases transparency and helps the public understand the current state of climate change (e.g., Callaghan et al., 2021; Bingler et al., 2022b). Many studies in this domain apply traditional NLP methods, such as dictionaries, bag-of-words approaches, or simple extensions (e.g., Gruning, 2011; Sautner et al., 2022). However, these analyses face significant limitations, as climate-related wording can vary substantially by source (Kim and Kang, 2018). Deep learning techniques that promise higher accuracy are gradually replacing these approaches (e.g., Kolbel et al., 2020; Luccioni, Baylor, and Duchene, 2020; Bingler et al., 2022a; Callaghan et al., 2021; Wang, Chillrud, and McKeown, 2021; Friederich et al., 2021). Indeed, it has been shown in related domains that deep learning in NLP allows for impressive results, outperforming traditional methods by large margins (Varini et al., 2020). These deep learning-based approaches make use of language models, which are trained on large amounts of unlabelled textual data. This training on unlabelled data is called pretraining and leads to the model learning representations of words and patterns of common language.\n\nOne of the most prominent language models is BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2018), along with its successors, such as RoBERTa (Liu et al., 2019), Transformer-XL (Dai et al., 2019), and ELECTRA (Clark et al., 2020). These models have been trained on vast amounts of text crawled from online resources. After the pretraining phase, most LMs are trained on additional tasks, known as downstream tasks. For downstream tasks, the LM builds on and benefits from the word representations and language patterns learned in the pretraining phase. The pretraining benefit is particularly significant for downstream tasks with small training datasets (hundreds or thousands of samples). Furthermore, it has been shown that a model pretrained on downstream task-specific text exhibits better performance compared to a model pretrained solely on general text (Araci, 2019; Lee et al., 2020). A straightforward extension to the standard combination of pretraining is domain-adaptive pretraining (Gururangan et al., 2020), which involves pretraining multiple times, specifically pretraining in the language domain of the downstream task.\n\nTo date, despite the increasing use of NLP in climate change research, a model with climate domain-adaptive pretraining has not been publicly available. Research has instead relied on models pretrained on general language and fine-tuned on the downstream task."
}