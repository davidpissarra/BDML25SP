{
    "question": "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nThe following text is noisy and dirty, so please rephrase the following text while removing noisy text (e.g., incorrect characters coming from the pdf reader, emails, footnotes, etc.), but without removing important content or adding new data, however keep as much data as possible:\n\n\" of RL algorithms on realistic sustainability tasks. The first environment simulates the problem of scheduling decisions for a fleet of electric vehicle (EV) charging stations, and the second environment simulates decisions for a battery storage system bidding in an electricity market. We describe the structure and features of the environments and show that standard RL algorithms have significant room for improving performance. We discuss current challenges in introducing RL to real-world sustainability tasks, including physical constraints and distribution shift. 1 Introduction While reinforcement learning (RL) algorithms have demonstrated tremendous success in applications ranging from game-playing, e.g., Atari and Go, to robotic control [ 1–3], most RL algorithms continue to only be benchmarked on toy environments— e.g., OpenAI Gym [ 4]. These toy environments generally do not have realistic physical constraints, nor realistic environmental shifts over time. This realism gap limits our ability to reliably deploy off-the-shelf RL algorithms in real-world systems. Developing better RL algorithms to address these challenges requires a means of empirically bench- marking and comparing the performance of different algorithms in real-world settings. Our inspiration comes from progress in supervised machine learning (ML), where widespread adoption of break- through techniques were fueled by large datasets with standardized benchmarks, such as ImageNet for computer vision [ 5] and the GLUE benchmark for natural language processing [ 6]. More recently, many supervised learning datasets have been created to address specific real-world sustainability challenges, such as monitoring global progress towards the sustainable development goals [7]. In this work, we introduce SustainGym, a new test bed of RL environments that realistically model real-world sustainability settings. The initial release of SustainGym is comprised of two environments (with more environments to be added in the near future): •EVChargingEnv models the problem of scheduling electric vehicle (EV) charging to meet user needs while minimizing CO 2emissions. •ElectricityMarketEnv models a grid-scale battery storage system bidding into the elec- tricity market to generate profit (through price arbitrage) and reduce CO 2emissions. Prior related work include ConservationGym for ecological applications [ 8], PowerGridWorld for power systems [ 9], and CityLearn for demand response and urban energy management [ 10], among others. RL environments and algorithms for both EV charging [ 11–13] and electricity markets [14–16] have also been explored. Compared to these prior work, the unique aspects of SustainGym Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.Figure 1: EV arrival vs. departure times for the Caltech EV charging network. Historical data is in blue, and log-likelihood contours from a 30-component GMM are in orange. The distribution of EV arrival and departure times changed noticeably between 2019 (pre-COVID) to 2020. are its focus on tracking estimated CO 2emissions and its ability to test RL algorithms on challenging distribution shifts (see Figures A3 and A6). We expect SustainGym to serve as a benchmark for the progress of RL algorithm development for real-world sustainability applications. Empirically, we find that standard off-the-shelf RL algorithms have mixed performance on Sus- tainGym. Furthermore, we show how distribution shifts may reduce the performance of these RL algorithms, demonstrating a need for more robust algorithms. Finally, comparisons against non-RL baselines and oracles show that RL has significant room for improvement. Due to page constraints, the main text of this paper summarizes key design choices and experimental observations for SustainGym. Details (including an extended related works section) can be found in Appendix A. Code and instructions for using SustainGym can be found on GitHub.1 2 Environments This section summarizes design choices of the two environments included with the initial release of SustainGym. Both environments impose a cost on the simulated CO 2emissions induced by the actions of an agent as a result of changes in electricity consumption. Our environments use data on California’s historical marginal operating emissions rate (MOER, in kgCO 2/kWh), which is the increase in CO 2emissions per increase in energy demand. The MOER at time tis denoted mt∈R+, and the forecasts generated at time tfor the next ktime steps are denoted ˆmt:t+k−1|t∈Rk. By default, we use k= 36. We impose a price PCO2(in $/kgCO 2) on emissions generated by an agent. 2.1 EVChargingEnv EVChargingEnv uses ACNSim [ 11] to simulate the charging of electric vehicles (EVs) based on actual data gathered from EV charging networks between fall 2018 and summer 2021 [ 17,18]. ACNSim is a digital twin of actual charging networks at Caltech and JPL, taking into account nonlinear battery charging dynamics and unbalanced three-phase AC power flows, and is\".<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\n",
    "answer": "Here's the rephrased text with the noisy characters and data removed:\n\nReinforcement Learning for Sustainability: A New Test Bed\n\nReinforcement learning (RL) algorithms have achieved significant success in various applications, including game-playing and robotic control. However, most RL algorithms are still benchmarked on toy environments, such as OpenAI Gym, which lack realistic physical constraints and environmental shifts. This realism gap hinders the deployment of off-the-shelf RL algorithms in real-world systems. To address this challenge, we need a means of empirically benchmarking and comparing the performance of different algorithms in real-world settings.\n\nInspired by the progress in supervised machine learning, where breakthrough techniques were fueled by large datasets with standardized benchmarks, we introduce SustainGym, a new test bed of RL environments that realistically model real-world sustainability settings. The initial release of SustainGym consists of two environments:\n\n1.  EVChargingEnv: This environment models the problem of scheduling electric vehicle (EV) charging to meet user needs while minimizing CO2 emissions.\n2.  ElectricityMarketEnv: This environment models a grid-scale battery storage system bidding into the electricity market to generate profit (through price arbitrage) and reduce CO2 emissions.\n\nPrior related work includes ConservationGym for ecological applications, PowerGridWorld for power systems, and CityLearn for demand response and urban energy management. Compared to these prior work, SustainGym has a unique focus on tracking estimated CO2 emissions and testing RL algorithms on challenging distribution shifts.\n\nWe expect SustainGym to serve as a benchmark for the progress of RL algorithm development for real-world sustainability applications. Empirically, we find that standard off-the-shelf RL algorithms have mixed performance on SustainGym. Furthermore, we show how distribution shifts may reduce the performance of these RL algorithms, demonstrating a need for more robust algorithms. Finally, comparisons against non-RL baselines and oracles show that RL has significant room for improvement.\n\n2 Environments\n\nThis section summarizes the design choices of the two environments included with the initial release of SustainGym. Both environments impose a cost on the simulated CO2 emissions induced by the actions of an agent as a result of changes in electricity consumption. Our environments use data on California's historical marginal operating emissions rate (MOER), which is the increase in CO2 emissions per increase in energy demand. The MOER at time t is denoted mt, and the forecasts generated at time t for the next k time steps are denoted ˆmt:t+k−1|t. By default, we use k = 36. We impose a price PCO2 on emissions generated by an agent.\n\n2.1 EVChargingEnv\n\nEVChargingEnv uses ACNSim to simulate the charging of electric vehicles (EVs) based on actual data gathered from EV charging networks between fall 2018 and summer 2021. ACNSim is a digital twin of actual charging networks at Caltech and JPL, taking into account nonlinear battery charging dynamics and unbalanced three-phase AC power flows."
}