{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "DECODER_PATH='../Llama-3.2-3B-Instruct'\n",
    "ENCODER_PATH = \"../bge-large-en\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "QUESTIONS_PATH = \"./rag_questions_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 25.96it/s]\n"
     ]
    }
   ],
   "source": [
    "class RAGEngine:\n",
    "    def __init__(self):\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(DECODER_PATH, torch_dtype=torch.bfloat16).to(DEVICE)\n",
    "        self.encoder = SentenceTransformer(ENCODER_PATH).to(DEVICE)\n",
    "\n",
    "        self.decoder.config.use_cache = True\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(DECODER_PATH)\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        return self.encoder.encode(docs)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.encoder.encode([query])\n",
    "    \n",
    "engine = RAGEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 2703.73it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = [(fn.split(\".\")[0], open(os.path.join(DOCS_PATH, fn), 'r', encoding='utf-8').read()) for fn in tqdm(os.listdir(DOCS_PATH)) if fn.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 1066.93it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 4000\n",
    "MAX_CHAR_OVERLAP = 500\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=MAX_CHAR_LEN, chunk_overlap=MAX_CHAR_OVERLAP)\n",
    "split_docs = []\n",
    "for doc in tqdm(docs):\n",
    "    split_docs.extend(splitter.split_text(doc[1]) if len(doc[1]) > MAX_CHAR_LEN else [doc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = engine.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(context, question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_DOCS = 3\n",
    "D = doc_embeddings.shape[1]\n",
    "M = 32\n",
    "index = faiss.IndexHNSWFlat(D, M)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 20 Apr 2025\n",
      "\n",
      "Use only the following pieces of context to answer the question at the end. Different references are seperated by \"\n",
      "\n",
      "\". Please only use the references relevant to answer the question fresults and give relevant insight to policymakers. 1. Introduction Lowering human greenhouse gases emissions is one major goal of the efforts against climate change, and the focus and concern of international cooperation (Paris Agreement, 2015). Many indicators of human development - population, Gross Domestic Product (GDP), environmental footprint - have been following exponential curves during the past decades (Steffen et al., 2015); hence, drastic measures are needed if we are to switch from increasing to quickly de- creasing emissions, as expressed in global organisations goals (IPCC Fifth Assessment Report (2014)). Understanding and forecasting the evolution, on a country- scale, of various indicators related to carbon emissions, may help to give a clear idea of the progress we are making, or not, towards lower emissions. The main indicators that we chose to study are the variables appearing in Kaya identity (Kaya & Yokoburi, 1997), on a country level: population, national GDP, energy supply and CO2 emissions. Our main objective is to develop a model able to use this data to make accurate forecasts, on a medium/long-time horizon. Machine-learning models offer interesting advantages in comparison of traditional methods used for this type of *Equal contribution1Department of Computing, Impe- rial College, London, United Kingdom2Aquatic Informat- ics, Vancouver, Canada. Correspondence to: Pierre Browne <pierre.browne20@imperial.ac.uk >. Tackling Climate Change with Machine Learning Workshop at ICML 2021.work, typically statistical models (Cerqueira et al., 2019). In particular, the recent development of Neural Ordinary Differential Equations offers a promising perspective in this case (Chen et al., 2019) - we explain the reasons for this choice in 3.2. We adapted Neural ODEs for this problem and compared the performance with a baseline statistical model. 2. Kaya identity & impact on climate change The Kaya identity expresses a very simple relation between carbon emissions F, energy supply E, Gross Domestic Prod- uct (GDP) Gand population P:F=P\u0002G P\u0002E G\u0002F E. G Pis the GDP per capita, representing our average life stan- dard, which we generally want to increase;E Gis the energy intensity of the GDP - the energy needed to create one unit of GDP -, it quantiﬁes the efﬁciency of energy usage in our activities;E Fis the carbon intensity of energy - the CO2 emission corresponding to the supply of one unit of primary energy - and indicates how reliant on carbon emissions our electricity production is. Forecasting human development and carbon emissions is possible both with the set of raw variables fP; G; E; Fg and with the set of indicators appearing in the Kaya identity fP;G P;E G;F Eg. However, the latter gives a clearer analy- sis, from a macroscopic point of view (Ritchie & Roser, b). While the raw variables are very strongly correlated altogether and vary greatly between countries, the variables from the Kaya equation look more like consistent indica- tors actionable under the right choice of policies (Hwang et al., 2020). Overall, using these four indicators seems to be a good choice in order to assess efforts made by a coun- try or region concerning carbon emissions ( ˇStreimikien ˙e & Balezentis, 2016). 3. Methodology 3.1. Datasets Four datasets were used for this study: datasets for popula- tion (World Bank, 2020b) and GDP (World Bank, 2020a) were collected from the World Bank Open Data platform; the total energy supply (IEA, 2020a) and the CO2 emis-Forecasting emissions through Kaya identity using Neural ODEs sions from fuel combustion (IEA, 2020b) were extracted from public datasets from the International Energy Agency. It should be noted that this is not the total emission for each country - however, greenhouse gases emitted by fuel combustion represent around 75% of all greenhouse gases emissions (Ritchie & Roser, a). Each variable is available yearly, from 1971 to 2019, for at least 54 countries. 3.2. Motivations for a\n",
      "\n",
      "ability for various downstream prediction tasks. It can predict daily emissions for unseen countries where direct emissions data is unavailable by using accessible power generation data. CLEAN supports bidirectional prediction for policy simulation. The model is designed to help determine the optimal energy mix required to meet specific emission reduction targets, automatically identifying a feasible energy structure. This capability is crucial for developing sustainable energy policies and ensuring that emission goals are both realistic and achievable. 3 Carbon Monitor Energy-Emission Time Series Data Table 1: Detail Information of Carbon Monitor Energy-Emission (CMEE) Time Series Dataset. Characteristics Number Description Spatial Coverage 12 Regions (e.g. China, France, United States) Temporal Coverage 2008 Daily granularity from 01/01/2019 to 30/06/2024 Energy Type 8 Energy source (e.g. Coal, Gas, Solar) Emission Type 6 Secotral emission (e.g. Transportation, Industry, Power) We have developed a comprehensive, high-resolution Energy-Emission time series dataset CMEE based on the Carbon Monitor project to understand the relationship between power generation and CO2emissions, which is crucial for evaluating national energy and sectorial transition policies. The Carbon Monitor is an international initiative providing regularly updated, science-based estimates of daily carbon emissions and power generation data for 12 countries, covering the period from January 1, 2019, to June 30, 2024 [ 13,10]. Our dataset includes data on eight types of energy sources and six categories of sectoral emissions, spanning a total of 24,096 days (2,008 days each for 12 countries). It integrates two independently collected but related datasets, ensuring consistency through structured formatting and noise reduction techniques such as Gaussian filtering. This unified dataset serves as a robust foundation for machine learning tasks and supports further research, including data mining, causal analysis, and time series forecasting. (\n",
      "\n",
      "and the CO2 emis-Forecasting emissions through Kaya identity using Neural ODEs sions from fuel combustion (IEA, 2020b) were extracted from public datasets from the International Energy Agency. It should be noted that this is not the total emission for each country - however, greenhouse gases emitted by fuel combustion represent around 75% of all greenhouse gases emissions (Ritchie & Roser, a). Each variable is available yearly, from 1971 to 2019, for at least 54 countries. 3.2. Motivations for a machine-learning approach In the domain of energy and emissions, forecasts are often relying on expert knowledge and statistical models (York et al., 2003; Auffhammer & Steinhauser, 2008); an exten- sive set of Integrated Assessment Models are used to ex- plore how system Earth evolves during the next decades, following speciﬁc scenarios (IPCC Fifth Assessment Report (2014)), (Lucas et al., 2015). In other ﬁelds, grey models (Deng, 1989) have been used successfully<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?\"\n",
    "query_vec = engine.embed_query(question)\n",
    "distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "top_docs = [split_docs[i] for i in indices[0]]\n",
    "context = \"\\n\\n\".join(top_docs)\n",
    "prompt_with_context = engine.tokenizer.apply_chat_template(\n",
    "    create_chat(context, question), \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "According to the text, the main indicators chosen to study the evolution of carbon emissions on a country-scale are:\n",
      "\n",
      "1. Population (P)\n",
      "2. Gross Domestic Product (GDP)\n",
      "3. Energy supply (E)\n",
      "4. CO2 emissions (F)\n",
      "\n",
      "These indicators were chosen because they are related to the Kaya identity, which expresses a simple relation between carbon emissions, energy supply, GDP, and population. The authors argue that the Kaya identity provides a clearer analysis from a macroscopic point of view, and that these indicators are actionable under the right choice of policies.\n",
      "\n",
      "In other words, the authors chose these indicators because they are closely related to the factors that influence carbon emissions, and they can be used to assess a country's efforts to reduce carbon emissions. The Kaya identity suggests that:\n",
      "\n",
      "F = P × G × E × F / G\n",
      "\n",
      "where F is carbon emissions, P is population, G is GDP, E is energy supply, and G is energy intensity (energy needed to produce one unit of GDP). By analyzing these indicators, the authors aim to understand the factors that contribute to carbon emissions and to develop a model that can forecast carbon emissions on a country-scale.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(engine.tokenizer, skip_prompt=True)\n",
    "\n",
    "input_ids = engine.tokenizer.encode(prompt_with_context, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = engine.decoder.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    pad_token_id=128004,\n",
    "    eos_token_id=128009,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_chat(context, question, answer):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{answer}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [01:32<00:00,  8.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "losses = list()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "    target_chat = engine.tokenizer.apply_chat_template(\n",
    "        create_target_chat(context, qa[\"question\"], qa[\"answer\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokens = engine.tokenizer(target_chat, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).to(DEVICE)\n",
    "    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    ignore_idx = engine.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).shape[1]\n",
    "\n",
    "    filtered_labels = tokens[\"input_ids\"].clone()\n",
    "    ignore_mask = torch.zeros_like(filtered_labels, dtype=torch.bool)\n",
    "    ignore_mask[0, :ignore_idx] = True\n",
    "    filtered_labels[ignore_mask] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = engine.decoder(**tokens, labels=filtered_labels)\n",
    "        losses.append(outputs.loss.item())\n",
    "        loss += outputs.loss.item()\n",
    "\n",
    "perplexity = math.exp(loss/len(docs))\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Average time per request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/747 [00:00<?, ?it/s]/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 747/747 [1:11:21<00:00,  5.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per request: 5.73 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = engine.tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    engine.decoder.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=128004,\n",
    "        eos_token_id=128009,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "print(f\"Average time per request: {(time.time() - begin) / len(docs):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
