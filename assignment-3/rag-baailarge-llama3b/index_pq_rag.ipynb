{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "DECODER_PATH='../Llama-3.2-3B-Instruct'\n",
    "ENCODER_PATH = \"../bge-large-en\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "QUESTIONS_PATH = \"./rag_questions_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 26.58it/s]\n"
     ]
    }
   ],
   "source": [
    "class RAGEngine:\n",
    "    def __init__(self):\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(DECODER_PATH, torch_dtype=torch.bfloat16).to(DEVICE)\n",
    "        self.encoder = SentenceTransformer(ENCODER_PATH).to(DEVICE)\n",
    "\n",
    "        self.decoder.config.use_cache = True\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(DECODER_PATH)\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        return self.encoder.encode(docs)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.encoder.encode([query])\n",
    "    \n",
    "engine = RAGEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 2174.59it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = [(fn.split(\".\")[0], open(os.path.join(DOCS_PATH, fn), 'r', encoding='utf-8').read()) for fn in tqdm(os.listdir(DOCS_PATH)) if fn.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 1066.36it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 4000\n",
    "MAX_CHAR_OVERLAP = 500\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=MAX_CHAR_LEN, chunk_overlap=MAX_CHAR_OVERLAP)\n",
    "split_docs = []\n",
    "for doc in tqdm(docs):\n",
    "    split_docs.extend(splitter.split_text(doc[1]) if len(doc[1]) > MAX_CHAR_LEN else [doc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = engine.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(context, question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_DOCS = 3\n",
    "D = doc_embeddings.shape[1]\n",
    "m = 8\n",
    "assert D % m == 0\n",
    "nbits = 5\n",
    "index = faiss.IndexPQ(D, m, nbits)\n",
    "index.train(doc_embeddings)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 20 Apr 2025\n",
      "\n",
      "Use only the following pieces of context to answer the question at the end. Different references are seperated by \"\n",
      "\n",
      "\". Please only use the references relevant to answer the question fto calculate emission reduction in a REDD+ project. A key concept of VM0007 is Reference Region for projecting rate of Deforestation (RRD), which is used as a control unit for estimating a baseline. An RRD is chosen so that the following variables are as close as possible to those of the Project Area (PA): deforestation drivers, landscape factors (e.g. forest types, soil types, elevation, etc.), and socio-economic variables (access to infrastructures, policies, and regulations, etc.). For the past 10–12 years before intervention, deforestation rates are aggregated over the RRD, and projected as a baseline for the crediting period. The projection method can be a simple historical average or a pre-deﬁned linear/non-linear model, where the former is often used (see Appendix for an example of RRD and baseline setting). Several studies have reported that baselines set under VM0007 were overestimated because they failed to consider a counterfactual scenario or to eliminate the effect of external factors, e.g., policy changes [ 2]. Causal inference for time-series data Synthetic Control Method (SCM) [ 7] is one of the most popular methods for causal inference with time-series data. This method is designed for a case with one treatment unit and multiple control units, which is suited for a REDD+ project setting. Given that an RRD consists of multiple sub-units (hereinafter “control units”), SCM ﬁnds an optimal weight to match both pre-intervention deforestation trends and covariates of the synthetic control (i.e. the weighted average of control units)\n",
      "\n",
      "al., 2010). 2 Data The main contribution of this study is the conversion of online news articles to meaningful variables that enhances our understanding of ETS. Therefore, we use GDELT, a free open platform covering global news from numerous countries in over 100 languages with daily frequency. The database includes, along with others, the actors, locations, organizations, themes, and sources of the news items (Leetaru and Schrodt, 2013). GDELT has been used in various articles that apply NLP to extract alternative information from the news (Alamro et al., 2019; Galla and Burke, 2018; Guidolin and Pedio, 2021). We take the daily futures closing prices of the European Union Allowance (EUA) ( €/ton) as the dependent variable since that is the underlying carbon price of ETS. Besides news data, we include the most fundamental drivers of ETS prices (Ye and Xue, 2021) in our analysis to serve as control variables. The data was collected from January 2, 2018 until November 30\n",
      "\n",
      "apply the proposed model to a REDD+ project in Brazil, and show that it might have had a small, positive effect but had been over-credited and that the 90% predictive interval of the ex-ante baseline included the ex-post baseline, implying our ex-ante estimation can work effectively. 1 Introduction Background Carbon credit is an incentive scheme to promote projects that have additional beneﬁts for climate change mitigation, and is expected to play an important role in offsetting the gap from net zero emission after reduction efforts [ 1]. Reducing deforestation and forest degradation are considered to be one of the most effective approaches to reduce carbon emission and REDD+ is a framework to promote such efforts through the issuance of carbon credit. However, carbon credits from REDD+ have been subject to several criticisms. Credits issued for projects without actual positive effects on climate change mitigation are called “junk carbon credit”, and several studies have showed that many REDD+ projects may have produced junk carbon credits [ 2]. Criticisms to carbon credit are mainly about the validity of baseline, i.e., a counterfactual scenario in the absence of a project. Considering this issue, the concept of dynamic baseline has recently been discussed [ 3,4]. In this framework, baseline is sequentially updated at every observation of the forest cover after intervention, allowing for the effects of changes in the external environment to be taken into account. Ex-post approach, e.g., the use of synthetic control method (SCM), has been investigated in this context [ 2]. However, there still remain a ﬁnancing issue since result-based payment requires several years for project proponents to wait until they obtain the ﬁrst credit issuance. From investor’s perspective, ex- ante baseline projection is needed to quantify the risk of projects for their investment decision [ 5]. With those in mind, we can ﬁnd a need for the integration of both ex-ante baseline prediction before intervention and ex-post dynamic baseline updating at each observation after intervention. Summary of our contributions We propose a new model for solving the issue mentioned above. First, we introduce a Bayesian state-space model that naturally integrates the forecast of defor- estation baseline before intervention and the dynamic updating of baseline after intervention. We achieve this by combining state-space modeling for forecasting and SCM for dynamic updating. Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022. ∗sustainacraft Inc., Tokyo, Japan. †National Institute for Environmental Studies, Ibaraki, Japan. ‡Center for the Promotion of Social Data Science Education and Research, Hitotsubashi University, Tokyo, Japan. Email: shinichiro.shirota@gmail.comSecond, we consider covariate balancing in state-space modeling by using the method of general Bayesian updating for a valid causal inference. Finally, we apply the proposed model to a REDD+ project in Brazil and show that both ex-ante and ex-post baseline by our model can work effectively. Our approach would enable appropriate ex-ante risk assessment and ex-post performance evaluation of forest conservation projects, and contribute to the sound allocation of funds to projects that have signiﬁcant positive impacts to climate change action. 2 Preliminaries and Related Work VM0007: A REDD+ methodology for emission reduction evaluation VM0007 [ 6] is one of the major methodologies that deﬁne how to calculate emission reduction in a REDD+ project. A key concept of VM0007 is Reference Region for projecting rate of Deforestation (RRD), which is used as a control unit for estimating a baseline. An RRD is chosen so that the following variables are as close as possible to those of the Project Area (PA): deforestation drivers, landscape factors (e.g. forest types, soil types, elevation, etc.), and socio-economic variables (access to infrastructures, policies, and regulations, etc.). For the<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?\"\n",
    "query_vec = engine.embed_query(question)\n",
    "distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "top_docs = [split_docs[i] for i in indices[0]]\n",
    "context = \"\\n\\n\".join(top_docs)\n",
    "prompt_with_context = engine.tokenizer.apply_chat_template(\n",
    "    create_chat(context, question), \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main indicators chosen to study the evolution of carbon emissions on a country-scale, in order to understand and forecast the evolution of carbon emissions, are:\n",
      "\n",
      "1. Deforestation rates \n",
      "2. Daily futures closing prices of the European Union Allowance (EUA) \n",
      "3. News data from GDELT, a free open platform covering global news from numerous countries in over 100 languages with daily frequency.\n",
      "\n",
      "These indicators were chosen because:\n",
      "\n",
      "* Deforestation rates are a key driver of carbon emissions, and studying them can provide insights into the effectiveness of conservation efforts and land-use changes.\n",
      "* The daily futures closing prices of the European Union Allowance (EUA) are a fundamental driver of the European Emissions Trading System (EU ETS), which is a key mechanism for reducing greenhouse gas emissions in the EU. Analyzing these prices can provide insights into market trends and volatility, which can inform forecasting of carbon emissions.\n",
      "* News data from GDELT provides information on global news trends and events that can impact carbon emissions, such as changes in government policies, natural disasters, or economic trends. This can help identify potential drivers of changes in carbon emissions and inform forecasting models.\n",
      "\n",
      "These indicators were chosen because they are relevant to understanding and forecasting carbon emissions, and they are also readily available and accessible.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(engine.tokenizer, skip_prompt=True)\n",
    "\n",
    "input_ids = engine.tokenizer.encode(prompt_with_context, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = engine.decoder.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    pad_token_id=128004,\n",
    "    eos_token_id=128009,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_chat(context, question, answer):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{answer}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [01:26<00:00,  8.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 8.80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "losses = list()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "    target_chat = engine.tokenizer.apply_chat_template(\n",
    "        create_target_chat(context, qa[\"question\"], qa[\"answer\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokens = engine.tokenizer(target_chat, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).to(DEVICE)\n",
    "    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    ignore_idx = engine.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).shape[1]\n",
    "\n",
    "    filtered_labels = tokens[\"input_ids\"].clone()\n",
    "    ignore_mask = torch.zeros_like(filtered_labels, dtype=torch.bool)\n",
    "    ignore_mask[0, :ignore_idx] = True\n",
    "    filtered_labels[ignore_mask] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = engine.decoder(**tokens, labels=filtered_labels)\n",
    "        losses.append(outputs.loss.item())\n",
    "        loss += outputs.loss.item()\n",
    "\n",
    "perplexity = math.exp(loss/len(docs))\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Average time per request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/747 [00:00<?, ?it/s]/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 747/747 [1:17:55<00:00,  6.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per request: 6.26 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = engine.tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    engine.decoder.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=128004,\n",
    "        eos_token_id=128009,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "print(f\"Average time per request: {(time.time() - begin) / len(docs):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
