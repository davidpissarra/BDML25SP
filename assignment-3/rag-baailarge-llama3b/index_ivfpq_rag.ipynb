{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "DECODER_PATH='../Llama-3.2-3B-Instruct'\n",
    "ENCODER_PATH = \"../bge-large-en\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "QUESTIONS_PATH = \"./rag_questions_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 27.69it/s]\n"
     ]
    }
   ],
   "source": [
    "class RAGEngine:\n",
    "    def __init__(self):\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(DECODER_PATH, torch_dtype=torch.bfloat16).to(DEVICE)\n",
    "        self.encoder = SentenceTransformer(ENCODER_PATH).to(DEVICE)\n",
    "\n",
    "        self.decoder.config.use_cache = True\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(DECODER_PATH)\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        return self.encoder.encode(docs)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.encoder.encode([query])\n",
    "    \n",
    "engine = RAGEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 2357.30it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = [(fn.split(\".\")[0], open(os.path.join(DOCS_PATH, fn), 'r', encoding='utf-8').read()) for fn in tqdm(os.listdir(DOCS_PATH)) if fn.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 1057.80it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 4000\n",
    "MAX_CHAR_OVERLAP = 500\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=MAX_CHAR_LEN, chunk_overlap=MAX_CHAR_OVERLAP)\n",
    "split_docs = []\n",
    "for doc in tqdm(docs):\n",
    "    split_docs.extend(splitter.split_text(doc[1]) if len(doc[1]) > MAX_CHAR_LEN else [doc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = engine.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(context, question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_DOCS = 3\n",
    "D = doc_embeddings.shape[1]\n",
    "m = 8\n",
    "assert D % m == 0\n",
    "nlist = 2**5\n",
    "nbits = 5\n",
    "quantizer = faiss.IndexFlatIP(D)\n",
    "index = faiss.IndexIVFPQ(quantizer, D, nlist, m, nbits)\n",
    "index.train(doc_embeddings)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 20 Apr 2025\n",
      "\n",
      "Use only the following pieces of context to answer the question at the end. Different references are seperated by \"\n",
      "\n",
      "\". Please only use the references relevant to answer the question fresults and give relevant insight to policymakers. 1. Introduction Lowering human greenhouse gases emissions is one major goal of the efforts against climate change, and the focus and concern of international cooperation (Paris Agreement, 2015). Many indicators of human development - population, Gross Domestic Product (GDP), environmental footprint - have been following exponential curves during the past decades (Steffen et al., 2015); hence, drastic measures are needed if we are to switch from increasing to quickly de- creasing emissions, as expressed in global organisations goals (IPCC Fifth Assessment Report (2014)). Understanding and forecasting the evolution, on a country- scale, of various indicators related to carbon emissions, may help to give a clear idea of the progress we are making, or not, towards lower emissions. The main indicators that we chose to study are the variables appearing in Kaya identity (Kaya & Yokoburi, 1997), on a country level: population, national GDP, energy supply and CO2 emissions. Our main objective is to develop a model able to use this data to make accurate forecasts, on a medium/long-time horizon. Machine-learning models offer interesting advantages in comparison of traditional methods used for this type of *Equal contribution1Department of Computing, Impe- rial College, London, United Kingdom2Aquatic Informat- ics, Vancouver, Canada. Correspondence to: Pierre Browne <pierre.browne20@imperial.ac.uk >. Tackling Climate Change with Machine Learning Workshop at ICML 2021.work, typically statistical models (Cerqueira et al., 2019). In particular, the recent development of Neural Ordinary Differential Equations offers a promising perspective in this case (Chen et al., 2019) - we explain the reasons for this choice in 3.2. We adapted Neural ODEs for this problem and compared the performance with a baseline statistical model. 2. Kaya identity & impact on climate change The Kaya identity expresses a very simple relation between carbon emissions F, energy supply E, Gross Domestic Prod- uct (GDP) Gand population P:F=P\u0002G P\u0002E G\u0002F E. G Pis the GDP per capita, representing our average life stan- dard, which we generally want to increase;E Gis the energy intensity of the GDP - the energy needed to create one unit of GDP -, it quantiﬁes the efﬁciency of energy usage in our activities;E Fis the carbon intensity of energy - the CO2 emission corresponding to the supply of one unit of primary energy - and indicates how reliant on carbon emissions our electricity production is. Forecasting human development and carbon emissions is possible both with the set of raw variables fP; G; E; Fg and with the set of indicators appearing in the Kaya identity fP;G P;E G;F Eg. However, the latter gives a clearer analy- sis, from a macroscopic point of view (Ritchie & Roser, b). While the raw variables are very strongly correlated altogether and vary greatly between countries, the variables from the Kaya equation look more like consistent indica- tors actionable under the right choice of policies (Hwang et al., 2020). Overall, using these four indicators seems to be a good choice in order to assess efforts made by a coun- try or region concerning carbon emissions ( ˇStreimikien ˙e & Balezentis, 2016). 3. Methodology 3.1. Datasets Four datasets were used for this study: datasets for popula- tion (World Bank, 2020b) and GDP (World Bank, 2020a) were collected from the World Bank Open Data platform; the total energy supply (IEA, 2020a) and the CO2 emis-Forecasting emissions through Kaya identity using Neural ODEs sions from fuel combustion (IEA, 2020b) were extracted from public datasets from the International Energy Agency. It should be noted that this is not the total emission for each country - however, greenhouse gases emitted by fuel combustion represent around 75% of all greenhouse gases emissions (Ritchie & Roser, a). Each variable is available yearly, from 1971 to 2019, for at least 54 countries. 3.2. Motivations for a\n",
      "\n",
      "ally informative about the model parameters. We deﬁne a set of 18 actions and collect a dataset of 2183 comparisons from 176 users on a university campus. The early results reveal promising directions to improve climate communication and enhance climate mitigation. 1 Introduction To put the focus on actions that have high potential for emission reduction, we must ﬁrst understand whether people have an accurate perception of the carbon footprint of these actions. If they do not, their efforts might be wasted. As an example, recent work by Wynes and Nicholas [6]shows that Canadian high-school textbooks encourage daily actions that yield negligible emission reduction. Actions with a higher potential of emission reduction are poorly documented. In this work, we model how people perceive the carbon footprint of their actions, which could guide educators and policy-makers. In their daily life, consumers repeatedly face multiple options with varying environmental effects. Except for a handful of experts, no one is able to estimate the absolute quantity of CO 2emitted by their actions of say, ﬂying from Paris to London. Most people, however, are aware that taking the train for the same trip would release less CO 2. Hence, in the spirit of Thurstone [5]and Salganik and Levy [4](among many others), we posit that the perception of a population can be probed by simple pairwise comparisons. By doing so, we shift the complexity from the probing system to the model: Instead of asking difﬁcult questions about each action and simply averaging the answers, we ask simple questions in the form of comparisons and design a non-trivial model to estimate the perception. In ﬁne, human behaviour boils down to making choices: For example, we choose between eating local food and eating imported food; we do not choose between eating or not eating. Our awareness of relative emissions between actions (of the same purpose) is often sufﬁcient to improve our carbon footprint. Our contributions are as follows. First, we cast the problem of inferring a population’s global perception from pairwise comparisons as a linear regression. Second, we adapt a well-known active- learning method to maximize the information gained from each comparison. We describe the model and the active-learning algorithm in Section 2. We design an interactive platform to collect real data for an experiment on our university campus, and we show early results in Section 3. Our approach could help climate scientists, sociologists, journalists, governments, and individuals improve climate communication and enhance climate mitigation. 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.2 Model LetAbe a set ofMactions. For instance, \"ﬂying from London to New York\" or \"eating meat for a year\" are both actions in A. Let (i;j;y )be a triplet encoding that action i2A has an impact ratio of y2R>0over actionj2A. Said otherwise, if y>1, actionihas a carbon footprint ytimes greater than actionj, and ify<1, actionihas a carbon footprint 1=ytimes smaller than actionj. Given some parameters wi;wj2Rrepresenting the perceived (log-)carbon footprint in CO 2- equivalent of action iand actionj, we posit y=expwi expwj: We gather the parameters in a vector w2RM. Assuming a centered Gaussian noise \u000f\u0018N(0;\u001b2 n), \u001b2 n2R, we model the (log-)impact ratio logy=wi\u0000wj+\u000f=x|w+\u000f; (1) where the comparison vector x2RMis zero everywhere except in entry iwhere it is +1and in entryjwhere it is\u00001. Vectorx\"selects\" the pair of actions to compare. For a dataset D=f(in;jn;yn) :n= 1;:::;NgofNindependent triplets and since logy\u0018N(x|w;\u001b2 n), the likelihood of the model is p(yjX;w) =NY i=1p(yijx| iw;\u001b2 n) =N(Xw;\u001b2 nI); wherey2RNis the vector of observed (log-)impact ratios, and X2RN\u0002Mis a matrix of N comparison vectors. We assume a Gaussian prior for the weight parameters w\u0018N (\u0016;\u0006p), where\u00162RMis the prior mean and \u0006p2RM\u0002Mis the prior covariance matrix. To obtain the global perceived carbon\n",
      "\n",
      ", climate change and environment degradation. Investment in raising efficiency of appliances is among the most significant attempts to save energy. Ironically, introduction of many such energy saving appliances increased the total energy consumption instead of reducing it. This effect in literature is attributed to the inherent Jevons paradox (JP) and optimism bias (OB) in consumer behavior. However, the magnitude of these instincts vary among different people. Identification of this magnitude for each household can enable the development of appropriate policies that induce desired energy saving behaviour. Using the RECS 2015 dataset, the paper uses machine learning for each electrical appliance to determine the dependence of their total energy consumption on their energy star rating. This shows that only substitutable appliances register increase in energy demand upon boosted efficiency. Lastly, an index is noted to indicate the varying influence of JP and OB on different households. 1 Introduction Climate change is one of the greatest environmental challenge in today’s era. Increasing amount of greenhouse gas (GHG) emissions is among the crucial factors for this deterioration [ 1]. According to a 2021 report by the Intergovernmental Panel on Climate Change (IPCC) [ 2], there has been a significant rise in the concentration of GHG emissions since the pre-industrial period. Increased usage of energy, even in the form of electrical energy, has been one of the most contributing factors for the rise in GHG concentrations in the atmosphere [ 3]. Consequently, a vast amount of research has been conducted to engineer energy efficient technologies and consumer goods [4]. However, time and again, previous literature has shown a rebound effect of introducing energy efficient technologies. This effect is much more significant at the level of end consumers of such technologies. Herring and Roy [ 5] have shown that technological advancements modify the lifestyle and attitude of people. They further state that even if the product in itself consumes less energy per ∗Authors Contributed Equally †Corresponding Author Tackling Climate Change with Machine Learning: workshop at NeurIPS 2022.unit time, an increased usage of it by the consumers nullifies/reduces the benefits. This is in coherence with the well-known Jevons paradox (JP), formulated by Stanley Jevons in 1865 [6]. It diminishes the expected benefit from the newer technology. Apart from JP, human psychology also plays a crucial role in defining their behavior/response to any change. Several researchers in the domain have claimed that people possess optimism bias (OB) [ 7,8]. It is a tendency of humans to underestimate negative events for themselves or from their actions. This raises a possibility that post adoption of energy efficient appliance, consumers might start to further underestimate the environmental impact of consuming the appliance and thereby increasing their demand for electrical energy. In short, it is deemed that people consume limited electrical energy primarily because of two main factors, namely, 1) monetary cost of electricity, and 2) environmental impact. Since the adoption of energy efficient technologies is expected to reduce both of these factors, electricity demand might increase corresponding to the effects of JP and OB, respectively. This intuition is validated at a macro-level in previous studies. Greening et al. [9] consolidated the results from previous research to identify the behavioural influences upon introduction of appliances with higher energy efficiency. The results showed a net increase in the electricity demand. In another national level study, Frondel [ 10] showed that the introduction of a more efficient technology influence the behavior of consumers for worse. More recently, Li et al. [11] showed that without proper policies, technological progress does not reduce energy consumption. Focusing on regional data, they further identified<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?\"\n",
    "query_vec = engine.embed_query(question)\n",
    "distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "top_docs = [split_docs[i] for i in indices[0]]\n",
    "context = \"\\n\\n\".join(top_docs)\n",
    "prompt_with_context = engine.tokenizer.apply_chat_template(\n",
    "    create_chat(context, question), \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main indicators chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale are:\n",
      "\n",
      "1. Population (P)\n",
      "2. National GDP (G)\n",
      "3. Energy supply (E)\n",
      "4. CO2 emissions (F)\n",
      "\n",
      "These indicators were chosen because they are related to the Kaya identity, which expresses a simple relation between carbon emissions, energy supply, GDP, and population. The Kaya identity is:\n",
      "\n",
      "F = P x G x E^(-1)\n",
      "\n",
      "where F is the carbon emissions, P is the population, G is the GDP, and E is the energy intensity of the GDP (i.e., the energy needed to produce one unit of GDP).\n",
      "\n",
      "The authors chose these indicators because they are considered to be actionable under the right choice of policies, and they provide a clearer analysis from a macroscopic point of view. They also noted that the raw variables (population, GDP, energy supply, and CO2 emissions) are strongly correlated with each other, but the variables from the Kaya identity are more indicative of the country's efforts to reduce carbon emissions.\n",
      "\n",
      "In particular, the authors chose these indicators because they can be easily measured and collected, and they are relevant to understanding the relationship between carbon emissions and economic and demographic factors. By studying these indicators, policymakers can gain insights into the effectiveness of their efforts to reduce carbon emissions and identify areas for improvement.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(engine.tokenizer, skip_prompt=True)\n",
    "\n",
    "input_ids = engine.tokenizer.encode(prompt_with_context, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = engine.decoder.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    pad_token_id=128004,\n",
    "    eos_token_id=128009,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_chat(context, question, answer):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{answer}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [01:19<00:00,  9.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 7.82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "losses = list()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "    target_chat = engine.tokenizer.apply_chat_template(\n",
    "        create_target_chat(context, qa[\"question\"], qa[\"answer\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokens = engine.tokenizer(target_chat, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).to(DEVICE)\n",
    "    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    ignore_idx = engine.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).shape[1]\n",
    "\n",
    "    filtered_labels = tokens[\"input_ids\"].clone()\n",
    "    ignore_mask = torch.zeros_like(filtered_labels, dtype=torch.bool)\n",
    "    ignore_mask[0, :ignore_idx] = True\n",
    "    filtered_labels[ignore_mask] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = engine.decoder(**tokens, labels=filtered_labels)\n",
    "        losses.append(outputs.loss.item())\n",
    "        loss += outputs.loss.item()\n",
    "\n",
    "perplexity = math.exp(loss/len(docs))\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Average time per request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/747 [00:00<?, ?it/s]/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 747/747 [1:13:00<00:00,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per request: 5.86 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = engine.tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    engine.decoder.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=128004,\n",
    "        eos_token_id=128009,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "print(f\"Average time per request: {(time.time() - begin) / len(docs):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
