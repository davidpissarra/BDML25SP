{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch\n",
    "import faiss\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import os\n",
    "\n",
    "DECODER_PATH='../Llama-3.2-3B-Instruct'\n",
    "ENCODER_PATH = \"../bge-small-en\"\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DOCS_PATH = \"../dataset_txt_small/train\"\n",
    "QUESTIONS_PATH = \"./rag_questions_json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00, 29.06it/s]\n"
     ]
    }
   ],
   "source": [
    "class RAGEngine:\n",
    "    def __init__(self):\n",
    "        self.decoder = AutoModelForCausalLM.from_pretrained(DECODER_PATH, torch_dtype=torch.bfloat16).to(DEVICE)\n",
    "        self.encoder = SentenceTransformer(ENCODER_PATH).to(DEVICE)\n",
    "\n",
    "        self.decoder.config.use_cache = True\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(DECODER_PATH)\n",
    "\n",
    "    def embed_documents(self, docs):\n",
    "        return self.encoder.encode(docs)\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return self.encoder.encode([query])\n",
    "    \n",
    "engine = RAGEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 2455.75it/s]\n"
     ]
    }
   ],
   "source": [
    "docs = [(fn.split(\".\")[0], open(os.path.join(DOCS_PATH, fn), 'r', encoding='utf-8').read()) for fn in tqdm(os.listdir(DOCS_PATH)) if fn.endswith(\".txt\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [00:00<00:00, 1042.91it/s]\n"
     ]
    }
   ],
   "source": [
    "MAX_CHAR_LEN = 4000\n",
    "MAX_CHAR_OVERLAP = 500\n",
    "splitter = CharacterTextSplitter(separator=\" \", chunk_size=MAX_CHAR_LEN, chunk_overlap=MAX_CHAR_OVERLAP)\n",
    "split_docs = []\n",
    "for doc in tqdm(docs):\n",
    "    split_docs.extend(splitter.split_text(doc[1]) if len(doc[1]) > MAX_CHAR_LEN else [doc[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_embeddings = engine.embed_documents(split_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chat(context, question):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K_DOCS = 3\n",
    "D = doc_embeddings.shape[1]\n",
    "M = 32\n",
    "index = faiss.IndexHNSWFlat(D, M)\n",
    "index.add(doc_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
      "\n",
      "Cutting Knowledge Date: December 2023\n",
      "Today Date: 21 Apr 2025\n",
      "\n",
      "Use only the following pieces of context to answer the question at the end. Different references are seperated by \"\n",
      "\n",
      "\". Please only use the references relevant to answer the question fresults and give relevant insight to policymakers. 1. Introduction Lowering human greenhouse gases emissions is one major goal of the efforts against climate change, and the focus and concern of international cooperation (Paris Agreement, 2015). Many indicators of human development - population, Gross Domestic Product (GDP), environmental footprint - have been following exponential curves during the past decades (Steffen et al., 2015); hence, drastic measures are needed if we are to switch from increasing to quickly de- creasing emissions, as expressed in global organisations goals (IPCC Fifth Assessment Report (2014)). Understanding and forecasting the evolution, on a country- scale, of various indicators related to carbon emissions, may help to give a clear idea of the progress we are making, or not, towards lower emissions. The main indicators that we chose to study are the variables appearing in Kaya identity (Kaya & Yokoburi, 1997), on a country level: population, national GDP, energy supply and CO2 emissions. Our main objective is to develop a model able to use this data to make accurate forecasts, on a medium/long-time horizon. Machine-learning models offer interesting advantages in comparison of traditional methods used for this type of *Equal contribution1Department of Computing, Impe- rial College, London, United Kingdom2Aquatic Informat- ics, Vancouver, Canada. Correspondence to: Pierre Browne <pierre.browne20@imperial.ac.uk >. Tackling Climate Change with Machine Learning Workshop at ICML 2021.work, typically statistical models (Cerqueira et al., 2019). In particular, the recent development of Neural Ordinary Differential Equations offers a promising perspective in this case (Chen et al., 2019) - we explain the reasons for this choice in 3.2. We adapted Neural ODEs for this problem and compared the performance with a baseline statistical model. 2. Kaya identity & impact on climate change The Kaya identity expresses a very simple relation between carbon emissions F, energy supply E, Gross Domestic Prod- uct (GDP) Gand population P:F=P\u0002G P\u0002E G\u0002F E. G Pis the GDP per capita, representing our average life stan- dard, which we generally want to increase;E Gis the energy intensity of the GDP - the energy needed to create one unit of GDP -, it quantiﬁes the efﬁciency of energy usage in our activities;E Fis the carbon intensity of energy - the CO2 emission corresponding to the supply of one unit of primary energy - and indicates how reliant on carbon emissions our electricity production is. Forecasting human development and carbon emissions is possible both with the set of raw variables fP; G; E; Fg and with the set of indicators appearing in the Kaya identity fP;G P;E G;F Eg. However, the latter gives a clearer analy- sis, from a macroscopic point of view (Ritchie & Roser, b). While the raw variables are very strongly correlated altogether and vary greatly between countries, the variables from the Kaya equation look more like consistent indica- tors actionable under the right choice of policies (Hwang et al., 2020). Overall, using these four indicators seems to be a good choice in order to assess efforts made by a coun- try or region concerning carbon emissions ( ˇStreimikien ˙e & Balezentis, 2016). 3. Methodology 3.1. Datasets Four datasets were used for this study: datasets for popula- tion (World Bank, 2020b) and GDP (World Bank, 2020a) were collected from the World Bank Open Data platform; the total energy supply (IEA, 2020a) and the CO2 emis-Forecasting emissions through Kaya identity using Neural ODEs sions from fuel combustion (IEA, 2020b) were extracted from public datasets from the International Energy Agency. It should be noted that this is not the total emission for each country - however, greenhouse gases emitted by fuel combustion represent around 75% of all greenhouse gases emissions (Ritchie & Roser, a). Each variable is available yearly, from 1971 to 2019, for at least 54 countries. 3.2. Motivations for a\n",
      "\n",
      "ability for various downstream prediction tasks. It can predict daily emissions for unseen countries where direct emissions data is unavailable by using accessible power generation data. CLEAN supports bidirectional prediction for policy simulation. The model is designed to help determine the optimal energy mix required to meet specific emission reduction targets, automatically identifying a feasible energy structure. This capability is crucial for developing sustainable energy policies and ensuring that emission goals are both realistic and achievable. 3 Carbon Monitor Energy-Emission Time Series Data Table 1: Detail Information of Carbon Monitor Energy-Emission (CMEE) Time Series Dataset. Characteristics Number Description Spatial Coverage 12 Regions (e.g. China, France, United States) Temporal Coverage 2008 Daily granularity from 01/01/2019 to 30/06/2024 Energy Type 8 Energy source (e.g. Coal, Gas, Solar) Emission Type 6 Secotral emission (e.g. Transportation, Industry, Power) We have developed a comprehensive, high-resolution Energy-Emission time series dataset CMEE based on the Carbon Monitor project to understand the relationship between power generation and CO2emissions, which is crucial for evaluating national energy and sectorial transition policies. The Carbon Monitor is an international initiative providing regularly updated, science-based estimates of daily carbon emissions and power generation data for 12 countries, covering the period from January 1, 2019, to June 30, 2024 [ 13,10]. Our dataset includes data on eight types of energy sources and six categories of sectoral emissions, spanning a total of 24,096 days (2,008 days each for 12 countries). It integrates two independently collected but related datasets, ensuring consistency through structured formatting and noise reduction techniques such as Gaussian filtering. This unified dataset serves as a robust foundation for machine learning tasks and supports further research, including data mining, causal analysis, and time series forecasting. (\n",
      "\n",
      "and active role in environmental sus- tainability. In this paper, we take analyses usually applied at the industrial level and make them accessible for individual computer science researchers with an easy-to-use Python package. Localizing to the energy mixture of the electrical power grid, we make the conversion from energy usage to CO 2emissions, in addition to contextualizing these results with more human-understandable bench- marks, such as automobile miles driven. We also include comparisons with energy mixtures employed in electrical grids around the world. We propose including these automatically-generated Energy Usage Reports as part of standard algorith- mic accountability practices, and demonstrate the use of these reports as part of model-choice in a machine learning context. 1 Introduction Anthropogenic climate change is a global environmental problem caused by human-induced changes to the global carbon cycle through emissions of greenhouse gases, particularly carbon dioxide (CO 2) (IPCC (2013); Myhre et al.(2013)). Carbon dioxide emissions change the Earth’s energy balance: a higher concentration of CO 2in the atmosphere increases the amount of longwave radiation absorbed by the atmosphere and reradiated to the surface, increasing global average surface temperatures and altering long-term climatic trends (Stocker et al.(2013); Myhre et al.(2013)). In most industrialized countries, the energy sector is one of the top contributors to national CO 2emissions (Myhre et al. (2013)). Minimizing carbon emissions through the use of increasingly efﬁcient computational methods is an appropriate response to climate change under the framework of Environmental Justice (Denton et al.(2014)). Computer scientists could play a central role in reducing carbon emissions economy-wide through the design and implementation of more energy-efﬁcient algorithms. There have been many attempts to measure the carbon footprint of computing (Strubell et al.(2019); Muhammad Arif (2015); Posani (2018); Ensmenger (2018, 2015); Coroama et al.(2015)), with special attention paid to energy-intensive operations such as Bitcoin mining and cloud computing (de Vries (2018); Baliga et al.(2011); Muhammad Arif (2015); Posani (2018)). However, the details of electricity generation and power consumption make it difﬁcult to translate from an individual device’s operations to the energy use and carbon emissions which result from those operations. \u0003This work is funded in part by the Mozilla Responsible Computer Science Challenge and by the NSF under grant IIS-1633387. yBoth authors contributed equally to this research. Workshop on Tackling Climate Change with Machine Learning at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada.Energy Usage Report Energy usage and CO 2emissions for the function expwith input 10. Energy Usage Readings Average baseline wattage: 2.35 watts Average total wattage: 15.53 watts Average process wattage: 13.18 watts Process duration: 0:16:40Energy Mix Data Total kilowatt hours used: 0.00367 kWh Effective emissions :1.78e-03 kg CO 2 Assumed Carbon Equivalencies Coal: 996 kg CO 2/MWh Oil: 817 kg CO 2/MWh Natural gas: 744 kg CO 2/MWh Low carbon: 0 kg CO 2/MWhCO 2Emissions Equivalents Miles driven: 7.26 e-10 mi Min. of 32-in. LCD TV: 1.10 min. % of CO 2per US house/day: 5.84 e-10% Emission Comparisons CO2emissions for the function if the computation had been performed elsewhere. Figure 1: An example Energy Usage Report for a simple exponential function (code in Figure 5). In this paper, we introduce a Python package energyusage3that can calculate the energy and CO 2 emissions of a given function as well as output an Energy Usage Report giving context to these results. We argue that any attempts to use algorithms to tackle climate change should also report on the algorithms’ direct emissions impacts. 2 Energy Usage Report When considering the environmental impact of algorithms, it is generally<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
      "\n",
      "What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "question = \"What are the main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale, and why were they chosen?\"\n",
    "query_vec = engine.embed_query(question)\n",
    "distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "top_docs = [split_docs[i] for i in indices[0]]\n",
    "context = \"\\n\\n\".join(top_docs)\n",
    "prompt_with_context = engine.tokenizer.apply_chat_template(\n",
    "    create_chat(context, question), \n",
    "    tokenize=False, \n",
    "    add_generation_prompt=True\n",
    ")\n",
    "print(prompt_with_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main indicators that were chosen to study in order to understand and forecast the evolution of carbon emissions on a country-scale are:\n",
      "\n",
      "1. Population (P)\n",
      "2. National GDP (G)\n",
      "3. Energy supply (E)\n",
      "4. CO2 emissions (F)\n",
      "\n",
      "These indicators were chosen because they are all related to carbon emissions through the Kaya identity, which is a mathematical relationship that describes the relationship between carbon emissions, energy supply, GDP, and population.\n",
      "\n",
      "The Kaya identity is expressed as:\n",
      "\n",
      "F = P x G x E^G x F/E\n",
      "\n",
      "Where:\n",
      "\n",
      "* F is carbon emissions\n",
      "* P is population\n",
      "* G is GDP\n",
      "* E is energy supply\n",
      "* G is GDP per capita (which represents the average life standard)\n",
      "* E is energy intensity of the GDP (the energy needed to create one unit of GDP)\n",
      "* F is the carbon intensity of energy (the CO2 emission corresponding to the supply of one unit of primary energy)\n",
      "\n",
      "These four indicators were chosen because they provide a clear and actionable relationship between carbon emissions and other macroeconomic and demographic factors, making them useful for assessing efforts made by a country or region concerning carbon emissions.<|eot_id|>\n"
     ]
    }
   ],
   "source": [
    "streamer = TextStreamer(engine.tokenizer, skip_prompt=True)\n",
    "\n",
    "input_ids = engine.tokenizer.encode(prompt_with_context, return_tensors=\"pt\").to(DEVICE)\n",
    "outputs = engine.decoder.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=500,\n",
    "    pad_token_id=128004,\n",
    "    eos_token_id=128009,\n",
    "    streamer=streamer,\n",
    "    do_sample=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_chat(context, question, answer):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": f\"Use only the following pieces of context to answer the question at the end. Different references are seperated by \\\"\\n\\n\\\". Please only use the references relevant to answer the question f{context}\"},\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "        {\"role\": \"assistant\", \"content\": f\"{answer}\"},\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 747/747 [01:27<00:00,  8.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 2.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss = 0\n",
    "losses = list()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "    target_chat = engine.tokenizer.apply_chat_template(\n",
    "        create_target_chat(context, qa[\"question\"], qa[\"answer\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=False\n",
    "    )\n",
    "    tokens = engine.tokenizer(target_chat, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).to(DEVICE)\n",
    "    tokens = {k: v.to(DEVICE) for k, v in tokens.items()}\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    ignore_idx = engine.tokenizer.encode(prompt, return_tensors=\"pt\", truncation=False, padding=False, add_special_tokens=False).shape[1]\n",
    "\n",
    "    filtered_labels = tokens[\"input_ids\"].clone()\n",
    "    ignore_mask = torch.zeros_like(filtered_labels, dtype=torch.bool)\n",
    "    ignore_mask[0, :ignore_idx] = True\n",
    "    filtered_labels[ignore_mask] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = engine.decoder(**tokens, labels=filtered_labels)\n",
    "        losses.append(outputs.loss.item())\n",
    "        loss += outputs.loss.item()\n",
    "\n",
    "perplexity = math.exp(loss/len(docs))\n",
    "print(f\"Perplexity: {perplexity:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (Average time per request)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/747 [00:00<?, ?it/s]/home/ddp8196/miniconda3/envs/bdml/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "100%|██████████| 747/747 [1:12:05<00:00,  5.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average time per request: 5.79 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "begin = time.time()\n",
    "\n",
    "for d in tqdm(docs):\n",
    "    with open(os.path.join(QUESTIONS_PATH, f\"{d[0]}.json\"), 'r', encoding='utf-8') as f:\n",
    "        qa = json.load(f)\n",
    "\n",
    "    query_vec = engine.embed_query(qa[\"question\"])\n",
    "    distances, indices = index.search(query_vec, TOP_K_DOCS)\n",
    "    top_docs = [split_docs[i] for i in indices[0]]\n",
    "    context = \"\\n\\n\".join(top_docs)\n",
    "\n",
    "    prompt = engine.tokenizer.apply_chat_template(\n",
    "        create_chat(context, qa[\"question\"]), \n",
    "        tokenize=False, \n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    input_ids = engine.tokenizer.encode(prompt, return_tensors=\"pt\").to(DEVICE)\n",
    "    engine.decoder.generate(\n",
    "        input_ids,\n",
    "        max_new_tokens=500,\n",
    "        pad_token_id=128004,\n",
    "        eos_token_id=128009,\n",
    "        do_sample=False,\n",
    "        top_p=1.0,\n",
    "    )\n",
    "\n",
    "print(f\"Average time per request: {(time.time() - begin) / len(docs):.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bdml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
