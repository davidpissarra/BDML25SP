{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 830/830 [00:00<00:00, 3088.52it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import PyPDF2\n",
    "from tqdm import tqdm\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def process_pdfs(folder_path, output_folder):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    for filename in tqdm(os.listdir(folder_path)):\n",
    "        if filename.endswith(\".pdf\") and not os.path.exists(os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")):\n",
    "            pdf_path = os.path.join(folder_path, filename)\n",
    "            text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "            output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            with open(output_path, \"w\", encoding=\"utf-8\") as output_file:\n",
    "                output_file.write(text)\n",
    "        elif not filename.endswith(\".pdf\"):\n",
    "            print(f\"file type not supported ({filename})\")\n",
    "\n",
    "input_folder = \"./climate_text_dataset\"\n",
    "output_folder = \"./dataset_txt_small\"\n",
    "\n",
    "process_pdfs(input_folder, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "SPLIT_PCT = 0.9\n",
    "\n",
    "ds_txt = os.listdir(output_folder)\n",
    "random.shuffle(ds_txt)\n",
    "\n",
    "if not os.path.exists(os.path.join(output_folder, \"train\")):\n",
    "    os.makedirs(os.path.join(output_folder, \"train\"))\n",
    "if not os.path.exists(os.path.join(output_folder, \"test\")):\n",
    "    os.makedirs(os.path.join(output_folder, \"test\"))\n",
    "\n",
    "for filename in ds_txt[:int(len(ds_txt) * SPLIT_PCT)]:\n",
    "    shutil.move(os.path.join(output_folder, filename), os.path.join(output_folder, \"train\", filename))\n",
    "for filename in ds_txt[int(len(ds_txt) * SPLIT_PCT):]:\n",
    "    shutil.move(os.path.join(output_folder, filename), os.path.join(output_folder, \"test\", filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 3 (deprecated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SPLIT_DOC_SIZE = 1024\n",
    "\n",
    "# for filename in tqdm(os.listdir(os.path.join(output_folder, \"train\"))):\n",
    "#     with open(os.path.join(output_folder, \"train\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "#         text = file.read().split(\" \")\n",
    "#         for split in range(0, (len(text)-1)//SPLIT_DOC_SIZE+1):\n",
    "#             with open(os.path.join(output_folder, \"train\", f\"{os.path.splitext(filename)[0]}_split{split}.txt\"), \"w\", encoding=\"utf-8\") as output_file:\n",
    "#                 output_file.write(\" \".join(text[split*SPLIT_DOC_SIZE:(split+1)*SPLIT_DOC_SIZE]))\n",
    "#         os.remove(os.path.join(output_folder, \"train\", filename))\n",
    "\n",
    "# for filename in tqdm(os.listdir(os.path.join(output_folder, \"test\"))):\n",
    "#     with open(os.path.join(output_folder, \"test\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "#         text = file.read().split(\" \")\n",
    "#         for split in range(0, (len(text)-1)//SPLIT_DOC_SIZE+1):\n",
    "#             with open(os.path.join(output_folder, \"test\", f\"{os.path.splitext(filename)[0]}_split{split}.txt\"), \"w\", encoding=\"utf-8\") as output_file:\n",
    "#                 output_file.write(\" \".join(text[split*SPLIT_DOC_SIZE:(split+1)*SPLIT_DOC_SIZE]))\n",
    "#         os.remove(os.path.join(output_folder, \"test\", filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ddp8196/miniconda3/envs/gpu-dev/lib/python3.11/site-packages/tqdm-4.67.1-py3.11.egg/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "100%|██████████| 747/747 [01:16<00:00,  9.74it/s]\n",
      "100%|██████████| 83/83 [00:37<00:00,  2.20it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "SKIP_INTRO_GRACE = 128\n",
    "SPLIT_DOC_SIZE = 1024\n",
    "\n",
    "model_name = \"./Llama-3.2-3B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for filename in tqdm(os.listdir(os.path.join(output_folder, \"train\"))):\n",
    "    with open(os.path.join(output_folder, \"train\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read().replace(\"\\n\", \" \")\n",
    "        tokenized_text = tokenizer(text, padding=\"max_length\", truncation=True, max_length=SPLIT_DOC_SIZE+SKIP_INTRO_GRACE, return_attention_mask=False)[\"input_ids\"]\n",
    "        with open(os.path.join(output_folder, \"train\", f\"{os.path.splitext(filename)[0]}_small.txt\"), \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(tokenizer.decode(tokenized_text[SKIP_INTRO_GRACE:]))\n",
    "        os.remove(os.path.join(output_folder, \"train\", filename))\n",
    "\n",
    "for filename in tqdm(os.listdir(os.path.join(output_folder, \"test\"))):\n",
    "    with open(os.path.join(output_folder, \"test\", filename), \"r\", encoding=\"utf-8\") as file:\n",
    "        text = file.read().replace(\"\\n\", \" \")\n",
    "        tokenized_text = tokenizer(text, padding=\"max_length\", truncation=True, max_length=SPLIT_DOC_SIZE+SKIP_INTRO_GRACE, return_attention_mask=False)[\"input_ids\"]\n",
    "        with open(os.path.join(output_folder, \"test\", f\"{os.path.splitext(filename)[0]}_small.txt\"), \"w\", encoding=\"utf-8\") as output_file:\n",
    "            output_file.write(tokenizer.decode(tokenized_text[SKIP_INTRO_GRACE:]))\n",
    "        os.remove(os.path.join(output_folder, \"test\", filename))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
